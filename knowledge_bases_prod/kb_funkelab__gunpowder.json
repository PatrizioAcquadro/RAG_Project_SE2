[
  "class Array(Freezable):\n    \"\"\"A numpy array with a specification describing the data.\n\n    Args:\n\n        data (array-like):\n\n            The data to be stored in the array. Will be converted to a numpy\n            array, if necessary.\n\n        spec (:class:`ArraySpec`, optional):\n\n            A spec describing the data.\n\n        attrs (``dict``, optional):\n\n            Optional attributes to describe this array.\n    \"\"\"\n\n    def __init__(self, data, spec=None, attrs=None):\n        self.spec = spec.copy()\n        self.data = np.asarray(data)\n        self.attrs = attrs\n\n        if attrs is None:\n            self.attrs = {}\n\n        if spec is not None and spec.roi is not None and spec.voxel_size is not None:\n            for d in range(len(spec.voxel_size)):\n                assert (\n                    spec.voxel_size[d] * data.shape[-spec.roi.dims + d]\n                    == spec.roi.shape[d]\n                ), \"ROI %s does not align with voxel size %s * data shape %s\" % (\n                    spec.roi,\n                    spec.voxel_size,\n                    data.shape,\n                )\n                if spec.roi.offset[d] is not None:\n                    assert (\n                        spec.roi.offset[d] % spec.voxel_size[d] == 0\n                    ), \"ROI offset %s must be a multiple of voxel size %s\" % (\n                        spec.roi.offset,\n                        spec.voxel_size,\n                    )\n\n        if spec.dtype is not None:\n            assert (\n                data.dtype == spec.dtype\n            ), \"data dtype %s does not match spec dtype %s\" % (data.dtype, spec.dtype)\n\n        self.freeze()\n\n    def crop(self, roi, copy=True):\n        \"\"\"Create a cropped copy of this Array.\n\n        Args:\n\n            roi (:class:`Roi`):\n\n                ROI in world units to crop to.\n\n            copy (``bool``):\n\n                Make a copy of the data.\n        \"\"\"\n\n        assert self.spec.roi.contains(\n            roi\n        ), \"Requested crop ROI (%s) doesn't fit in array (%s)\" % (roi, self.spec.roi)\n\n        if self.spec.roi == roi and not copy:\n            return self\n\n        voxel_size = self.spec.voxel_size\n        data_roi = (roi - self.spec.roi.offset) / voxel_size\n        slices = data_roi.get_bounding_box()\n\n        while len(slices) < len(self.data.shape):\n            slices = (slice(None),) + slices\n\n        data = self.data[slices]\n        if copy:\n            data = np.array(data)\n\n        spec = self.spec.copy()\n        attrs = deepcopy(self.attrs)\n        spec.roi = roi.copy()\n        return Array(data, spec, attrs)\n\n    def merge(self, array, copy_from_self=False, copy=False):\n        \"\"\"Merge this array with another one. The resulting array will have the\n        size of the larger one, with values replaced from ``array``.\n\n        This only works if one of the two arrays is contained in the other. In\n        this case, ``array`` will overwrite values in ``self`` (unless\n        ``copy_from_self`` is set to ``True``).\n\n        A copy will only be made if necessary or ``copy`` is set to ``True``.\n        \"\"\"\n        # It is unclear how to merge arrays in all cases. Consider a 10x10 array,\n        # you crop out a 5x5 area, do a shift augment, and attempt to merge.\n        # What does that mean? specs have changed. It should be a new key.\n        raise NotImplementedError(\"Merge function should not be used!\")\n\n        self_roi = self.spec.roi\n        array_roi = array.spec.roi\n\n        assert self_roi.contains(array_roi) or array_roi.contains(\n            self_roi\n        ), \"Can not merge arrays that are not contained in each other.\"\n\n        assert (\n            self.spec.voxel_size == array.spec.voxel_size\n        ), \"Can not merge arrays with different voxel sizes.\"\n\n        # make sure self contains array\n        if not self_roi.contains(array_roi):\n            return array.merge(self, not copy_from_self, copy)\n\n        # -> here we know that self contains array\n\n        # simple case, self overwrites all of array\n        if copy_from_self:\n            return self if not copy else self.copy()\n\n        # -> here we know that copy_from_self == False\n\n        # simple case, ROIs are the same\n        if self_roi == array_roi:\n            return array if not copy else array.copy()\n\n        # part of self have to be replaced, a copy is needed\n        merged = self.copy()\n\n        voxel_size = self.spec.voxel_size\n        data_roi = (array_roi - self_roi.offset) / voxel_size\n        slices = data_roi.get_bounding_box()\n\n        while len(slices) < len(self.data.shape):\n            slices = (slice(None),) + slices\n\n        merged.data[slices] = array.data\n\n        return merged\n\n    def __repr__(self):\n        return str(self.spec)\n\n    def copy(self):\n        \"\"\"Create a copy of this array.\"\"\"\n        return copy.deepcopy(self)",
  "class ArrayKey(Freezable):\n    \"\"\"A key to identify arrays in requests, batches, and across nodes.\n\n    Used as key in :class:`BatchRequest` and :class:`Batch` to retrieve array\n    specs or arrays.\n\n    Args:\n\n        identifier (``string``):\n\n            A unique, human readable identifier for this array key. Will be\n            used in log messages and to look up arrays in requests and batches.\n            Should be upper case (like ``RAW``, ``GT_LABELS``). The identifier\n            is unique: Two array keys with the same identifier will refer to\n            the same array.\n    \"\"\"\n\n    def __init__(self, identifier):\n        self.identifier = identifier\n        self.hash = hash(identifier)\n        self.freeze()\n        logger.debug(\"Registering array key %s\", self)\n        setattr(ArrayKeys, self.identifier, self)\n\n    def __eq__(self, other):\n        return hasattr(other, \"identifier\") and self.identifier == other.identifier\n\n    def __hash__(self):\n        return self.hash\n\n    def __repr__(self):\n        return self.identifier",
  "class ArrayKeys:\n    \"\"\"Convenience access to all created :class:``ArrayKey``s. A key generated\n    with::\n\n        raw = ArrayKey('RAW')\n\n    can be retrieved as::\n\n        ArrayKeys.RAW\n    \"\"\"\n\n    pass",
  "def __init__(self, data, spec=None, attrs=None):\n        self.spec = spec.copy()\n        self.data = np.asarray(data)\n        self.attrs = attrs\n\n        if attrs is None:\n            self.attrs = {}\n\n        if spec is not None and spec.roi is not None and spec.voxel_size is not None:\n            for d in range(len(spec.voxel_size)):\n                assert (\n                    spec.voxel_size[d] * data.shape[-spec.roi.dims + d]\n                    == spec.roi.shape[d]\n                ), \"ROI %s does not align with voxel size %s * data shape %s\" % (\n                    spec.roi,\n                    spec.voxel_size,\n                    data.shape,\n                )\n                if spec.roi.offset[d] is not None:\n                    assert (\n                        spec.roi.offset[d] % spec.voxel_size[d] == 0\n                    ), \"ROI offset %s must be a multiple of voxel size %s\" % (\n                        spec.roi.offset,\n                        spec.voxel_size,\n                    )\n\n        if spec.dtype is not None:\n            assert (\n                data.dtype == spec.dtype\n            ), \"data dtype %s does not match spec dtype %s\" % (data.dtype, spec.dtype)\n\n        self.freeze()",
  "def crop(self, roi, copy=True):\n        \"\"\"Create a cropped copy of this Array.\n\n        Args:\n\n            roi (:class:`Roi`):\n\n                ROI in world units to crop to.\n\n            copy (``bool``):\n\n                Make a copy of the data.\n        \"\"\"\n\n        assert self.spec.roi.contains(\n            roi\n        ), \"Requested crop ROI (%s) doesn't fit in array (%s)\" % (roi, self.spec.roi)\n\n        if self.spec.roi == roi and not copy:\n            return self\n\n        voxel_size = self.spec.voxel_size\n        data_roi = (roi - self.spec.roi.offset) / voxel_size\n        slices = data_roi.get_bounding_box()\n\n        while len(slices) < len(self.data.shape):\n            slices = (slice(None),) + slices\n\n        data = self.data[slices]\n        if copy:\n            data = np.array(data)\n\n        spec = self.spec.copy()\n        attrs = deepcopy(self.attrs)\n        spec.roi = roi.copy()\n        return Array(data, spec, attrs)",
  "def merge(self, array, copy_from_self=False, copy=False):\n        \"\"\"Merge this array with another one. The resulting array will have the\n        size of the larger one, with values replaced from ``array``.\n\n        This only works if one of the two arrays is contained in the other. In\n        this case, ``array`` will overwrite values in ``self`` (unless\n        ``copy_from_self`` is set to ``True``).\n\n        A copy will only be made if necessary or ``copy`` is set to ``True``.\n        \"\"\"\n        # It is unclear how to merge arrays in all cases. Consider a 10x10 array,\n        # you crop out a 5x5 area, do a shift augment, and attempt to merge.\n        # What does that mean? specs have changed. It should be a new key.\n        raise NotImplementedError(\"Merge function should not be used!\")\n\n        self_roi = self.spec.roi\n        array_roi = array.spec.roi\n\n        assert self_roi.contains(array_roi) or array_roi.contains(\n            self_roi\n        ), \"Can not merge arrays that are not contained in each other.\"\n\n        assert (\n            self.spec.voxel_size == array.spec.voxel_size\n        ), \"Can not merge arrays with different voxel sizes.\"\n\n        # make sure self contains array\n        if not self_roi.contains(array_roi):\n            return array.merge(self, not copy_from_self, copy)\n\n        # -> here we know that self contains array\n\n        # simple case, self overwrites all of array\n        if copy_from_self:\n            return self if not copy else self.copy()\n\n        # -> here we know that copy_from_self == False\n\n        # simple case, ROIs are the same\n        if self_roi == array_roi:\n            return array if not copy else array.copy()\n\n        # part of self have to be replaced, a copy is needed\n        merged = self.copy()\n\n        voxel_size = self.spec.voxel_size\n        data_roi = (array_roi - self_roi.offset) / voxel_size\n        slices = data_roi.get_bounding_box()\n\n        while len(slices) < len(self.data.shape):\n            slices = (slice(None),) + slices\n\n        merged.data[slices] = array.data\n\n        return merged",
  "def __repr__(self):\n        return str(self.spec)",
  "def copy(self):\n        \"\"\"Create a copy of this array.\"\"\"\n        return copy.deepcopy(self)",
  "def __init__(self, identifier):\n        self.identifier = identifier\n        self.hash = hash(identifier)\n        self.freeze()\n        logger.debug(\"Registering array key %s\", self)\n        setattr(ArrayKeys, self.identifier, self)",
  "def __eq__(self, other):\n        return hasattr(other, \"identifier\") and self.identifier == other.identifier",
  "def __hash__(self):\n        return self.hash",
  "def __repr__(self):\n        return self.identifier",
  "def replace(array, old_values, new_values):\n    \"\"\"Replace all occurences of ``old_values[i]`` with ``new_values[i]`` in the\n    given array.\"\"\"\n\n    old_values = np.array(old_values)\n    new_values = np.array(new_values)\n\n    values_map = np.arange(int(array.max() + 1), dtype=new_values.dtype)\n    values_map[old_values] = new_values\n\n    return values_map[array]",
  "class Node(Freezable):\n    \"\"\"\n    A stucture representing each node in a Graph.\n\n    Args:\n\n        id (``int``):\n\n            A unique identifier for this Node\n\n        location (``np.ndarray``):\n\n            A numpy array containing a nodes location\n\n        Optional attrs (``dict``, str -> ``Any``):\n\n            A dictionary containing a mapping from attribute to value.\n            Used to store any extra attributes associated with the\n            Node such as color, size, etc.\n\n        Optional temporary (bool):\n\n            A tag to mark a node as temporary. Some operations such\n            as `trim` might make new nodes that are just biproducts\n            of viewing the data with a limited scope. These nodes\n            are only guaranteed to have an id different from those\n            in the same Graph, but may have conflicts if you request\n            multiple graphs from the same source with different rois.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        location: np.ndarray,\n        temporary: bool = False,\n        attrs: Optional[Dict[str, Any]] = None,\n    ):\n        self.__attrs = attrs if attrs is not None else {}\n        self.attrs[\"id\"] = id\n        self.location = location\n        # purpose is to keep track of nodes that were created during\n        # processing and do not have a corresponding node in the original source\n        self.attrs[\"temporary\"] = temporary\n        self.freeze()\n\n    def __getattr__(self, attr):\n        if \"__\" not in attr:\n            return self.attrs[attr]\n        else:\n            return super().__getattr__(attr)\n\n    def __setattr__(self, attr, value):\n        if \"__\" not in attr:\n            self.attrs[attr] = value\n        else:\n            super().__setattr__(attr, value)\n\n    @property\n    def location(self):\n        location = self.attrs[\"location\"]\n        return location\n\n    @location.setter\n    def location(self, new_location):\n        assert isinstance(new_location, np.ndarray)\n        self.attrs[\"location\"] = new_location\n\n    @property\n    def id(self):\n        return self.attrs[\"id\"]\n\n    @property\n    def original_id(self):\n        return self.id if not self.temporary else None\n\n    @property\n    def temporary(self):\n        return self.attrs[\"temporary\"]\n\n    @property\n    def attrs(self):\n        return self.__attrs\n\n    @property\n    def all(self):\n        return self.attrs\n\n    @classmethod\n    def from_attrs(cls, attrs: Dict[str, Any]):\n        node_id = attrs[\"id\"]\n        location = attrs[\"location\"]\n        temporary = attrs.get(\"temporary\", False)\n        return cls(id=node_id, location=location, temporary=temporary, attrs=attrs)\n\n    def __str__(self):\n        return f\"Node({self.temporary}) ({self.id}) at ({self.location})\"\n\n    def __repr__(self):\n        return str(self)\n\n    def __eq__(self, other):\n        return isinstance(other, Node) and self.id == other.id\n\n    def __hash__(self):\n        return hash(self.id)",
  "class Edge(Freezable):\n    \"\"\"\n    A structure representing edges in a graph.\n\n    Args:\n\n        u (``int``)\n\n            The id of the 'u' node of this edge\n\n        v (``int``)\n\n            the id of the `v` node of this edge\n    \"\"\"\n\n    def __init__(self, u: int, v: int, attrs: Optional[Dict[str, Any]] = None):\n        self.__u = u\n        self.__v = v\n        self.__attrs = attrs if attrs is not None else {}\n        self.freeze()\n\n    @property\n    def u(self):\n        return self.__u\n\n    @property\n    def v(self):\n        return self.__v\n\n    @property\n    def all(self):\n        return self.__attrs\n\n    def __iter__(self):\n        return iter([self.u, self.v])\n\n    def __str__(self):\n        return f\"({self.u}, {self.v})\"\n\n    def __repr__(self):\n        return f\"({self.u}, {self.v})\"\n\n    def __eq__(self, other):\n        return self.u == other.u and self.v == other.v\n\n    def __hash__(self):\n        return hash((self.u, self.v))\n\n    def directed_eq(self, other):\n        return self.u == other.u and self.v == other.v\n\n    def undirected_eq(self, other):\n        return set([self.u, self.v]) == set([other.u, other.v])",
  "class Graph(Freezable):\n    \"\"\"A structure containing a list of :class:`Node`, a list of :class:`Edge`,\n    and a specification describing the data.\n\n    Args:\n\n        nodes (``iterator``, :class:`Node`):\n\n            An iterator containing Vertices.\n\n        edges (``iterator``, :class:`Edge`):\n\n            An iterator containing Edges.\n\n        spec (:class:`GraphSpec`):\n\n            A spec describing the data.\n    \"\"\"\n\n    def __init__(self, nodes: Iterator[Node], edges: Iterator[Edge], spec: GraphSpec):\n        self.__spec = spec\n        self.__graph = self.create_graph(nodes, edges)\n\n    @property\n    def spec(self):\n        return self.__spec\n\n    @spec.setter\n    def spec(self, new_spec):\n        self.__spec = new_spec\n\n    @property\n    def directed(self):\n        return (\n            self.spec.directed\n            if self.spec.directed is not None\n            else self.__graph.is_directed()\n        )\n\n    def create_graph(self, nodes: Iterator[Node], edges: Iterator[Edge]):\n        if self.__spec.directed is None:\n            logger.debug(\n                \"Trying to create a Graph without specifying directionality. Using default Directed!\"\n            )\n            graph = nx.DiGraph()\n        elif self.__spec.directed:\n            graph = nx.DiGraph()\n        else:\n            graph = nx.Graph()\n\n        for node in nodes:\n            node.location = node.location.astype(self.spec.dtype)\n\n        vs = [(v.id, v.all) for v in nodes]\n        graph.add_nodes_from(vs)\n        graph.add_edges_from([(e.u, e.v, e.all) for e in edges])\n        return graph\n\n    @property\n    def nodes(self):\n        for node_id, node_attrs in self.__graph.nodes.items():\n            if \"id\" not in node_attrs:\n                node_attrs[\"id\"] = node_id\n            v = Node.from_attrs(node_attrs)\n            if not np.issubdtype(v.location.dtype, self.spec.dtype):\n                raise Exception(\n                    f\"expected location to have dtype {self.spec.dtype} but it had {v.location.dtype}\"\n                )\n            yield v\n\n    def num_vertices(self):\n        return self.__graph.number_of_nodes()\n\n    def num_edges(self):\n        return self.__graph.number_of_edges()\n\n    @property\n    def edges(self):\n        for (u, v), attrs in self.__graph.edges.items():\n            yield Edge(u, v, attrs)\n\n    def neighbors(self, node):\n        if self.directed:\n            for neighbor in self.__graph.successors(node.id):\n                yield Node.from_attrs(self.__graph.nodes[neighbor])\n            if self.directed:\n                for neighbor in self.__graph.predecessors(node.id):\n                    yield Node.from_attrs(self.__graph.nodes[neighbor])\n        else:\n            for neighbor in self.__graph.neighbors(node.id):\n                yield Node.from_attrs(self.__graph.nodes[neighbor])\n\n    def __str__(self):\n        string = \"Vertices:\\n\"\n        for node in self.nodes:\n            string += f\"{node}\\n\"\n        string += \"Edges:\\n\"\n        for edge in self.edges:\n            string += f\"{edge}\\n\"\n        return string\n\n    def __repr__(self):\n        return str(self)\n\n    def node(self, id: int):\n        \"\"\"\n        Get node with a specific id\n        \"\"\"\n        attrs = self.__graph.nodes[id]\n        return Node.from_attrs(attrs)\n\n    def contains(self, node_id: int):\n        return node_id in self.__graph.nodes\n\n    def remove_node(self, node: Node, retain_connectivity=False):\n        \"\"\"\n        Remove a node.\n\n        retain_connectivity: preserve removed nodes neighboring edges.\n        Given graph: a->b->c, removing `b` without retain_connectivity\n        would leave us with two connected components, {'a'} and {'b'}.\n        removing 'b' with retain_connectivity flag set to True would\n        leave us with the graph: a->c, and only one connected component\n        {a, c}, thus preserving the connectivity of 'a' and 'c'\n        \"\"\"\n        if retain_connectivity:\n            predecessors = self.predecessors(node)\n            successors = self.successors(node)\n\n            for pred_id in predecessors:\n                for succ_id in successors:\n                    if pred_id != succ_id:\n                        self.add_edge(Edge(pred_id, succ_id))\n        self.__graph.remove_node(node.id)\n\n    def add_node(self, node: Node):\n        \"\"\"\n        Adds a node to the graph.\n        If a node exists with the same id as the node you are adding,\n        its attributes will be overwritten.\n        \"\"\"\n        node.location = node.location.astype(self.spec.dtype)\n        self.__graph.add_node(node.id, **node.all)\n\n    def remove_edge(self, edge: Edge):\n        \"\"\"\n        Remove an edge from the graph.\n        \"\"\"\n        self.__graph.remove_edge(edge.u, edge.v)\n\n    def add_edge(self, edge: Edge):\n        \"\"\"\n        Adds an edge to the graph.\n        If an edge exists with the same u and v, its attributes\n        will be overwritten.\n        \"\"\"\n        self.__graph.add_edge(edge.u, edge.v, **edge.all)\n\n    def copy(self):\n        return deepcopy(self)\n\n    def crop(self, roi: Roi):\n        \"\"\"\n        Will remove all nodes from self that are not contained in `roi` except for\n        \"dangling\" nodes. This means that if there are nodes A, B s.t. there\n        is an edge (A, B) and A is contained in `roi` but B is not, the edge (A, B)\n        is considered contained in the `roi` and thus node B will be kept as a\n        \"dangling\" node.\n\n        Note there is a helper function `trim` that will remove B and replace it with\n        a node at the intersection of the edge (A, B) and the bounding box of `roi`.\n\n        Args:\n\n            roi (:class:`Roi`):\n\n                ROI in world units to crop to.\n        \"\"\"\n\n        cropped = self.copy()\n\n        contained_nodes = set([v.id for v in cropped.nodes if roi.contains(v.location)])\n        all_contained_edges = set(\n            [\n                e\n                for e in cropped.edges\n                if e.u in contained_nodes or e.v in contained_nodes\n            ]\n        )\n        fully_contained_edges = set(\n            [\n                e\n                for e in all_contained_edges\n                if e.u in contained_nodes and e.v in contained_nodes\n            ]\n        )\n        partially_contained_edges = all_contained_edges - fully_contained_edges\n        contained_edge_nodes = set(list(itertools.chain(*all_contained_edges)))\n        all_nodes = contained_edge_nodes | contained_nodes\n        dangling_nodes = all_nodes - contained_nodes\n\n        for node in list(cropped.nodes):\n            if node.id not in all_nodes:\n                cropped.remove_node(node)\n        for edge in list(cropped.edges):\n            if edge not in all_contained_edges:\n                cropped.remove_edge(edge)\n\n        cropped.spec.roi = roi\n        return cropped\n\n    def shift(self, offset):\n        for node in self.nodes:\n            node.location += offset\n\n    def new_graph(self):\n        if self.directed():\n            return nx.DiGraph()\n        else:\n            return nx.Graph()\n\n    def trim(self, roi: Roi):\n        \"\"\"\n        Create a copy of self and replace \"dangling\" nodes with contained nodes.\n\n        A \"dangling\" node is defined by: Let A, B be nodes s.t. there exists an\n        edge (A, B) and A is contained in `roi` but B is not. Edge (A, B) is considered\n        contained, and thus B is kept as a \"dangling\" node.\n        \"\"\"\n\n        trimmed = self.copy()\n\n        contained_nodes = set([v.id for v in trimmed.nodes if roi.contains(v.location)])\n        all_contained_edges = set(\n            [\n                e\n                for e in trimmed.edges\n                if e.u in contained_nodes or e.v in contained_nodes\n            ]\n        )\n        fully_contained_edges = set(\n            [\n                e\n                for e in all_contained_edges\n                if e.u in contained_nodes and e.v in contained_nodes\n            ]\n        )\n        partially_contained_edges = all_contained_edges - fully_contained_edges\n        contained_edge_nodes = set(list(itertools.chain(*all_contained_edges)))\n        all_nodes = contained_edge_nodes | contained_nodes\n        dangling_nodes = all_nodes - contained_nodes\n\n        next_node = 0 if len(all_nodes) == 0 else max(all_nodes) + 1\n\n        trimmed._handle_boundaries(\n            partially_contained_edges,\n            contained_nodes,\n            roi,\n            node_id=itertools.count(next_node),\n        )\n\n        for node in trimmed.nodes:\n            assert roi.contains(\n                node.location\n            ), f\"Failed to properly contain node {node.id} at {node.location}\"\n\n        return trimmed\n\n    def _handle_boundaries(\n        self,\n        crossing_edges: Iterator[Edge],\n        contained_nodes: Set[int],\n        roi: Roi,\n        node_id: Iterator[int],\n    ):\n        nodes_to_remove = set([])\n        for e in crossing_edges:\n            u, v = self.node(e.u), self.node(e.v)\n            u_in = u.id in contained_nodes\n            v_in, v_out = (u, v) if u_in else (v, u)\n            in_location, out_location = (v_in.location, v_out.location)\n            new_location = self._roi_intercept(in_location, out_location, roi)\n            if not all(np.isclose(new_location, in_location)):\n                # use deepcopy because modifying this node should not modify original\n                new_attrs = deepcopy(v_out.attrs)\n                new_attrs[\"id\"] = next(node_id)\n                new_attrs[\"location\"] = new_location\n                new_attrs[\"temporary\"] = True\n                new_v = Node.from_attrs(new_attrs)\n                new_e = Edge(\n                    u=v_in.id if u_in else new_v.id, v=new_v.id if u_in else v_in.id\n                )\n                self.add_node(new_v)\n                self.add_edge(new_e)\n            nodes_to_remove.add(v_out)\n        for node in nodes_to_remove:\n            self.remove_node(node)\n\n    def _roi_intercept(\n        self, inside: np.ndarray, outside: np.ndarray, bb: Roi\n    ) -> np.ndarray:\n        \"\"\"\n        Given two points, one inside a bounding box and one outside,\n        get the intercept between the line and the bounding box.\n        \"\"\"\n\n        offset = outside - inside\n        distance = np.linalg.norm(offset)\n        assert not np.isclose(distance, 0), f\"Inside and Outside are the same location\"\n        direction = offset / distance\n\n        # `offset` can be 0 on some but not all axes leaving a 0 in the denominator.\n        # `inside` can be on the bounding box, leaving a 0 in the numerator.\n        # `x/0` throws a division warning, `0/0` throws an invalid warning (both are fine here)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            bb_x = np.asarray(\n                [\n                    (np.asarray(bb.begin) - inside) / offset,\n                    (np.asarray(bb.end) - inside) / offset,\n                ],\n                dtype=self.spec.dtype,\n            )\n\n        with np.errstate(invalid=\"ignore\"):\n            s = np.min(bb_x[np.logical_and((bb_x >= 0), (bb_x <= 1))])\n\n        new_location = inside + s * distance * direction\n        upper = np.array(bb.end, dtype=self.spec.dtype)\n        new_location = np.clip(\n            new_location, bb.begin, upper - upper * np.finfo(self.spec.dtype).eps\n        )\n        return new_location\n\n    def merge(self, other, copy_from_self=False, copy=False):\n        \"\"\"\n        Merge this graph with another. The resulting graph will have the Roi\n        of the larger one.\n\n        This only works if one of the two graphs contains the other.\n        In this case, ``other`` will overwrite edges and nodes with the same\n        ID in ``self`` (unless ``copy_from_self`` is set to ``True``).\n        Vertices and edges in ``self`` that are contained in the Roi of ``other``\n        will be removed (vice versa for ``copy_from_self``)\n\n        A copy will only be made if necessary or ``copy`` is set to ``True``.\n        \"\"\"\n\n        # It is unclear how to merge points in all cases. Consider a 10x10 graph,\n        # you crop out a 5x5 area, do a shift augment, and attempt to merge.\n        # What does that mean? specs have changed. It should be a new key.\n        raise NotImplementedError(\"Merge function should not be used!\")\n\n        self_roi = self.spec.roi\n        other_roi = other.spec.roi\n\n        assert self_roi.contains(other_roi) or other_roi.contains(\n            self_roi\n        ), \"Can not merge graphs that are not contained in each other.\"\n\n        # make sure self contains other\n        if not self_roi.contains(other_roi):\n            return other.merge(self, not copy_from_self, copy)\n\n        # edges and nodes in addition are guaranteed to be in merged\n        base = other if copy_from_self else self\n        addition = self if copy_from_self else other\n\n        if copy:\n            merged = base.copy()\n        else:\n            merged = base\n\n        for node in list(merged.nodes):\n            if merged.spec.roi.contains(node.location):\n                merged.remove_node(node)\n        for edge in list(merged.edges):\n            if merged.spec.roi.contains(\n                merged.node(edge.u)\n            ) or merged.spec.roi.contains(merged.node(edge.v)):\n                merged.remove_edge(edge)\n        for node in addition.nodes:\n            merged.add_node(node)\n        for edge in addition.edges:\n            merged.add_edge(edge)\n\n        return merged\n\n    def to_nx_graph(self):\n        \"\"\"\n        returns a pure networkx graph containing data from\n        this Graph.\n        \"\"\"\n        return deepcopy(self.__graph)\n\n    @classmethod\n    def from_nx_graph(cls, graph, spec):\n        \"\"\"\n        Create a gunpowder graph from a networkx graph.\n        The network graph is expected to have a \"location\"\n        attribute for each node. If it is a subclass of a networkx\n        graph with extra functionality, this may not work.\n        \"\"\"\n        if spec.directed is None:\n            spec.directed = graph.is_directed()\n        g = cls([], [], spec)\n        g.__graph = graph\n        return g\n\n    def relabel_connected_components(self):\n        \"\"\"\n        create a new attribute \"component\" for each node\n        in this Graph\n        \"\"\"\n        for i, wcc in enumerate(self.connected_components):\n            for node in wcc:\n                self.__graph.nodes[node][\"component\"] = i\n\n    @property\n    def connected_components(self):\n        if not self.directed:\n            return nx.connected_components(self.__graph)\n        else:\n            return nx.weakly_connected_components(self.__graph)\n\n    def in_degree(self):\n        return self.__graph.in_degree()\n\n    def successors(self, node):\n        if self.directed:\n            return self.__graph.successors(node.id)\n        else:\n            return self.__graph.neighbors(node.id)\n\n    def predecessors(self, node):\n        if self.directed:\n            return self.__graph.predecessors(node.id)\n        else:\n            return self.__graph.neighbors(node.id)",
  "class GraphKey(Freezable):\n    \"\"\"A key to identify graphs in requests, batches, and across\n    nodes.\n\n    Used as key in :class:`BatchRequest` and :class:`Batch` to retrieve specs\n    or graphs.\n\n    Args:\n\n        identifier (``string``):\n\n            A unique, human readable identifier for this graph key. Will be\n            used in log messages and to look up graphs in requests and batches.\n            Should be upper case (like ``CENTER_GRAPH``). The identifier is\n            unique: Two graph keys with the same identifier will refer to the\n            same graph.\n    \"\"\"\n\n    def __init__(self, identifier):\n        self.identifier = identifier\n        self.hash = hash(identifier)\n        self.freeze()\n        logger.debug(\"Registering graph type %s\", self)\n        setattr(GraphKeys, self.identifier, self)\n\n    def __eq__(self, other):\n        return hasattr(other, \"identifier\") and self.identifier == other.identifier\n\n    def __hash__(self):\n        return self.hash\n\n    def __repr__(self):\n        return self.identifier",
  "class GraphKeys:\n    \"\"\"Convenience access to all created :class:`GraphKey`s. A key generated\n    with::\n\n        centers = GraphKey('CENTER_GRAPH')\n\n    can be retrieved as::\n\n        GraphKeys.CENTER_GRAPH\n    \"\"\"\n\n    pass",
  "def __init__(\n        self,\n        id: int,\n        location: np.ndarray,\n        temporary: bool = False,\n        attrs: Optional[Dict[str, Any]] = None,\n    ):\n        self.__attrs = attrs if attrs is not None else {}\n        self.attrs[\"id\"] = id\n        self.location = location\n        # purpose is to keep track of nodes that were created during\n        # processing and do not have a corresponding node in the original source\n        self.attrs[\"temporary\"] = temporary\n        self.freeze()",
  "def __getattr__(self, attr):\n        if \"__\" not in attr:\n            return self.attrs[attr]\n        else:\n            return super().__getattr__(attr)",
  "def __setattr__(self, attr, value):\n        if \"__\" not in attr:\n            self.attrs[attr] = value\n        else:\n            super().__setattr__(attr, value)",
  "def location(self):\n        location = self.attrs[\"location\"]\n        return location",
  "def location(self, new_location):\n        assert isinstance(new_location, np.ndarray)\n        self.attrs[\"location\"] = new_location",
  "def id(self):\n        return self.attrs[\"id\"]",
  "def original_id(self):\n        return self.id if not self.temporary else None",
  "def temporary(self):\n        return self.attrs[\"temporary\"]",
  "def attrs(self):\n        return self.__attrs",
  "def all(self):\n        return self.attrs",
  "def from_attrs(cls, attrs: Dict[str, Any]):\n        node_id = attrs[\"id\"]\n        location = attrs[\"location\"]\n        temporary = attrs.get(\"temporary\", False)\n        return cls(id=node_id, location=location, temporary=temporary, attrs=attrs)",
  "def __str__(self):\n        return f\"Node({self.temporary}) ({self.id}) at ({self.location})\"",
  "def __repr__(self):\n        return str(self)",
  "def __eq__(self, other):\n        return isinstance(other, Node) and self.id == other.id",
  "def __hash__(self):\n        return hash(self.id)",
  "def __init__(self, u: int, v: int, attrs: Optional[Dict[str, Any]] = None):\n        self.__u = u\n        self.__v = v\n        self.__attrs = attrs if attrs is not None else {}\n        self.freeze()",
  "def u(self):\n        return self.__u",
  "def v(self):\n        return self.__v",
  "def all(self):\n        return self.__attrs",
  "def __iter__(self):\n        return iter([self.u, self.v])",
  "def __str__(self):\n        return f\"({self.u}, {self.v})\"",
  "def __repr__(self):\n        return f\"({self.u}, {self.v})\"",
  "def __eq__(self, other):\n        return self.u == other.u and self.v == other.v",
  "def __hash__(self):\n        return hash((self.u, self.v))",
  "def directed_eq(self, other):\n        return self.u == other.u and self.v == other.v",
  "def undirected_eq(self, other):\n        return set([self.u, self.v]) == set([other.u, other.v])",
  "def __init__(self, nodes: Iterator[Node], edges: Iterator[Edge], spec: GraphSpec):\n        self.__spec = spec\n        self.__graph = self.create_graph(nodes, edges)",
  "def spec(self):\n        return self.__spec",
  "def spec(self, new_spec):\n        self.__spec = new_spec",
  "def directed(self):\n        return (\n            self.spec.directed\n            if self.spec.directed is not None\n            else self.__graph.is_directed()\n        )",
  "def create_graph(self, nodes: Iterator[Node], edges: Iterator[Edge]):\n        if self.__spec.directed is None:\n            logger.debug(\n                \"Trying to create a Graph without specifying directionality. Using default Directed!\"\n            )\n            graph = nx.DiGraph()\n        elif self.__spec.directed:\n            graph = nx.DiGraph()\n        else:\n            graph = nx.Graph()\n\n        for node in nodes:\n            node.location = node.location.astype(self.spec.dtype)\n\n        vs = [(v.id, v.all) for v in nodes]\n        graph.add_nodes_from(vs)\n        graph.add_edges_from([(e.u, e.v, e.all) for e in edges])\n        return graph",
  "def nodes(self):\n        for node_id, node_attrs in self.__graph.nodes.items():\n            if \"id\" not in node_attrs:\n                node_attrs[\"id\"] = node_id\n            v = Node.from_attrs(node_attrs)\n            if not np.issubdtype(v.location.dtype, self.spec.dtype):\n                raise Exception(\n                    f\"expected location to have dtype {self.spec.dtype} but it had {v.location.dtype}\"\n                )\n            yield v",
  "def num_vertices(self):\n        return self.__graph.number_of_nodes()",
  "def num_edges(self):\n        return self.__graph.number_of_edges()",
  "def edges(self):\n        for (u, v), attrs in self.__graph.edges.items():\n            yield Edge(u, v, attrs)",
  "def neighbors(self, node):\n        if self.directed:\n            for neighbor in self.__graph.successors(node.id):\n                yield Node.from_attrs(self.__graph.nodes[neighbor])\n            if self.directed:\n                for neighbor in self.__graph.predecessors(node.id):\n                    yield Node.from_attrs(self.__graph.nodes[neighbor])\n        else:\n            for neighbor in self.__graph.neighbors(node.id):\n                yield Node.from_attrs(self.__graph.nodes[neighbor])",
  "def __str__(self):\n        string = \"Vertices:\\n\"\n        for node in self.nodes:\n            string += f\"{node}\\n\"\n        string += \"Edges:\\n\"\n        for edge in self.edges:\n            string += f\"{edge}\\n\"\n        return string",
  "def __repr__(self):\n        return str(self)",
  "def node(self, id: int):\n        \"\"\"\n        Get node with a specific id\n        \"\"\"\n        attrs = self.__graph.nodes[id]\n        return Node.from_attrs(attrs)",
  "def contains(self, node_id: int):\n        return node_id in self.__graph.nodes",
  "def remove_node(self, node: Node, retain_connectivity=False):\n        \"\"\"\n        Remove a node.\n\n        retain_connectivity: preserve removed nodes neighboring edges.\n        Given graph: a->b->c, removing `b` without retain_connectivity\n        would leave us with two connected components, {'a'} and {'b'}.\n        removing 'b' with retain_connectivity flag set to True would\n        leave us with the graph: a->c, and only one connected component\n        {a, c}, thus preserving the connectivity of 'a' and 'c'\n        \"\"\"\n        if retain_connectivity:\n            predecessors = self.predecessors(node)\n            successors = self.successors(node)\n\n            for pred_id in predecessors:\n                for succ_id in successors:\n                    if pred_id != succ_id:\n                        self.add_edge(Edge(pred_id, succ_id))\n        self.__graph.remove_node(node.id)",
  "def add_node(self, node: Node):\n        \"\"\"\n        Adds a node to the graph.\n        If a node exists with the same id as the node you are adding,\n        its attributes will be overwritten.\n        \"\"\"\n        node.location = node.location.astype(self.spec.dtype)\n        self.__graph.add_node(node.id, **node.all)",
  "def remove_edge(self, edge: Edge):\n        \"\"\"\n        Remove an edge from the graph.\n        \"\"\"\n        self.__graph.remove_edge(edge.u, edge.v)",
  "def add_edge(self, edge: Edge):\n        \"\"\"\n        Adds an edge to the graph.\n        If an edge exists with the same u and v, its attributes\n        will be overwritten.\n        \"\"\"\n        self.__graph.add_edge(edge.u, edge.v, **edge.all)",
  "def copy(self):\n        return deepcopy(self)",
  "def crop(self, roi: Roi):\n        \"\"\"\n        Will remove all nodes from self that are not contained in `roi` except for\n        \"dangling\" nodes. This means that if there are nodes A, B s.t. there\n        is an edge (A, B) and A is contained in `roi` but B is not, the edge (A, B)\n        is considered contained in the `roi` and thus node B will be kept as a\n        \"dangling\" node.\n\n        Note there is a helper function `trim` that will remove B and replace it with\n        a node at the intersection of the edge (A, B) and the bounding box of `roi`.\n\n        Args:\n\n            roi (:class:`Roi`):\n\n                ROI in world units to crop to.\n        \"\"\"\n\n        cropped = self.copy()\n\n        contained_nodes = set([v.id for v in cropped.nodes if roi.contains(v.location)])\n        all_contained_edges = set(\n            [\n                e\n                for e in cropped.edges\n                if e.u in contained_nodes or e.v in contained_nodes\n            ]\n        )\n        fully_contained_edges = set(\n            [\n                e\n                for e in all_contained_edges\n                if e.u in contained_nodes and e.v in contained_nodes\n            ]\n        )\n        partially_contained_edges = all_contained_edges - fully_contained_edges\n        contained_edge_nodes = set(list(itertools.chain(*all_contained_edges)))\n        all_nodes = contained_edge_nodes | contained_nodes\n        dangling_nodes = all_nodes - contained_nodes\n\n        for node in list(cropped.nodes):\n            if node.id not in all_nodes:\n                cropped.remove_node(node)\n        for edge in list(cropped.edges):\n            if edge not in all_contained_edges:\n                cropped.remove_edge(edge)\n\n        cropped.spec.roi = roi\n        return cropped",
  "def shift(self, offset):\n        for node in self.nodes:\n            node.location += offset",
  "def new_graph(self):\n        if self.directed():\n            return nx.DiGraph()\n        else:\n            return nx.Graph()",
  "def trim(self, roi: Roi):\n        \"\"\"\n        Create a copy of self and replace \"dangling\" nodes with contained nodes.\n\n        A \"dangling\" node is defined by: Let A, B be nodes s.t. there exists an\n        edge (A, B) and A is contained in `roi` but B is not. Edge (A, B) is considered\n        contained, and thus B is kept as a \"dangling\" node.\n        \"\"\"\n\n        trimmed = self.copy()\n\n        contained_nodes = set([v.id for v in trimmed.nodes if roi.contains(v.location)])\n        all_contained_edges = set(\n            [\n                e\n                for e in trimmed.edges\n                if e.u in contained_nodes or e.v in contained_nodes\n            ]\n        )\n        fully_contained_edges = set(\n            [\n                e\n                for e in all_contained_edges\n                if e.u in contained_nodes and e.v in contained_nodes\n            ]\n        )\n        partially_contained_edges = all_contained_edges - fully_contained_edges\n        contained_edge_nodes = set(list(itertools.chain(*all_contained_edges)))\n        all_nodes = contained_edge_nodes | contained_nodes\n        dangling_nodes = all_nodes - contained_nodes\n\n        next_node = 0 if len(all_nodes) == 0 else max(all_nodes) + 1\n\n        trimmed._handle_boundaries(\n            partially_contained_edges,\n            contained_nodes,\n            roi,\n            node_id=itertools.count(next_node),\n        )\n\n        for node in trimmed.nodes:\n            assert roi.contains(\n                node.location\n            ), f\"Failed to properly contain node {node.id} at {node.location}\"\n\n        return trimmed",
  "def _handle_boundaries(\n        self,\n        crossing_edges: Iterator[Edge],\n        contained_nodes: Set[int],\n        roi: Roi,\n        node_id: Iterator[int],\n    ):\n        nodes_to_remove = set([])\n        for e in crossing_edges:\n            u, v = self.node(e.u), self.node(e.v)\n            u_in = u.id in contained_nodes\n            v_in, v_out = (u, v) if u_in else (v, u)\n            in_location, out_location = (v_in.location, v_out.location)\n            new_location = self._roi_intercept(in_location, out_location, roi)\n            if not all(np.isclose(new_location, in_location)):\n                # use deepcopy because modifying this node should not modify original\n                new_attrs = deepcopy(v_out.attrs)\n                new_attrs[\"id\"] = next(node_id)\n                new_attrs[\"location\"] = new_location\n                new_attrs[\"temporary\"] = True\n                new_v = Node.from_attrs(new_attrs)\n                new_e = Edge(\n                    u=v_in.id if u_in else new_v.id, v=new_v.id if u_in else v_in.id\n                )\n                self.add_node(new_v)\n                self.add_edge(new_e)\n            nodes_to_remove.add(v_out)\n        for node in nodes_to_remove:\n            self.remove_node(node)",
  "def _roi_intercept(\n        self, inside: np.ndarray, outside: np.ndarray, bb: Roi\n    ) -> np.ndarray:\n        \"\"\"\n        Given two points, one inside a bounding box and one outside,\n        get the intercept between the line and the bounding box.\n        \"\"\"\n\n        offset = outside - inside\n        distance = np.linalg.norm(offset)\n        assert not np.isclose(distance, 0), f\"Inside and Outside are the same location\"\n        direction = offset / distance\n\n        # `offset` can be 0 on some but not all axes leaving a 0 in the denominator.\n        # `inside` can be on the bounding box, leaving a 0 in the numerator.\n        # `x/0` throws a division warning, `0/0` throws an invalid warning (both are fine here)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            bb_x = np.asarray(\n                [\n                    (np.asarray(bb.begin) - inside) / offset,\n                    (np.asarray(bb.end) - inside) / offset,\n                ],\n                dtype=self.spec.dtype,\n            )\n\n        with np.errstate(invalid=\"ignore\"):\n            s = np.min(bb_x[np.logical_and((bb_x >= 0), (bb_x <= 1))])\n\n        new_location = inside + s * distance * direction\n        upper = np.array(bb.end, dtype=self.spec.dtype)\n        new_location = np.clip(\n            new_location, bb.begin, upper - upper * np.finfo(self.spec.dtype).eps\n        )\n        return new_location",
  "def merge(self, other, copy_from_self=False, copy=False):\n        \"\"\"\n        Merge this graph with another. The resulting graph will have the Roi\n        of the larger one.\n\n        This only works if one of the two graphs contains the other.\n        In this case, ``other`` will overwrite edges and nodes with the same\n        ID in ``self`` (unless ``copy_from_self`` is set to ``True``).\n        Vertices and edges in ``self`` that are contained in the Roi of ``other``\n        will be removed (vice versa for ``copy_from_self``)\n\n        A copy will only be made if necessary or ``copy`` is set to ``True``.\n        \"\"\"\n\n        # It is unclear how to merge points in all cases. Consider a 10x10 graph,\n        # you crop out a 5x5 area, do a shift augment, and attempt to merge.\n        # What does that mean? specs have changed. It should be a new key.\n        raise NotImplementedError(\"Merge function should not be used!\")\n\n        self_roi = self.spec.roi\n        other_roi = other.spec.roi\n\n        assert self_roi.contains(other_roi) or other_roi.contains(\n            self_roi\n        ), \"Can not merge graphs that are not contained in each other.\"\n\n        # make sure self contains other\n        if not self_roi.contains(other_roi):\n            return other.merge(self, not copy_from_self, copy)\n\n        # edges and nodes in addition are guaranteed to be in merged\n        base = other if copy_from_self else self\n        addition = self if copy_from_self else other\n\n        if copy:\n            merged = base.copy()\n        else:\n            merged = base\n\n        for node in list(merged.nodes):\n            if merged.spec.roi.contains(node.location):\n                merged.remove_node(node)\n        for edge in list(merged.edges):\n            if merged.spec.roi.contains(\n                merged.node(edge.u)\n            ) or merged.spec.roi.contains(merged.node(edge.v)):\n                merged.remove_edge(edge)\n        for node in addition.nodes:\n            merged.add_node(node)\n        for edge in addition.edges:\n            merged.add_edge(edge)\n\n        return merged",
  "def to_nx_graph(self):\n        \"\"\"\n        returns a pure networkx graph containing data from\n        this Graph.\n        \"\"\"\n        return deepcopy(self.__graph)",
  "def from_nx_graph(cls, graph, spec):\n        \"\"\"\n        Create a gunpowder graph from a networkx graph.\n        The network graph is expected to have a \"location\"\n        attribute for each node. If it is a subclass of a networkx\n        graph with extra functionality, this may not work.\n        \"\"\"\n        if spec.directed is None:\n            spec.directed = graph.is_directed()\n        g = cls([], [], spec)\n        g.__graph = graph\n        return g",
  "def relabel_connected_components(self):\n        \"\"\"\n        create a new attribute \"component\" for each node\n        in this Graph\n        \"\"\"\n        for i, wcc in enumerate(self.connected_components):\n            for node in wcc:\n                self.__graph.nodes[node][\"component\"] = i",
  "def connected_components(self):\n        if not self.directed:\n            return nx.connected_components(self.__graph)\n        else:\n            return nx.weakly_connected_components(self.__graph)",
  "def in_degree(self):\n        return self.__graph.in_degree()",
  "def successors(self, node):\n        if self.directed:\n            return self.__graph.successors(node.id)\n        else:\n            return self.__graph.neighbors(node.id)",
  "def predecessors(self, node):\n        if self.directed:\n            return self.__graph.predecessors(node.id)\n        else:\n            return self.__graph.neighbors(node.id)",
  "def __init__(self, identifier):\n        self.identifier = identifier\n        self.hash = hash(identifier)\n        self.freeze()\n        logger.debug(\"Registering graph type %s\", self)\n        setattr(GraphKeys, self.identifier, self)",
  "def __eq__(self, other):\n        return hasattr(other, \"identifier\") and self.identifier == other.identifier",
  "def __hash__(self):\n        return self.hash",
  "def __repr__(self):\n        return self.identifier",
  "class NoResult(Exception):\n    pass",
  "class ParentDied(Exception):\n    pass",
  "class WorkersDied(Exception):\n    pass",
  "class ProducerPool(object):\n    def __init__(self, callables, queue_size=10):\n        self.__watch_dog = multiprocessing.Process(\n            target=self._run_watch_dog, args=(callables,)\n        )\n        self.__stop = multiprocessing.Event()\n        self.__result_queue = multiprocessing.Queue(queue_size)\n\n    def __del__(self):\n        self.stop()\n\n    def start(self):\n        \"\"\"Start the pool of producers.\"\"\"\n\n        if self.__watch_dog is None:\n            raise RuntimeError(\"can't start a ProducerPool a second time\")\n\n        if self.__watch_dog.is_alive():\n            logger.warning(\"trying to start workers, but they are already running\")\n            return\n\n        self.__stop.clear()\n        self.__watch_dog.start()\n\n    def get(self, timeout=0):\n        \"\"\"Return the next result from the producer pool.\n\n        If timeout is set and there is not result after the given number of\n        seconds, exception NoResult is raised.\n        \"\"\"\n\n        block = False\n        if timeout == 0:\n            timeout = 1\n            block = True\n\n        item = None\n        while item == None:\n            try:\n                item = self.__result_queue.get(timeout=timeout)\n            except Queue.Empty:\n                if not block:\n                    raise NoResult()\n\n        if isinstance(item, Exception):\n            raise item\n        return item\n\n    def stop(self):\n        \"\"\"Stop the pool of producers.\n\n        Items currently being produced will not be waited for and be discarded.\"\"\"\n\n        if self.__watch_dog is None:\n            return\n\n        self.__stop.set()\n        if self.__watch_dog._popen is not None:\n            self.__watch_dog.join()\n        self.__watch_dog = None\n\n    def _run_watch_dog(self, callables):\n        parent_pid = os.getppid()\n\n        logger.debug(\"watchdog started with PID \" + str(os.getpid()))\n        logger.debug(\"parent PID \" + str(parent_pid))\n\n        workers = [\n            multiprocessing.Process(target=self._run_worker, args=(c,))\n            for c in callables\n        ]\n\n        try:\n            logger.debug(\"starting %d workers\" % len(workers))\n            for worker in workers:\n                worker.start()\n\n            while not self.__stop.wait(1):\n                if os.getppid() != parent_pid:\n                    logger.error(\"parent of producer pool died, shutting down\")\n                    self.__result_queue.put(ParentDied())\n                    break\n                if not self._all_workers_alive(workers):\n                    logger.error(\"at least one of my workers died, shutting down\")\n                    self.__result_queue.put(WorkersDied())\n                    break\n        except:\n            pass\n\n        finally:\n            logger.info(\"terminating workers...\")\n            for worker in workers:\n                worker.terminate()\n\n            logger.info(\"joining workers...\")\n            for worker in workers:\n                worker.join()\n\n            logger.info(\"done\")\n\n    def _run_worker(self, target):\n        parent_pid = os.getppid()\n\n        logger.debug(\"worker started with PID \" + str(os.getpid()))\n        logger.debug(\"parent PID \" + str(parent_pid))\n\n        result = None\n        np.random.seed(None)\n        while True:\n            if os.getppid() != parent_pid:\n                logger.debug(\"worker %d: watch-dog died, stopping\" % os.getpid())\n                break\n\n            if result is None:\n                try:\n                    result = target()\n                except Exception as e:\n                    logger.error(e, exc_info=True)\n                    result = e\n                    traceback.print_exc()\n                    # don't stop on normal exceptions -- place them in result queue\n                    # and let them be handled by caller\n                except:\n                    logger.error(\"received error: \" + str(sys.exc_info()[0]))\n                    # this is most likely a keyboard interrupt, stop process\n                    break\n\n            try:\n                self.__result_queue.put(result, timeout=1)\n                result = None\n            except Queue.Full:\n                logger.debug(\n                    \"worker %d: result queue is full, waiting to place my result\"\n                    % os.getpid()\n                )\n\n        logger.debug(\"worker with PID \" + str(os.getpid()) + \" exiting\")\n        os._exit(1)\n\n    def _all_workers_alive(self, workers):\n        return all([worker.is_alive() for worker in workers])",
  "def __init__(self, callables, queue_size=10):\n        self.__watch_dog = multiprocessing.Process(\n            target=self._run_watch_dog, args=(callables,)\n        )\n        self.__stop = multiprocessing.Event()\n        self.__result_queue = multiprocessing.Queue(queue_size)",
  "def __del__(self):\n        self.stop()",
  "def start(self):\n        \"\"\"Start the pool of producers.\"\"\"\n\n        if self.__watch_dog is None:\n            raise RuntimeError(\"can't start a ProducerPool a second time\")\n\n        if self.__watch_dog.is_alive():\n            logger.warning(\"trying to start workers, but they are already running\")\n            return\n\n        self.__stop.clear()\n        self.__watch_dog.start()",
  "def get(self, timeout=0):\n        \"\"\"Return the next result from the producer pool.\n\n        If timeout is set and there is not result after the given number of\n        seconds, exception NoResult is raised.\n        \"\"\"\n\n        block = False\n        if timeout == 0:\n            timeout = 1\n            block = True\n\n        item = None\n        while item == None:\n            try:\n                item = self.__result_queue.get(timeout=timeout)\n            except Queue.Empty:\n                if not block:\n                    raise NoResult()\n\n        if isinstance(item, Exception):\n            raise item\n        return item",
  "def stop(self):\n        \"\"\"Stop the pool of producers.\n\n        Items currently being produced will not be waited for and be discarded.\"\"\"\n\n        if self.__watch_dog is None:\n            return\n\n        self.__stop.set()\n        if self.__watch_dog._popen is not None:\n            self.__watch_dog.join()\n        self.__watch_dog = None",
  "def _run_watch_dog(self, callables):\n        parent_pid = os.getppid()\n\n        logger.debug(\"watchdog started with PID \" + str(os.getpid()))\n        logger.debug(\"parent PID \" + str(parent_pid))\n\n        workers = [\n            multiprocessing.Process(target=self._run_worker, args=(c,))\n            for c in callables\n        ]\n\n        try:\n            logger.debug(\"starting %d workers\" % len(workers))\n            for worker in workers:\n                worker.start()\n\n            while not self.__stop.wait(1):\n                if os.getppid() != parent_pid:\n                    logger.error(\"parent of producer pool died, shutting down\")\n                    self.__result_queue.put(ParentDied())\n                    break\n                if not self._all_workers_alive(workers):\n                    logger.error(\"at least one of my workers died, shutting down\")\n                    self.__result_queue.put(WorkersDied())\n                    break\n        except:\n            pass\n\n        finally:\n            logger.info(\"terminating workers...\")\n            for worker in workers:\n                worker.terminate()\n\n            logger.info(\"joining workers...\")\n            for worker in workers:\n                worker.join()\n\n            logger.info(\"done\")",
  "def _run_worker(self, target):\n        parent_pid = os.getppid()\n\n        logger.debug(\"worker started with PID \" + str(os.getpid()))\n        logger.debug(\"parent PID \" + str(parent_pid))\n\n        result = None\n        np.random.seed(None)\n        while True:\n            if os.getppid() != parent_pid:\n                logger.debug(\"worker %d: watch-dog died, stopping\" % os.getpid())\n                break\n\n            if result is None:\n                try:\n                    result = target()\n                except Exception as e:\n                    logger.error(e, exc_info=True)\n                    result = e\n                    traceback.print_exc()\n                    # don't stop on normal exceptions -- place them in result queue\n                    # and let them be handled by caller\n                except:\n                    logger.error(\"received error: \" + str(sys.exc_info()[0]))\n                    # this is most likely a keyboard interrupt, stop process\n                    break\n\n            try:\n                self.__result_queue.put(result, timeout=1)\n                result = None\n            except Queue.Full:\n                logger.debug(\n                    \"worker %d: result queue is full, waiting to place my result\"\n                    % os.getpid()\n                )\n\n        logger.debug(\"worker with PID \" + str(os.getpid()) + \" exiting\")\n        os._exit(1)",
  "def _all_workers_alive(self, workers):\n        return all([worker.is_alive() for worker in workers])",
  "class Timing(Freezable):\n    def __init__(self, node, method_name=None):\n        self.__name = type(node).__name__\n        self.__method_name = method_name\n        self.__start = 0\n        self.__first_start = 0\n        self.__last_stop = 0\n        self.__time = 0\n        self.freeze()\n\n    def start(self):\n        self.__start = time.time()\n        if self.__first_start == 0:\n            self.__first_start = self.__start\n\n    def stop(self):\n        if self.__start == 0:\n            return\n        t = time.time()\n        self.__time += t - self.__start\n        self.__start = 0\n        self.__last_stop = t\n\n    def elapsed(self):\n        \"\"\"Accumulated time elapsed between calls to start() and stop().\"\"\"\n\n        if self.__start == 0:\n            return self.__time\n\n        return self.__time + (time.time() - self.__start)\n\n    def span(self):\n        \"\"\"Timestamps of the first call to start() and last call to stop().\"\"\"\n        return self.__first_start, self.__last_stop\n\n    def get_node_name(self):\n        return self.__name\n\n    def get_method_name(self):\n        return self.__method_name",
  "class TimingSummary(Freezable):\n    \"\"\"Holds repeated Timings of the same node/method to be queried for statistics.\"\"\"\n\n    def __init__(self):\n        self.timings = []\n        self.times = []\n        self.freeze()\n\n    def add(self, timing):\n        \"\"\"Add a Timing to this summary.\"\"\"\n        self.timings.append(timing)\n        self.times.append(timing.elapsed())\n\n    def merge(self, other):\n        \"\"\"Merge another summary into this one.\"\"\"\n        for timing in other.timings:\n            self.add(timing)\n\n    def counts(self):\n        return len(self.times)\n\n    def min(self):\n        return np.min(self.times)\n\n    def max(self):\n        return np.max(self.times)\n\n    def mean(self):\n        return np.mean(self.times)\n\n    def median(self):\n        return np.median(self.times)",
  "class ProfilingStats(Freezable):\n    def __init__(self):\n        self.__summaries = {}\n        self.freeze()\n\n    def add(self, timing):\n        \"\"\"Add a Timing instance. Timings are grouped by their class and method names.\"\"\"\n\n        node_name = timing.get_node_name()\n        method_name = timing.get_method_name()\n        id = (node_name, method_name)\n\n        if id not in self.__summaries:\n            self.__summaries[id] = TimingSummary()\n        self.__summaries[id].add(copy.deepcopy(timing))\n\n    def merge_with(self, other):\n        \"\"\"Combine statitics of two ProfilingStats.\"\"\"\n\n        for id, summary in other.__summaries.items():\n            if id in self.__summaries:\n                self.__summaries[id].merge(copy.deepcopy(summary))\n            else:\n                self.__summaries[id] = copy.deepcopy(summary)\n\n    def get_timing_summaries(self):\n        \"\"\"Get a dictionary (node_name,method_name) -> TimingSummary.\"\"\"\n        return self.__summaries\n\n    def get_timing_summary(self, node_name, method_name=None):\n        \"\"\"Get a :class:`TimingSummary` for the given node and method name.\"\"\"\n\n        if (node_name, method_name) not in self.__summaries:\n            raise RuntimeError(\n                \"No timing summary for node %s, method %s\" % (node_name, method_name)\n            )\n\n        return self.__summaries[(node_name, method_name)]\n\n    def span(self):\n        \"\"\"Timestamps of the first call to start() and last call to stop() over\n        all Timings added.\"\"\"\n\n        spans = [\n            t.span()\n            for (_, summary) in self.__summaries.items()\n            for t in summary.timings\n        ]\n        first_start = min([span[0] for span in spans])\n        last_stop = max([span[1] for span in spans])\n\n        return first_start, last_stop\n\n    def span_time(self):\n        \"\"\"Time between the first call to start() and last call to stop() over\n        any timing.\"\"\"\n\n        start, stop = self.span()\n        return stop - start",
  "def __init__(self, node, method_name=None):\n        self.__name = type(node).__name__\n        self.__method_name = method_name\n        self.__start = 0\n        self.__first_start = 0\n        self.__last_stop = 0\n        self.__time = 0\n        self.freeze()",
  "def start(self):\n        self.__start = time.time()\n        if self.__first_start == 0:\n            self.__first_start = self.__start",
  "def stop(self):\n        if self.__start == 0:\n            return\n        t = time.time()\n        self.__time += t - self.__start\n        self.__start = 0\n        self.__last_stop = t",
  "def elapsed(self):\n        \"\"\"Accumulated time elapsed between calls to start() and stop().\"\"\"\n\n        if self.__start == 0:\n            return self.__time\n\n        return self.__time + (time.time() - self.__start)",
  "def span(self):\n        \"\"\"Timestamps of the first call to start() and last call to stop().\"\"\"\n        return self.__first_start, self.__last_stop",
  "def get_node_name(self):\n        return self.__name",
  "def get_method_name(self):\n        return self.__method_name",
  "def __init__(self):\n        self.timings = []\n        self.times = []\n        self.freeze()",
  "def add(self, timing):\n        \"\"\"Add a Timing to this summary.\"\"\"\n        self.timings.append(timing)\n        self.times.append(timing.elapsed())",
  "def merge(self, other):\n        \"\"\"Merge another summary into this one.\"\"\"\n        for timing in other.timings:\n            self.add(timing)",
  "def counts(self):\n        return len(self.times)",
  "def min(self):\n        return np.min(self.times)",
  "def max(self):\n        return np.max(self.times)",
  "def mean(self):\n        return np.mean(self.times)",
  "def median(self):\n        return np.median(self.times)",
  "def __init__(self):\n        self.__summaries = {}\n        self.freeze()",
  "def add(self, timing):\n        \"\"\"Add a Timing instance. Timings are grouped by their class and method names.\"\"\"\n\n        node_name = timing.get_node_name()\n        method_name = timing.get_method_name()\n        id = (node_name, method_name)\n\n        if id not in self.__summaries:\n            self.__summaries[id] = TimingSummary()\n        self.__summaries[id].add(copy.deepcopy(timing))",
  "def merge_with(self, other):\n        \"\"\"Combine statitics of two ProfilingStats.\"\"\"\n\n        for id, summary in other.__summaries.items():\n            if id in self.__summaries:\n                self.__summaries[id].merge(copy.deepcopy(summary))\n            else:\n                self.__summaries[id] = copy.deepcopy(summary)",
  "def get_timing_summaries(self):\n        \"\"\"Get a dictionary (node_name,method_name) -> TimingSummary.\"\"\"\n        return self.__summaries",
  "def get_timing_summary(self, node_name, method_name=None):\n        \"\"\"Get a :class:`TimingSummary` for the given node and method name.\"\"\"\n\n        if (node_name, method_name) not in self.__summaries:\n            raise RuntimeError(\n                \"No timing summary for node %s, method %s\" % (node_name, method_name)\n            )\n\n        return self.__summaries[(node_name, method_name)]",
  "def span(self):\n        \"\"\"Timestamps of the first call to start() and last call to stop() over\n        all Timings added.\"\"\"\n\n        spans = [\n            t.span()\n            for (_, summary) in self.__summaries.items()\n            for t in summary.timings\n        ]\n        first_start = min([span[0] for span in spans])\n        last_stop = max([span[1] for span in spans])\n\n        return first_start, last_stop",
  "def span_time(self):\n        \"\"\"Time between the first call to start() and last call to stop() over\n        any timing.\"\"\"\n\n        start, stop = self.span()\n        return stop - start",
  "class ArraySpec(Freezable):\n    \"\"\"Contains meta-information about an array. This is used by\n    :class:`BatchProviders<BatchProvider>` to communicate the arrays they\n    offer, as well as by :class:`Arrays<Array>` to describe the data they\n    contain.\n\n    Attributes:\n\n        roi (:class:`Roi`):\n\n            The region of interested represented by this array spec. Can be\n            ``None`` for :class:`BatchProviders<BatchProvider>` that allow\n            requests for arrays everywhere, but will always be set for array\n            specs that are part of a :class:`Array`.\n\n        voxel_size (:class:`Coordinate`):\n\n            The size of the spatial axises in world units.\n\n        interpolatable (``bool``):\n\n            Whether the values of this array can be interpolated.\n\n        nonspatial (``bool``, optional):\n\n            If set, this array does not represent spatial data (e.g., a list of\n            labels for samples in a batch). ``roi`` and ``voxel_size`` have to\n            be ``None``. No consistency checks will be performed.\n\n        dtype (``np.dtype``):\n\n            The data type of the array.\n    \"\"\"\n\n    def __init__(\n        self,\n        roi=None,\n        voxel_size=None,\n        interpolatable=None,\n        nonspatial=False,\n        dtype=None,\n        placeholder=False,\n    ):\n        self.roi = roi\n        self.voxel_size = None if voxel_size is None else Coordinate(voxel_size)\n        self.interpolatable = interpolatable\n        self.nonspatial = nonspatial\n        self.dtype = dtype\n        self.placeholder = placeholder\n\n        if nonspatial:\n            assert roi is None, \"Non-spatial arrays can not have a ROI\"\n            assert voxel_size is None, \"Non-spatial arrays can not \" \"have a voxel size\"\n\n        self.freeze()\n\n    def update_with(self, spec):\n        if self.roi is not None and spec.roi is not None:\n            self.roi = self.roi.union(spec.roi)\n        elif spec.roi is not None:\n            self.roi = spec.roi\n\n        if spec.voxel_size is not None:\n            self.voxel_size = spec.voxel_size\n\n        if spec.interpolatable is not None:\n            self.interpolatable = spec.interpolatable\n\n        if spec.nonspatial is not None:\n            self.nonspatial = spec.nonspatial\n\n        if spec.dtype is not None:\n            self.dtype = spec.dtype\n\n        if spec.placeholder is not None:\n            self.placeholder = spec.placeholder\n\n    def copy(self):\n        \"\"\"Create a copy of this spec.\"\"\"\n        return copy.deepcopy(self)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented\n\n    def __repr__(self):\n        r = \"\"\n        r += \"ROI: \" + str(self.roi) + \", \"\n        r += \"voxel size: \" + str(self.voxel_size) + \", \"\n        r += \"interpolatable: \" + str(self.interpolatable) + \", \"\n        r += \"non-spatial: \" + str(self.nonspatial) + \", \"\n        r += \"dtype: \" + str(self.dtype) + \", \"\n        r += \"placeholder: \" + str(self.placeholder)\n        return r",
  "def __init__(\n        self,\n        roi=None,\n        voxel_size=None,\n        interpolatable=None,\n        nonspatial=False,\n        dtype=None,\n        placeholder=False,\n    ):\n        self.roi = roi\n        self.voxel_size = None if voxel_size is None else Coordinate(voxel_size)\n        self.interpolatable = interpolatable\n        self.nonspatial = nonspatial\n        self.dtype = dtype\n        self.placeholder = placeholder\n\n        if nonspatial:\n            assert roi is None, \"Non-spatial arrays can not have a ROI\"\n            assert voxel_size is None, \"Non-spatial arrays can not \" \"have a voxel size\"\n\n        self.freeze()",
  "def update_with(self, spec):\n        if self.roi is not None and spec.roi is not None:\n            self.roi = self.roi.union(spec.roi)\n        elif spec.roi is not None:\n            self.roi = spec.roi\n\n        if spec.voxel_size is not None:\n            self.voxel_size = spec.voxel_size\n\n        if spec.interpolatable is not None:\n            self.interpolatable = spec.interpolatable\n\n        if spec.nonspatial is not None:\n            self.nonspatial = spec.nonspatial\n\n        if spec.dtype is not None:\n            self.dtype = spec.dtype\n\n        if spec.placeholder is not None:\n            self.placeholder = spec.placeholder",
  "def copy(self):\n        \"\"\"Create a copy of this spec.\"\"\"\n        return copy.deepcopy(self)",
  "def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented",
  "def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented",
  "def __repr__(self):\n        r = \"\"\n        r += \"ROI: \" + str(self.roi) + \", \"\n        r += \"voxel size: \" + str(self.voxel_size) + \", \"\n        r += \"interpolatable: \" + str(self.interpolatable) + \", \"\n        r += \"non-spatial: \" + str(self.nonspatial) + \", \"\n        r += \"dtype: \" + str(self.dtype) + \", \"\n        r += \"placeholder: \" + str(self.placeholder)\n        return r",
  "def ensure_str(s):\n    if PY2:\n        if isinstance(s, buffer):\n            s = str(s)\n    else:\n        if isinstance(s, memoryview):\n            s = s.tobytes()\n        if isinstance(s, binary_type):\n            s = s.decode(\"ascii\")\n    return s",
  "class GraphSpec(Freezable):\n    \"\"\"Contains meta-information about a graph. This is used by\n    :class:`BatchProviders<BatchProvider>` to communicate the graphs they\n    offer, as well as by :class:`Graph` to describe the data they contain.\n\n    Attributes:\n\n        roi (:class:`Roi`):\n\n            The region of interested represented by this graph.\n\n        directed (``bool``, optional):\n\n            Whether the graph is directed or not.\n\n        dtype (``dtype``, optional):\n\n            The data type of the \"location\" attribute.\n            Currently only supports np.float32.\n    \"\"\"\n\n    def __init__(self, roi=None, directed=None, dtype=np.float32, placeholder=False):\n        self.roi = roi\n        self.directed = directed\n        self.dtype = dtype\n        self.placeholder = placeholder\n\n        self.freeze()\n\n    def update_with(self, spec):\n        if self.roi is not None and spec.roi is not None:\n            self.roi = self.roi.union(spec.roi)\n        elif spec.roi is not None:\n            self.roi = spec.roi\n\n        if spec.directed is not None:\n            self.directed = spec.directed\n\n        if spec.dtype is not None:\n            self.dtype = spec.dtype\n\n        if spec.placeholder is not None:\n            self.placeholder = spec.placeholder\n\n    def copy(self):\n        \"\"\"Create a copy of this spec.\"\"\"\n        return copy.deepcopy(self)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented\n\n    def __repr__(self):\n        r = \"\"\n        r += \"ROI: \" + str(self.roi) + \", \"\n        r += \"dtype: \" + str(self.dtype) + \", \"\n        r += \"directed: \" + str(self.directed) + \", \"\n        r += \"placeholder: \" + str(self.placeholder)\n        return r",
  "def __init__(self, roi=None, directed=None, dtype=np.float32, placeholder=False):\n        self.roi = roi\n        self.directed = directed\n        self.dtype = dtype\n        self.placeholder = placeholder\n\n        self.freeze()",
  "def update_with(self, spec):\n        if self.roi is not None and spec.roi is not None:\n            self.roi = self.roi.union(spec.roi)\n        elif spec.roi is not None:\n            self.roi = spec.roi\n\n        if spec.directed is not None:\n            self.directed = spec.directed\n\n        if spec.dtype is not None:\n            self.dtype = spec.dtype\n\n        if spec.placeholder is not None:\n            self.placeholder = spec.placeholder",
  "def copy(self):\n        \"\"\"Create a copy of this spec.\"\"\"\n        return copy.deepcopy(self)",
  "def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return NotImplemented",
  "def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented",
  "def __repr__(self):\n        r = \"\"\n        r += \"ROI: \" + str(self.roi) + \", \"\n        r += \"dtype: \" + str(self.dtype) + \", \"\n        r += \"directed: \" + str(self.directed) + \", \"\n        r += \"placeholder: \" + str(self.placeholder)\n        return r",
  "class Batch(Freezable):\n    \"\"\"Contains the requested batch as a collection of :class:`Arrays<Array>`\n    and :class:`Graph` that is passed through the pipeline from sources to\n    sinks.\n\n    This collection mimics a dictionary. Items can be added with::\n\n        batch = Batch()\n        batch[array_key] = Array(...)\n        batch[graph_key] = Graph(...)\n\n    Here, ``array_key`` and ``graph_key`` are :class:`ArrayKey` and\n    :class:`GraphKey`. The items can be queried with::\n\n        array = batch[array_key]\n        graph = batch[graph_key]\n\n    Furthermore, pairs of keys/values can be iterated over using\n    ``batch.items()``.\n\n    To access only arrays or graphs, use the dictionaries ``batch.arrays``\n    or ``batch.graphs``, respectively.\n\n    Attributes:\n\n        arrays (dict from :class:`ArrayKey` to :class:`Array`):\n\n            Contains all arrays that have been requested for this batch.\n\n        graphs (dict from :class:`GraphKey` to :class:`Graph`):\n\n            Contains all graphs that have been requested for this batch.\n    \"\"\"\n\n    __next_id = multiprocessing.Value(\"L\")\n\n    @staticmethod\n    def get_next_id():\n        with Batch.__next_id.get_lock():\n            next_id = Batch.__next_id.value\n            Batch.__next_id.value += 1\n        return next_id\n\n    def __init__(self):\n        self.id = Batch.get_next_id()\n        self.profiling_stats = ProfilingStats()\n        self.arrays = {}\n        self.graphs = {}\n        self.affinity_neighborhood = None\n        self.loss = None\n        self.iteration = None\n\n        self.freeze()\n\n    def __setitem__(self, key, value):\n        if isinstance(value, Array):\n            assert isinstance(\n                key, ArrayKey\n            ), \"Only a ArrayKey is allowed as key for an Array value.\"\n            self.arrays[key] = value\n\n        elif isinstance(value, Graph):\n            assert isinstance(\n                key, GraphKey\n            ), f\"Only a GraphKey is allowed as key for Graph value.\"\n            self.graphs[key] = value\n\n        else:\n            raise RuntimeError(\n                \"Only Array or Graph can be set in a %s.\" % type(self).__name__\n            )\n\n    def __getitem__(self, key):\n        if isinstance(key, ArrayKey):\n            return self.arrays[key]\n\n        elif isinstance(key, GraphKey):\n            return self.graphs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )\n\n    def __len__(self):\n        return len(self.arrays) + len(self.graphs)\n\n    def __contains__(self, key):\n        if isinstance(key, ArrayKey):\n            return key in self.arrays\n\n        elif isinstance(key, GraphKey):\n            return key in self.graphs\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s. Key %s is a %s\" % (type(self).__name__, key, type(key).__name__)\n            )\n\n    def __delitem__(self, key):\n        if isinstance(key, ArrayKey):\n            del self.arrays[key]\n\n        elif isinstance(key, GraphKey):\n            del self.graphs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )\n\n    def items(self):\n        \"\"\"Provides a generator iterating over key/value pairs.\"\"\"\n\n        for k, v in self.arrays.items():\n            yield k, v\n        for k, v in self.graphs.items():\n            yield k, v\n\n    def get_total_roi(self):\n        \"\"\"Get the union of all the array ROIs in the batch.\"\"\"\n\n        total_roi = None\n\n        for _, array in self.arrays.items():\n            if not array.spec.nonspatial:\n                if total_roi is None:\n                    total_roi = array.spec.roi\n                else:\n                    total_roi = total_roi.union(array.spec.roi)\n\n        for _, graph in self.graphs.items():\n            if total_roi is None:\n                total_roi = graph.spec.roi\n            else:\n                total_roi = total_roi.union(graph.spec.roi)\n\n        return total_roi\n\n    def __repr__(self):\n        r = \"\\n\"\n        for collection_type in [self.arrays, self.graphs]:\n            for key, obj in collection_type.items():\n                r += \"\\t%s: %s\\n\" % (key, obj.spec)\n        return r\n\n    def crop(self, request, copy=False):\n        \"\"\"Crop batch to meet the given request.\"\"\"\n\n        cropped = Batch()\n        cropped.profiling_stats = self.profiling_stats\n        cropped.loss = self.loss\n        cropped.iteration = self.iteration\n\n        for key, val in request.items():\n            assert key in self, \"%s not contained in this batch\" % key\n            if val.roi is None:\n                cropped[key] = self[key]\n            else:\n                if isinstance(key, GraphKey):\n                    cropped[key] = self[key].crop(val.roi)\n                else:\n                    cropped[key] = self[key].crop(val.roi, copy)\n\n        return cropped\n\n    def merge(self, batch, merge_profiling_stats=True):\n        \"\"\"Merge this batch (``a``) with another batch (``b``).\n\n        This creates a new batch ``c`` containing arrays and graphs from\n        both batches ``a`` and ``b``:\n\n            * Arrays or Graphs that exist in either ``a`` or ``b`` will be\n              referenced in ``c`` (not copied).\n\n            * Arrays or Graphs that exist in both batches will keep only\n              a reference to the version in ``b`` in ``c``.\n\n        All other cases will lead to an exception.\n        \"\"\"\n\n        merged = shallow_copy(self)\n\n        for key, val in batch.items():\n            # TODO: What is the goal of `val.spec.roi is None`? Why should that\n            # mean that the key in merged gets overwritten?\n            if key not in merged or val.spec.roi is None:\n                merged[key] = val\n            elif key in merged:\n                merged[key] = val\n\n        if merge_profiling_stats:\n            merged.profiling_stats.merge_with(batch.profiling_stats)\n        if batch.loss is not None:\n            merged.loss = batch.loss\n        if batch.iteration is not None:\n            merged.iteration = batch.iteration\n\n        return merged",
  "def get_next_id():\n        with Batch.__next_id.get_lock():\n            next_id = Batch.__next_id.value\n            Batch.__next_id.value += 1\n        return next_id",
  "def __init__(self):\n        self.id = Batch.get_next_id()\n        self.profiling_stats = ProfilingStats()\n        self.arrays = {}\n        self.graphs = {}\n        self.affinity_neighborhood = None\n        self.loss = None\n        self.iteration = None\n\n        self.freeze()",
  "def __setitem__(self, key, value):\n        if isinstance(value, Array):\n            assert isinstance(\n                key, ArrayKey\n            ), \"Only a ArrayKey is allowed as key for an Array value.\"\n            self.arrays[key] = value\n\n        elif isinstance(value, Graph):\n            assert isinstance(\n                key, GraphKey\n            ), f\"Only a GraphKey is allowed as key for Graph value.\"\n            self.graphs[key] = value\n\n        else:\n            raise RuntimeError(\n                \"Only Array or Graph can be set in a %s.\" % type(self).__name__\n            )",
  "def __getitem__(self, key):\n        if isinstance(key, ArrayKey):\n            return self.arrays[key]\n\n        elif isinstance(key, GraphKey):\n            return self.graphs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )",
  "def __len__(self):\n        return len(self.arrays) + len(self.graphs)",
  "def __contains__(self, key):\n        if isinstance(key, ArrayKey):\n            return key in self.arrays\n\n        elif isinstance(key, GraphKey):\n            return key in self.graphs\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s. Key %s is a %s\" % (type(self).__name__, key, type(key).__name__)\n            )",
  "def __delitem__(self, key):\n        if isinstance(key, ArrayKey):\n            del self.arrays[key]\n\n        elif isinstance(key, GraphKey):\n            del self.graphs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )",
  "def items(self):\n        \"\"\"Provides a generator iterating over key/value pairs.\"\"\"\n\n        for k, v in self.arrays.items():\n            yield k, v\n        for k, v in self.graphs.items():\n            yield k, v",
  "def get_total_roi(self):\n        \"\"\"Get the union of all the array ROIs in the batch.\"\"\"\n\n        total_roi = None\n\n        for _, array in self.arrays.items():\n            if not array.spec.nonspatial:\n                if total_roi is None:\n                    total_roi = array.spec.roi\n                else:\n                    total_roi = total_roi.union(array.spec.roi)\n\n        for _, graph in self.graphs.items():\n            if total_roi is None:\n                total_roi = graph.spec.roi\n            else:\n                total_roi = total_roi.union(graph.spec.roi)\n\n        return total_roi",
  "def __repr__(self):\n        r = \"\\n\"\n        for collection_type in [self.arrays, self.graphs]:\n            for key, obj in collection_type.items():\n                r += \"\\t%s: %s\\n\" % (key, obj.spec)\n        return r",
  "def crop(self, request, copy=False):\n        \"\"\"Crop batch to meet the given request.\"\"\"\n\n        cropped = Batch()\n        cropped.profiling_stats = self.profiling_stats\n        cropped.loss = self.loss\n        cropped.iteration = self.iteration\n\n        for key, val in request.items():\n            assert key in self, \"%s not contained in this batch\" % key\n            if val.roi is None:\n                cropped[key] = self[key]\n            else:\n                if isinstance(key, GraphKey):\n                    cropped[key] = self[key].crop(val.roi)\n                else:\n                    cropped[key] = self[key].crop(val.roi, copy)\n\n        return cropped",
  "def merge(self, batch, merge_profiling_stats=True):\n        \"\"\"Merge this batch (``a``) with another batch (``b``).\n\n        This creates a new batch ``c`` containing arrays and graphs from\n        both batches ``a`` and ``b``:\n\n            * Arrays or Graphs that exist in either ``a`` or ``b`` will be\n              referenced in ``c`` (not copied).\n\n            * Arrays or Graphs that exist in both batches will keep only\n              a reference to the version in ``b`` in ``c``.\n\n        All other cases will lead to an exception.\n        \"\"\"\n\n        merged = shallow_copy(self)\n\n        for key, val in batch.items():\n            # TODO: What is the goal of `val.spec.roi is None`? Why should that\n            # mean that the key in merged gets overwritten?\n            if key not in merged or val.spec.roi is None:\n                merged[key] = val\n            elif key in merged:\n                merged[key] = val\n\n        if merge_profiling_stats:\n            merged.profiling_stats.merge_with(batch.profiling_stats)\n        if batch.loss is not None:\n            merged.loss = batch.loss\n        if batch.iteration is not None:\n            merged.iteration = batch.iteration\n\n        return merged",
  "class PipelineSetupError(Exception):\n    def __init__(self, provider):\n        self.provider = provider\n\n    def __str__(self):\n        return f\"Exception in {self.provider.name()} while calling setup()\"",
  "class PipelineTeardownError(Exception):\n    def __init__(self, provider):\n        self.provider = provider\n\n    def __str__(self):\n        return f\"Exception in {self.provider.name()} while calling teardown()\"",
  "class PipelineRequestError(Exception):\n    def __init__(self, pipeline, request):\n        self.pipeline = pipeline\n        self.request = request\n\n    def __str__(self):\n        return (\n            \"Exception in pipeline:\\n\"\n            f\"{self.pipeline}\\n\"\n            \"while trying to process request\\n\"\n            f\"{self.request}\"\n        )",
  "class Pipeline:\n    def __init__(self, node):\n        \"\"\"Create a pipeline from a single :class:`BatchProvider`.\"\"\"\n\n        assert isinstance(node, BatchProvider), f\"{type(node)} is not a BatchProvider\"\n\n        self.output = node\n        self.children = []\n        self.initialized = False\n\n    def traverse(self, callback, reverse=False):\n        \"\"\"Visit every node in the pipeline recursively (either from root to\n        leaves of from leaves to the root if ``reverse`` is true). ``callback``\n        will be called for each node encountered.\"\"\"\n\n        result = []\n\n        if not reverse:\n            result.append(callback(self))\n        for child in self.children:\n            result.append(child.traverse(callback, reverse))\n        if reverse:\n            result.append(callback(self))\n\n        return result\n\n    def copy(self):\n        \"\"\"Make a shallow copy of the pipeline.\"\"\"\n\n        pipeline = Pipeline(self.output)\n        pipeline.children = [c.copy() for c in self.children]\n\n        return pipeline\n\n    def setup(self):\n        \"\"\"Connect all batch providers in the pipeline and call setup for\n        each, from source to sink.\"\"\"\n\n        def connect(node):\n            for child in node.children:\n                node.output.add_upstream_provider(child.output)\n\n        # connect all nodes\n        self.traverse(connect)\n\n        # call setup on all nodes\n        if not self.initialized:\n\n            def node_setup(node):\n                try:\n                    node.output.setup()\n                except Exception as e:\n                    raise PipelineSetupError(node.output) from e\n\n            self.traverse(node_setup, reverse=True)\n            self.initialized = True\n        else:\n            logger.warning(\n                \"pipeline.setup() called more than once (build() inside \" \"build()?)\"\n            )\n\n    def internal_teardown(self):\n        \"\"\"Call teardown on each batch provider in the pipeline and disconnect\n        all nodes.\"\"\"\n\n        try:\n\n            def node_teardown(node):\n                try:\n                    node.output.internal_teardown()\n                except Exception as e:\n                    raise PipelineTeardownError(node.output) from e\n\n            # call internal_teardown on all nodes\n            self.traverse(node_teardown, reverse=True)\n            self.initialized = False\n\n        finally:\n            # disconnect all nodes\n            def disconnect(node):\n                node.output.remove_upstream_providers()\n\n            self.traverse(disconnect)\n\n    def request_batch(self, request):\n        \"\"\"Request a batch from the pipeline.\"\"\"\n\n        try:\n            return self.output.request_batch(request)\n        except Exception as e:\n            raise PipelineRequestError(self, request) from e\n\n    @property\n    def spec(self):\n        return self.output.spec\n\n    def __add__(self, other):\n        if isinstance(other, BatchProvider):\n            other = Pipeline(other)\n\n        if isinstance(other, Pipeline):\n            result = other.copy()\n\n            # add this pipeline as child to all leaves in other\n            def add_self_to_leaves(node):\n                if len(node.children) == 0:\n                    node.children.append(self.copy())\n\n            result.traverse(add_self_to_leaves, reverse=True)\n\n        else:\n            raise RuntimeError(f\"Don't know how to add {type(other)} to Pipeline\")\n\n        return result\n\n    def __radd__(self, other):\n        assert isinstance(\n            other, tuple\n        ), f\"Don't know how to radd {type(other)} to Pipeline\"\n\n        for o in other:\n            assert isinstance(o, Pipeline) or isinstance(\n                o, BatchProvider\n            ), f\"Don't know how to radd {type(o)} to Pipeline\"\n\n        other = tuple(\n            Pipeline(o) if isinstance(o, BatchProvider) else o.copy() for o in other\n        )\n\n        result = self.copy()\n\n        # add every other as child to leaves in pipeline\n        def add_others_to_leaves(node):\n            if len(node.children) == 0:\n                for o in other:\n                    node.children.append(o)\n\n        result.traverse(add_others_to_leaves, reverse=True)\n\n        return result\n\n    def __repr__(self):\n        def to_string(node):\n            return node.output.name()\n\n        reprs = self.traverse(to_string, reverse=True)\n\n        return self.__rec_repr__(reprs)\n\n    def __rec_repr__(self, reprs):\n        if not isinstance(reprs, list):\n            return str(reprs)\n\n        num_children = len(reprs) - 1\n\n        res = \"\"\n        if num_children > 0:\n            if num_children > 1:\n                res = \"(\"\n            res += \", \".join(self.__rec_repr__(r) for r in reprs[:-1])\n        if num_children > 0:\n            if num_children > 1:\n                res += \")\"\n            res += \" -> \"\n        res += reprs[-1]\n        return res",
  "def __init__(self, provider):\n        self.provider = provider",
  "def __str__(self):\n        return f\"Exception in {self.provider.name()} while calling setup()\"",
  "def __init__(self, provider):\n        self.provider = provider",
  "def __str__(self):\n        return f\"Exception in {self.provider.name()} while calling teardown()\"",
  "def __init__(self, pipeline, request):\n        self.pipeline = pipeline\n        self.request = request",
  "def __str__(self):\n        return (\n            \"Exception in pipeline:\\n\"\n            f\"{self.pipeline}\\n\"\n            \"while trying to process request\\n\"\n            f\"{self.request}\"\n        )",
  "def __init__(self, node):\n        \"\"\"Create a pipeline from a single :class:`BatchProvider`.\"\"\"\n\n        assert isinstance(node, BatchProvider), f\"{type(node)} is not a BatchProvider\"\n\n        self.output = node\n        self.children = []\n        self.initialized = False",
  "def traverse(self, callback, reverse=False):\n        \"\"\"Visit every node in the pipeline recursively (either from root to\n        leaves of from leaves to the root if ``reverse`` is true). ``callback``\n        will be called for each node encountered.\"\"\"\n\n        result = []\n\n        if not reverse:\n            result.append(callback(self))\n        for child in self.children:\n            result.append(child.traverse(callback, reverse))\n        if reverse:\n            result.append(callback(self))\n\n        return result",
  "def copy(self):\n        \"\"\"Make a shallow copy of the pipeline.\"\"\"\n\n        pipeline = Pipeline(self.output)\n        pipeline.children = [c.copy() for c in self.children]\n\n        return pipeline",
  "def setup(self):\n        \"\"\"Connect all batch providers in the pipeline and call setup for\n        each, from source to sink.\"\"\"\n\n        def connect(node):\n            for child in node.children:\n                node.output.add_upstream_provider(child.output)\n\n        # connect all nodes\n        self.traverse(connect)\n\n        # call setup on all nodes\n        if not self.initialized:\n\n            def node_setup(node):\n                try:\n                    node.output.setup()\n                except Exception as e:\n                    raise PipelineSetupError(node.output) from e\n\n            self.traverse(node_setup, reverse=True)\n            self.initialized = True\n        else:\n            logger.warning(\n                \"pipeline.setup() called more than once (build() inside \" \"build()?)\"\n            )",
  "def internal_teardown(self):\n        \"\"\"Call teardown on each batch provider in the pipeline and disconnect\n        all nodes.\"\"\"\n\n        try:\n\n            def node_teardown(node):\n                try:\n                    node.output.internal_teardown()\n                except Exception as e:\n                    raise PipelineTeardownError(node.output) from e\n\n            # call internal_teardown on all nodes\n            self.traverse(node_teardown, reverse=True)\n            self.initialized = False\n\n        finally:\n            # disconnect all nodes\n            def disconnect(node):\n                node.output.remove_upstream_providers()\n\n            self.traverse(disconnect)",
  "def request_batch(self, request):\n        \"\"\"Request a batch from the pipeline.\"\"\"\n\n        try:\n            return self.output.request_batch(request)\n        except Exception as e:\n            raise PipelineRequestError(self, request) from e",
  "def spec(self):\n        return self.output.spec",
  "def __add__(self, other):\n        if isinstance(other, BatchProvider):\n            other = Pipeline(other)\n\n        if isinstance(other, Pipeline):\n            result = other.copy()\n\n            # add this pipeline as child to all leaves in other\n            def add_self_to_leaves(node):\n                if len(node.children) == 0:\n                    node.children.append(self.copy())\n\n            result.traverse(add_self_to_leaves, reverse=True)\n\n        else:\n            raise RuntimeError(f\"Don't know how to add {type(other)} to Pipeline\")\n\n        return result",
  "def __radd__(self, other):\n        assert isinstance(\n            other, tuple\n        ), f\"Don't know how to radd {type(other)} to Pipeline\"\n\n        for o in other:\n            assert isinstance(o, Pipeline) or isinstance(\n                o, BatchProvider\n            ), f\"Don't know how to radd {type(o)} to Pipeline\"\n\n        other = tuple(\n            Pipeline(o) if isinstance(o, BatchProvider) else o.copy() for o in other\n        )\n\n        result = self.copy()\n\n        # add every other as child to leaves in pipeline\n        def add_others_to_leaves(node):\n            if len(node.children) == 0:\n                for o in other:\n                    node.children.append(o)\n\n        result.traverse(add_others_to_leaves, reverse=True)\n\n        return result",
  "def __repr__(self):\n        def to_string(node):\n            return node.output.name()\n\n        reprs = self.traverse(to_string, reverse=True)\n\n        return self.__rec_repr__(reprs)",
  "def __rec_repr__(self, reprs):\n        if not isinstance(reprs, list):\n            return str(reprs)\n\n        num_children = len(reprs) - 1\n\n        res = \"\"\n        if num_children > 0:\n            if num_children > 1:\n                res = \"(\"\n            res += \", \".join(self.__rec_repr__(r) for r in reprs[:-1])\n        if num_children > 0:\n            if num_children > 1:\n                res += \")\"\n            res += \" -> \"\n        res += reprs[-1]\n        return res",
  "def connect(node):\n            for child in node.children:\n                node.output.add_upstream_provider(child.output)",
  "def add_others_to_leaves(node):\n            if len(node.children) == 0:\n                for o in other:\n                    node.children.append(o)",
  "def to_string(node):\n            return node.output.name()",
  "def node_setup(node):\n                try:\n                    node.output.setup()\n                except Exception as e:\n                    raise PipelineSetupError(node.output) from e",
  "def node_teardown(node):\n                try:\n                    node.output.internal_teardown()\n                except Exception as e:\n                    raise PipelineTeardownError(node.output) from e",
  "def disconnect(node):\n                node.output.remove_upstream_providers()",
  "def add_self_to_leaves(node):\n                if len(node.children) == 0:\n                    node.children.append(self.copy())",
  "def enlarge_binary_map(\n    binary_map, radius, voxel_size, ring_fraction=None, in_place=False\n):\n    \"\"\"Enlarge existing regions in a binary map.\n\n    Args:\n\n        binary_map (numpy array):\n\n            A matrix with zeros, in which regions to be enlarged are indicated\n            with a 1 (regions can already represent larger areas).\n\n        radius (``ndarray`` of ``float``):\n\n            The amount by which to enlarge forground objects in world units.\n\n        voxel_size (tuple, list or numpy array):\n\n            Indicates the physical voxel size of the binary_map.\n\n        ring_fraction (``float``, optional):\n\n            If set, instead of just enlargin objects, a ring is grown around\n            them (and the objects removed). The thickness of the ring is set\n            with this parameter as a fraction of the radius.\n\n        in_place (bool, optional):\n\n            If set to ``True``, argument ``binary_map`` will be modified\n            directly.\n\n    Returns:\n\n        A matrix with 0s and 1s of same dimension as input binary_map with\n        enlarged regions (indicated with 1), unless ``in_place`` is set.\n    \"\"\"\n\n    if len(np.unique(binary_map)) == 1:\n        # Check whether there are regions at all. If there is no region (or\n        # everything is full), return the same map.\n        return binary_map\n\n    if voxel_size is None:\n        voxel_size = (1,) * binary_map.shape[0]\n\n    voxel_size = np.asarray(voxel_size).astype(np.float32)\n\n    # normalize, such that radius == 1 in all dimensions\n    voxel_size = voxel_size / radius\n\n    if in_place:\n        np.logical_not(binary_map, out=binary_map)\n    else:\n        binary_map = np.logical_not(binary_map)\n    edtmap = distance_transform_edt(binary_map, sampling=voxel_size)\n\n    # grow objects\n    if in_place:\n        binary_map[:] = edtmap <= 1.0\n    else:\n        binary_map = edtmap <= 1.0\n\n    # unmask inner part, if requested\n    if ring_fraction is not None:\n        binary_map[edtmap <= 1.0 - ring_fraction] = False\n\n    if in_place:\n        return None\n\n    return binary_map",
  "def create_ball_kernel(radius, voxel_size):\n    \"\"\"Generates a ball-shaped structuring element.\n\n    Args:\n\n        radius (``ndarray`` of ``float``):\n\n            The radius of the ball-shaped structuring element in world-units.\n\n        voxel_size (tuple, list or numpy array):\n\n            Indicates the physical voxel size of the structuring element.\n\n    Returns:\n\n        The structuring element where elements of the neighborhood are 1 and 0\n        otherwise. The shape of the returned array depends on radius and\n        voxel_size. For instance voxel_size = [2, 1, 1], radius = 5 produces an\n        array of shape (7, 11, 11)\n    \"\"\"\n    voxel_size = np.asarray(voxel_size)\n\n    # Calculate shape for new kernel, make it sufficiently large (--> ceil)\n    radius_voxel = np.ceil(radius / voxel_size).astype(int)\n    kernel_shape = np.array(radius_voxel) * 2 + 1\n\n    kernel = np.zeros(kernel_shape, dtype=np.uint8)\n    middle_point = kernel_shape // 2\n    kernel[tuple(middle_point)] = 1\n\n    enlarge_binary_map(kernel, radius, voxel_size, in_place=True)\n\n    return kernel",
  "class ProviderSpec(Freezable):\n    \"\"\"A collection of (possibly partial) :class:`ArraySpecs<ArraySpec>` and\n    :class:`GraphSpecs<GraphSpec>` describing a\n    :class:`BatchProvider's<BatchProvider>` offered arrays and graphs.\n\n    This collection mimics a dictionary. Specs can be added with::\n\n        provider_spec = ProviderSpec()\n        provider_spec[array_key] = ArraySpec(...)\n        provider_spec[graph_key] = GraphSpec(...)\n\n    Here, ``array_key`` and ``graph_key`` are :class:`ArrayKey` and\n    :class:`GraphKey`. The specs can be queried with::\n\n        array_spec = provider_spec[array_key]\n        graph_spec = provider_spec[graph_key]\n\n    Furthermore, pairs of keys/values can be iterated over using\n    ``provider_spec.items()``.\n\n    To access only array or graph specs, use the dictionaries\n    ``provider_spec.array_specs`` or ``provider_spec.graph_specs``,\n    respectively.\n\n    Args:\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`):\n\n            Initial array specs.\n\n        graph_specs (``dict``, :class:`GraphKey` -> :class:`GraphSpec`):\n\n            Initial graph specs.\n\n    Attributes:\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`):\n\n            Contains all array specs contained in this provider spec.\n\n        graph_specs (``dict``, :class:`GraphKey` -> :class:`GraphSpec`):\n\n            Contains all graph specs contained in this provider spec.\n    \"\"\"\n\n    def __init__(self, array_specs=None, graph_specs=None):\n        self.array_specs = {}\n        self.graph_specs = {}\n        self.freeze()\n\n        # use __setitem__ instead of copying the dicts, this ensures type tests\n        # are run\n        if array_specs is not None:\n            for key, spec in array_specs.items():\n                self[key] = spec\n        if graph_specs is not None:\n            for key, spec in graph_specs.items():\n                self[key] = spec\n\n    def __setitem__(self, key, spec):\n        assert isinstance(key, ArrayKey) or isinstance(key, GraphKey), (\n            f\"Only ArrayKey or GraphKey (not {type(key).__name__} are \"\n            \"allowed as key for ProviderSpec, \"\n        )\n\n        if isinstance(key, ArrayKey):\n            if isinstance(spec, Roi):\n                spec = ArraySpec(roi=spec)\n\n            assert isinstance(spec, ArraySpec), (\n                f\"Only ArraySpec (not {type(spec).__name__}) can be set for \" \"ArrayKey\"\n            )\n\n            self.array_specs[key] = spec.copy()\n\n        else:\n            if isinstance(spec, Roi):\n                spec = GraphSpec(roi=spec)\n\n            assert isinstance(spec, GraphSpec), (\n                f\"Only GraphSpec (not {type(spec).__name__}) can be set for \" \"GraphKey\"\n            )\n\n            self.graph_specs[key] = spec.copy()\n\n    def __getitem__(self, key):\n        if isinstance(key, ArrayKey):\n            return self.array_specs[key]\n\n        elif isinstance(key, GraphKey):\n            return self.graph_specs[key]\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )\n\n    def __len__(self):\n        return len(self.array_specs) + len(self.graph_specs)\n\n    def __contains__(self, key):\n        if isinstance(key, ArrayKey):\n            return key in self.array_specs\n\n        elif isinstance(key, GraphKey):\n            return key in self.graph_specs\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey, can be used as keys in a \"\n                \"%s. Key %s is a %s\" % (type(self).__name__, key, type(key).__name__)\n            )\n\n    def __delitem__(self, key):\n        if isinstance(key, ArrayKey):\n            del self.array_specs[key]\n\n        elif isinstance(key, GraphKey):\n            del self.graph_specs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )\n\n    def remove_placeholders(self):\n        self.array_specs = {\n            k: v for k, v in self.array_specs.items() if not v.placeholder\n        }\n        self.graph_specs = {\n            k: v for k, v in self.graph_specs.items() if not v.placeholder\n        }\n\n    def items(self):\n        \"\"\"Provides a generator iterating over key/value pairs.\"\"\"\n\n        for k, v in self.array_specs.items():\n            yield k, v\n        for k, v in self.graph_specs.items():\n            yield k, v\n\n    def get_total_roi(self):\n        \"\"\"Get the union of all the ROIs.\"\"\"\n\n        total_roi = None\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for _, spec in specs_type.items():\n                if total_roi is None:\n                    total_roi = spec.roi\n                elif spec.roi is not None:\n                    total_roi = total_roi.union(spec.roi)\n        return total_roi\n\n    def get_common_roi(self):\n        \"\"\"Get the intersection of all the requested ROIs.\"\"\"\n\n        common_roi = None\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for _, spec in specs_type.items():\n                if common_roi is None:\n                    common_roi = spec.roi\n                else:\n                    common_roi = common_roi.intersect(spec.roi)\n\n        return common_roi\n\n    def get_lcm_voxel_size(self, array_keys=None):\n        \"\"\"Get the least common multiple of the voxel sizes in this spec.\n\n        Args:\n\n            array_keys (list of :class:`ArrayKey`, optional): If given,\n                consider only the given array types.\n        \"\"\"\n\n        if array_keys is None:\n            array_keys = self.array_specs.keys()\n\n        if not array_keys:\n            return None\n\n        lcm_voxel_size = None\n        for key in array_keys:\n            voxel_size = self.array_specs[key].voxel_size\n            if voxel_size is None:\n                continue\n            if lcm_voxel_size is None:\n                lcm_voxel_size = voxel_size\n            else:\n                lcm_voxel_size = Coordinate(\n                    (\n                        a * b // math.gcd(a, b)\n                        for a, b in zip(lcm_voxel_size, voxel_size)\n                    )\n                )\n\n        return lcm_voxel_size\n\n    def copy(self):\n        \"\"\"Create a copy of this instance.\"\"\"\n        return copy.deepcopy(self)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            other_dict = copy.deepcopy(other.__dict__)\n            self_dict = copy.deepcopy(self.__dict__)\n            return self_dict == other_dict\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented\n\n    def __repr__(self):\n        r = \"\\n\"\n        for key, spec in self.items():\n            r += \"\\t%s: %s\\n\" % (key, spec)\n        return r",
  "def __init__(self, array_specs=None, graph_specs=None):\n        self.array_specs = {}\n        self.graph_specs = {}\n        self.freeze()\n\n        # use __setitem__ instead of copying the dicts, this ensures type tests\n        # are run\n        if array_specs is not None:\n            for key, spec in array_specs.items():\n                self[key] = spec\n        if graph_specs is not None:\n            for key, spec in graph_specs.items():\n                self[key] = spec",
  "def __setitem__(self, key, spec):\n        assert isinstance(key, ArrayKey) or isinstance(key, GraphKey), (\n            f\"Only ArrayKey or GraphKey (not {type(key).__name__} are \"\n            \"allowed as key for ProviderSpec, \"\n        )\n\n        if isinstance(key, ArrayKey):\n            if isinstance(spec, Roi):\n                spec = ArraySpec(roi=spec)\n\n            assert isinstance(spec, ArraySpec), (\n                f\"Only ArraySpec (not {type(spec).__name__}) can be set for \" \"ArrayKey\"\n            )\n\n            self.array_specs[key] = spec.copy()\n\n        else:\n            if isinstance(spec, Roi):\n                spec = GraphSpec(roi=spec)\n\n            assert isinstance(spec, GraphSpec), (\n                f\"Only GraphSpec (not {type(spec).__name__}) can be set for \" \"GraphKey\"\n            )\n\n            self.graph_specs[key] = spec.copy()",
  "def __getitem__(self, key):\n        if isinstance(key, ArrayKey):\n            return self.array_specs[key]\n\n        elif isinstance(key, GraphKey):\n            return self.graph_specs[key]\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )",
  "def __len__(self):\n        return len(self.array_specs) + len(self.graph_specs)",
  "def __contains__(self, key):\n        if isinstance(key, ArrayKey):\n            return key in self.array_specs\n\n        elif isinstance(key, GraphKey):\n            return key in self.graph_specs\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey, can be used as keys in a \"\n                \"%s. Key %s is a %s\" % (type(self).__name__, key, type(key).__name__)\n            )",
  "def __delitem__(self, key):\n        if isinstance(key, ArrayKey):\n            del self.array_specs[key]\n\n        elif isinstance(key, GraphKey):\n            del self.graph_specs[key]\n\n        else:\n            raise RuntimeError(\n                \"Only ArrayKey or GraphKey can be used as keys in a \"\n                \"%s.\" % type(self).__name__\n            )",
  "def remove_placeholders(self):\n        self.array_specs = {\n            k: v for k, v in self.array_specs.items() if not v.placeholder\n        }\n        self.graph_specs = {\n            k: v for k, v in self.graph_specs.items() if not v.placeholder\n        }",
  "def items(self):\n        \"\"\"Provides a generator iterating over key/value pairs.\"\"\"\n\n        for k, v in self.array_specs.items():\n            yield k, v\n        for k, v in self.graph_specs.items():\n            yield k, v",
  "def get_total_roi(self):\n        \"\"\"Get the union of all the ROIs.\"\"\"\n\n        total_roi = None\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for _, spec in specs_type.items():\n                if total_roi is None:\n                    total_roi = spec.roi\n                elif spec.roi is not None:\n                    total_roi = total_roi.union(spec.roi)\n        return total_roi",
  "def get_common_roi(self):\n        \"\"\"Get the intersection of all the requested ROIs.\"\"\"\n\n        common_roi = None\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for _, spec in specs_type.items():\n                if common_roi is None:\n                    common_roi = spec.roi\n                else:\n                    common_roi = common_roi.intersect(spec.roi)\n\n        return common_roi",
  "def get_lcm_voxel_size(self, array_keys=None):\n        \"\"\"Get the least common multiple of the voxel sizes in this spec.\n\n        Args:\n\n            array_keys (list of :class:`ArrayKey`, optional): If given,\n                consider only the given array types.\n        \"\"\"\n\n        if array_keys is None:\n            array_keys = self.array_specs.keys()\n\n        if not array_keys:\n            return None\n\n        lcm_voxel_size = None\n        for key in array_keys:\n            voxel_size = self.array_specs[key].voxel_size\n            if voxel_size is None:\n                continue\n            if lcm_voxel_size is None:\n                lcm_voxel_size = voxel_size\n            else:\n                lcm_voxel_size = Coordinate(\n                    (\n                        a * b // math.gcd(a, b)\n                        for a, b in zip(lcm_voxel_size, voxel_size)\n                    )\n                )\n\n        return lcm_voxel_size",
  "def copy(self):\n        \"\"\"Create a copy of this instance.\"\"\"\n        return copy.deepcopy(self)",
  "def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            other_dict = copy.deepcopy(other.__dict__)\n            self_dict = copy.deepcopy(self.__dict__)\n            return self_dict == other_dict\n        return NotImplemented",
  "def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return not self.__eq__(other)\n        return NotImplemented",
  "def __repr__(self):\n        r = \"\\n\"\n        for key, spec in self.items():\n            r += \"\\t%s: %s\\n\" % (key, spec)\n        return r",
  "class BatchRequest(ProviderSpec):\n    \"\"\"A collection of (possibly partial) :class:`ArraySpec` and\n    :class:`GraphSpec` forming a request.\n\n    Inherits from :class:`ProviderSpec`.\n\n    Additional Kwargs:\n\n        random_seed (``int``):\n\n            The random seed that will be associated with this batch to\n            guarantee deterministic and repeatable batch requests.\n\n    \"\"\"\n\n    def __init__(self, *args, random_seed=None, **kwargs):\n        self._random_seed = random_seed\n        super().__init__(*args, **kwargs)\n\n    def add(self, key, shape, voxel_size=None, directed=None, placeholder=False):\n        \"\"\"Convenience method to add an array or graph spec by providing only\n        the shape of a ROI (in world units).\n\n        A ROI with zero-offset will be generated. If more than one request is\n        added, the ROIs with smaller shapes will be shifted to be centered in\n        the largest one.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The key for which to add a spec.\n\n            shape (:class:`Coordinate`):\n\n                A tuple containing the shape of the desired roi\n\n            voxel_size (:class:`Coordinate`):\n\n                A tuple contening the voxel sizes for each corresponding\n                dimension\n        \"\"\"\n\n        if isinstance(key, ArrayKey):\n            spec = ArraySpec(placeholder=placeholder)\n        elif isinstance(key, GraphKey):\n            spec = GraphSpec(placeholder=placeholder, directed=directed)\n        else:\n            raise RuntimeError(\"Only ArrayKey or GraphKey can be added.\")\n\n        spec.roi = Roi((0,) * len(shape), shape)\n\n        if voxel_size is not None:\n            spec.voxel_size = voxel_size\n\n        self[key] = spec\n        self.__center_rois()\n\n    def copy(self):\n        \"\"\"Create a copy of this request.\"\"\"\n        request_copy = copy.deepcopy(self)\n        request_copy._update_random_seed()\n        return request_copy\n\n    def is_deterministic(self):\n        \"\"\"Return true if a random seed has been set for this request.\"\"\"\n        return self._random_seed is not None\n\n    @property\n    def random_seed(self):\n        if not self.is_deterministic():\n            return int(time.time() * 1e6) % (2**32)\n        else:\n            return self._random_seed\n\n    def _update_random_seed(self):\n        if not self.is_deterministic():\n            return\n        self._random_seed = hash((self._random_seed + 1) ** 2) % (2**32)\n\n    def __center_rois(self):\n        \"\"\"Ensure that all ROIs are centered around the same location.\"\"\"\n\n        total_roi = self.get_total_roi()\n        if total_roi is None:\n            return\n\n        center = total_roi.center\n\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for key in specs_type:\n                roi = specs_type[key].roi\n                specs_type[key].roi = roi.shift(center - roi.center)\n\n    def update_with(self, request):\n        \"\"\"Update current request with another\"\"\"\n\n        assert isinstance(request, BatchRequest)\n\n        merged = self.copy()\n\n        for key, spec in request.items():\n            if key not in merged:\n                merged[key] = spec\n            else:\n                merged[key].update_with(spec)\n\n        return merged\n\n    def merge(self, request):\n        \"\"\"Merge another request with current request\"\"\"\n        warn(\n            \"merge is deprecated! please use update_with \"\n            \"as it accounts for spec metadata\"\n        )\n        assert isinstance(request, BatchRequest)\n\n        merged = self.copy()\n\n        for key, spec in request.items():\n            if key not in merged:\n                merged[key] = spec\n            else:\n                if isinstance(spec, ArraySpec) and merged[key].nonspatial:\n                    merged[key] = spec\n                else:\n                    merged[key].roi = merged[key].roi.union(spec.roi)\n\n        return merged\n\n    def __eq__(self, other):\n        \"\"\"\n        Override equality check to allow batche requests with different\n        seeds to still be checked. Otherwise equality check should\n        never succeed.\n        \"\"\"\n\n        if isinstance(other, self.__class__):\n            other_dict = copy.deepcopy(other.__dict__)\n            self_dict = copy.deepcopy(self.__dict__)\n            other_dict.pop(\"_random_seed\")\n            self_dict.pop(\"_random_seed\")\n            return self_dict == other_dict\n        return NotImplemented",
  "def __init__(self, *args, random_seed=None, **kwargs):\n        self._random_seed = random_seed\n        super().__init__(*args, **kwargs)",
  "def add(self, key, shape, voxel_size=None, directed=None, placeholder=False):\n        \"\"\"Convenience method to add an array or graph spec by providing only\n        the shape of a ROI (in world units).\n\n        A ROI with zero-offset will be generated. If more than one request is\n        added, the ROIs with smaller shapes will be shifted to be centered in\n        the largest one.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The key for which to add a spec.\n\n            shape (:class:`Coordinate`):\n\n                A tuple containing the shape of the desired roi\n\n            voxel_size (:class:`Coordinate`):\n\n                A tuple contening the voxel sizes for each corresponding\n                dimension\n        \"\"\"\n\n        if isinstance(key, ArrayKey):\n            spec = ArraySpec(placeholder=placeholder)\n        elif isinstance(key, GraphKey):\n            spec = GraphSpec(placeholder=placeholder, directed=directed)\n        else:\n            raise RuntimeError(\"Only ArrayKey or GraphKey can be added.\")\n\n        spec.roi = Roi((0,) * len(shape), shape)\n\n        if voxel_size is not None:\n            spec.voxel_size = voxel_size\n\n        self[key] = spec\n        self.__center_rois()",
  "def copy(self):\n        \"\"\"Create a copy of this request.\"\"\"\n        request_copy = copy.deepcopy(self)\n        request_copy._update_random_seed()\n        return request_copy",
  "def is_deterministic(self):\n        \"\"\"Return true if a random seed has been set for this request.\"\"\"\n        return self._random_seed is not None",
  "def random_seed(self):\n        if not self.is_deterministic():\n            return int(time.time() * 1e6) % (2**32)\n        else:\n            return self._random_seed",
  "def _update_random_seed(self):\n        if not self.is_deterministic():\n            return\n        self._random_seed = hash((self._random_seed + 1) ** 2) % (2**32)",
  "def __center_rois(self):\n        \"\"\"Ensure that all ROIs are centered around the same location.\"\"\"\n\n        total_roi = self.get_total_roi()\n        if total_roi is None:\n            return\n\n        center = total_roi.center\n\n        for specs_type in [self.array_specs, self.graph_specs]:\n            for key in specs_type:\n                roi = specs_type[key].roi\n                specs_type[key].roi = roi.shift(center - roi.center)",
  "def update_with(self, request):\n        \"\"\"Update current request with another\"\"\"\n\n        assert isinstance(request, BatchRequest)\n\n        merged = self.copy()\n\n        for key, spec in request.items():\n            if key not in merged:\n                merged[key] = spec\n            else:\n                merged[key].update_with(spec)\n\n        return merged",
  "def merge(self, request):\n        \"\"\"Merge another request with current request\"\"\"\n        warn(\n            \"merge is deprecated! please use update_with \"\n            \"as it accounts for spec metadata\"\n        )\n        assert isinstance(request, BatchRequest)\n\n        merged = self.copy()\n\n        for key, spec in request.items():\n            if key not in merged:\n                merged[key] = spec\n            else:\n                if isinstance(spec, ArraySpec) and merged[key].nonspatial:\n                    merged[key] = spec\n                else:\n                    merged[key].roi = merged[key].roi.union(spec.roi)\n\n        return merged",
  "def __eq__(self, other):\n        \"\"\"\n        Override equality check to allow batche requests with different\n        seeds to still be checked. Otherwise equality check should\n        never succeed.\n        \"\"\"\n\n        if isinstance(other, self.__class__):\n            other_dict = copy.deepcopy(other.__dict__)\n            self_dict = copy.deepcopy(self.__dict__)\n            other_dict.pop(\"_random_seed\")\n            self_dict.pop(\"_random_seed\")\n            return self_dict == other_dict\n        return NotImplemented",
  "class _Version(object):\n    def major(self):\n        return __major__\n\n    def minor(self):\n        return __minor__\n\n    def patch(self):\n        return __patch__\n\n    def tag(self):\n        return __tag__\n\n    def version(self):\n        return __version__\n\n    def __str__(self):\n        return self.version()",
  "def major(self):\n        return __major__",
  "def minor(self):\n        return __minor__",
  "def patch(self):\n        return __patch__",
  "def tag(self):\n        return __tag__",
  "def version(self):\n        return __version__",
  "def __str__(self):\n        return self.version()",
  "class Freezable(object):\n    __isfrozen = False\n\n    def __setattr__(self, key, value):\n        if self.__isfrozen and not hasattr(self, key):\n            raise TypeError(\"%r is frozen, you can't add attributes to it\" % self)\n        object.__setattr__(self, key, value)\n\n    def freeze(self):\n        self.__isfrozen = True\n\n    def thaw(self):\n        self.__isfrozen = False",
  "def __setattr__(self, key, value):\n        if self.__isfrozen and not hasattr(self, key):\n            raise TypeError(\"%r is frozen, you can't add attributes to it\" % self)\n        object.__setattr__(self, key, value)",
  "def freeze(self):\n        self.__isfrozen = True",
  "def thaw(self):\n        self.__isfrozen = False",
  "class build(object):\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n\n    def __enter__(self):\n        try:\n            self.pipeline.setup()\n        except:\n            logger.error(\n                \"something went wrong during the setup of the pipeline, calling tear down\"\n            )\n            self.pipeline.internal_teardown()\n            logger.debug(\"tear down completed\")\n            raise\n        return self.pipeline\n\n    def __exit__(self, type, value, traceback):\n        logger.debug(\"leaving context, tearing down pipeline\")\n        self.pipeline.internal_teardown()\n        logger.debug(\"tear down completed\")",
  "def __init__(self, pipeline):\n        self.pipeline = pipeline",
  "def __enter__(self):\n        try:\n            self.pipeline.setup()\n        except:\n            logger.error(\n                \"something went wrong during the setup of the pipeline, calling tear down\"\n            )\n            self.pipeline.internal_teardown()\n            logger.debug(\"tear down completed\")\n            raise\n        return self.pipeline",
  "def __exit__(self, type, value, traceback):\n        logger.debug(\"leaving context, tearing down pipeline\")\n        self.pipeline.internal_teardown()\n        logger.debug(\"tear down completed\")",
  "class LocalServer(Freezable):\n    \"\"\"Wrapper around ``tf.train.Server`` to create a local server on-demand.\n\n    This class is necessary because tensorflow's GPU support should not be\n    initialized before forking processes (the CUDA driver needs to be\n    initialized in each process separately, not in the main process and then\n    forked). Creating a ``tf.train.Server`` initializes GPU support, however.\n    With this wrapper, server creating can be delayed until a GPU process\n    creates a ``tf.Session``::\n\n        session = tf.Session(target=LocalServer.get_target())\n    \"\"\"\n\n    __target = multiprocessing.Array(ctypes.c_char, b\" \" * 256)\n    __server = None\n\n    @staticmethod\n    def get_target():\n        \"\"\"Get the target string of this tensorflow server to connect a\n        ``tf.Session()``. This will start the server, if it is not running\n        already.\n        \"\"\"\n\n        with LocalServer.__target.get_lock():\n            target = LocalServer.__target.value\n\n            if target == b\" \" * 256:\n                logger.info(\"Creating local tensorflow server\")\n                LocalServer.__server = tf.train.Server.create_local_server()\n                target = LocalServer.__server.target\n                if not isinstance(target, bytes):\n                    target = target.encode(\"ascii\")\n                logger.info(\"Server running at %s\", target)\n            else:\n                logger.info(\"Server already running at %s\", target)\n\n            LocalServer.__target.value = target\n\n        return target",
  "def get_target():\n        \"\"\"Get the target string of this tensorflow server to connect a\n        ``tf.Session()``. This will start the server, if it is not running\n        already.\n        \"\"\"\n\n        with LocalServer.__target.get_lock():\n            target = LocalServer.__target.value\n\n            if target == b\" \" * 256:\n                logger.info(\"Creating local tensorflow server\")\n                LocalServer.__server = tf.train.Server.create_local_server()\n                target = LocalServer.__server.target\n                if not isinstance(target, bytes):\n                    target = target.encode(\"ascii\")\n                logger.info(\"Server running at %s\", target)\n            else:\n                logger.info(\"Server already running at %s\", target)\n\n            LocalServer.__target.value = target\n\n        return target",
  "class Train(GenericTrain):\n    \"\"\"Tensorflow implementation of :class:`gunpowder.nodes.Train`.\n\n    Args:\n\n        graph (``string``):\n\n            Filename of a tensorflow meta-graph storing the tensorflow graph\n            containing an optimizer. A meta-graph file can be created by\n            running::\n\n                # create tensorflow graph\n                ...\n\n                # store it\n                tf.train.export_meta_graph(filename='...')\n\n        optimizer (``string`` or function):\n\n            Either the name of the tensorflow operator performing a training\n            iteration, or a function that, given the graph of the meta-graph\n            file, adds a custom loss and optimizer.\n\n            If a function is given, it should return a tuple ``(loss,\n            optimizer)`` of a tensor and an operator representing the loss and\n            the optimizer, respectively. In this case, parameter ``loss``\n            should be ``None``.\n\n            Example::\n\n                def add_custom_optimizer(graph):\n\n                    # get the output of your graph\n                    output = graph.get_tensor_by_name('...')\n\n                    # create your custom loss\n                    loss = custom_loss(output)\n\n                    # add an optimizer of your choice\n                    optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n                    return (loss, optimizer)\n\n        loss (``string`` or ``None``):\n\n            The name of the tensorflow tensor containing the loss, or ``None``\n            if ``optimizer`` is a function.\n\n        inputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of input tensors in the network to\n            array keys.\n\n        outputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of output tensors in the network to array\n            keys. New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        gradients (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of output tensors in the network to\n            array keys. New arrays containing the gradient of an output with\n            respect to the loss will be generated by this node for each entry\n            (if requested downstream).\n\n        summary (``string`` or\n                 ``dict``, ``string`` -> (``string`` (tensor name), freq),\n                 optional):\n\n            The name of the tensorflow tensor containing the tensorboard\n            summaries or dictionary for different subcategories of summaires\n            (key: string, value: tuple with tensor/op name and frequency,\n            of evaluation).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (``outputs``). This is\n            useful to set the ``voxel_size``, for example, if they differ from\n            the voxel size of the input arrays. Only fields that are not\n            ``None`` in the given :class:`ArraySpec` will be used.\n\n        save_every (``int``, optional):\n\n            After how many iterations to create a checkpoint to store the\n            learnt weights.\n\n        log_dir (``string``, optional):\n\n            Directory for saving tensorboard summaries.\n\n        log_every (``int``, optional):\n\n            After how many iterations to write out tensorboard summaries.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph,\n        optimizer,\n        loss,\n        inputs,\n        outputs,\n        gradients,\n        summary=None,\n        array_specs=None,\n        save_every=2000,\n        log_dir=\"./\",\n        log_every=1,\n    ):\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=False\n        )\n        self.meta_graph_filename = graph\n        self.optimizer_func = None\n        self.optimizer_loss_names = None\n        self.optimizer = None\n        self.loss = None\n        self.summary = summary\n        self.session = None\n        self.tf_gradient = {}\n        self.graph = None\n        self.basic_saver = None\n        self.full_saver = None\n        self.save_every = save_every\n        self.iteration = None\n        self.iteration_increment = None\n        self.summary_saver = None\n        self.log_dir = log_dir\n        self.log_every = log_every\n        # Check if optimizer is a str in python 2/3 compatible way.\n        if isinstance(optimizer, (\"\".__class__, \"\".__class__)):\n            self.optimizer_loss_names = (optimizer, loss)\n        else:\n            self.optimizer_func = optimizer\n\n        # at least for some versions of tensorflow, the checkpoint name has to\n        # start with a . if it is a relative path\n        if not os.path.isabs(self.meta_graph_filename):\n            self.meta_graph_filename = os.path.join(\".\", self.meta_graph_filename)\n\n    def start(self):\n        target = LocalServer.get_target()\n        logger.info(\"Initializing tf session, connecting to %s...\", target)\n\n        self.graph = tf.Graph()\n        self.session = tf.Session(target=target, graph=self.graph)\n\n        with self.graph.as_default():\n            self.__read_meta_graph()\n\n        if self.summary is not None:\n            self.summary_saver = tf.summary.FileWriter(self.log_dir, self.graph)\n\n        if self.optimizer_func is None:\n            # get actual operations/tensors from names\n            self.optimizer = self.graph.get_operation_by_name(\n                self.optimizer_loss_names[0]\n            )\n            self.loss = self.graph.get_tensor_by_name(self.optimizer_loss_names[1])\n\n        # add symbolic gradients\n        for tensor_name in self.gradients:\n            tensor = self.graph.get_tensor_by_name(tensor_name)\n            self.tf_gradient[tensor_name] = tf.gradients(self.loss, [tensor])[0]\n\n    def train_step(self, batch, request):\n        array_outputs = self.__collect_requested_outputs(request)\n        inputs = self.__collect_provided_inputs(batch)\n\n        to_compute = {\n            \"optimizer\": self.optimizer,\n            \"loss\": self.loss,\n            \"iteration\": self.iteration_increment,\n        }\n        to_compute.update(array_outputs)\n\n        # compute outputs, gradients, and update variables\n        if isinstance(self.summary, str):\n            to_compute[\"summaries\"] = self.summary\n        elif isinstance(self.summary, dict):\n            for k, (v, f) in self.summary.items():\n                if int(self.current_step + 1) % f == 0:\n                    to_compute[k] = v\n        outputs = self.session.run(to_compute, feed_dict=inputs)\n\n        for array_key in array_outputs:\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(outputs[array_key], spec)\n\n        batch.loss = outputs[\"loss\"]\n        batch.iteration = outputs[\"iteration\"][0]\n        self.current_step = batch.iteration\n        if self.summary is not None:\n            if isinstance(self.summary, str) and (\n                batch.iteration % self.log_every == 0 or batch.iteration == 1\n            ):\n                self.summary_saver.add_summary(outputs[\"summaries\"], batch.iteration)\n            else:\n                for k, (_, f) in self.summary.items():\n                    if int(self.current_step) % f == 0:\n                        self.summary_saver.add_summary(outputs[k], batch.iteration)\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = (\n                self.meta_graph_filename + \"_checkpoint_%i\" % batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            self.full_saver.save(self.session, checkpoint_name)\n\n    def stop(self):\n        if self.session is not None:\n            self.optimizer = None\n            self.loss = None\n            if self.summary is not None:\n                self.summary_saver.close()\n            self.session.close()\n            self.graph = None\n            self.session = None\n\n    def __read_meta_graph(self):\n        logger.info(\"Reading meta-graph...\")\n\n        # read the original meta-graph\n        tf.train.import_meta_graph(\n            self.meta_graph_filename + \".meta\", clear_devices=True\n        )\n\n        # add custom gunpowder variables\n        with tf.variable_scope(\"gunpowder\"):\n            self.iteration = tf.get_variable(\n                \"iteration\", shape=1, initializer=tf.zeros_initializer, trainable=False\n            )\n            self.iteration_increment = tf.assign(self.iteration, self.iteration + 1)\n\n        # Until now, only variables have been added to the graph that are part\n        # of every checkpoint. We create a 'basic_saver' for only those\n        # variables.\n        self.basic_saver = tf.train.Saver(max_to_keep=None)\n\n        # Add custom optimizer and loss, if requested. This potentially adds\n        # more variables, not covered by the basic_saver.\n        if self.optimizer_func is not None:\n            loss, optimizer = self.optimizer_func(self.graph)\n            self.loss = loss\n            self.optimizer = optimizer\n\n        # We create a 'full_saver' including those variables.\n        self.full_saver = tf.train.Saver(max_to_keep=None)\n\n        # find most recent checkpoint\n        checkpoint_dir = os.path.dirname(self.meta_graph_filename)\n        checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n\n        if checkpoint:\n            try:\n                # Try to restore the graph, including the custom optimizer\n                # state (if a custom optimizer was used).\n                self.__restore_graph(checkpoint, restore_full=True)\n\n            except tf.errors.NotFoundError:\n                # If that failed, we just transitioned from an earlier training\n                # without the custom optimizer. In this case, restore only the\n                # variables of the original meta-graph and 'gunpowder'\n                # variables. Custom optimizer variables will be default\n                # initialized.\n                logger.info(\"Checkpoint did not contain custom optimizer \" \"variables\")\n                self.__restore_graph(checkpoint, restore_full=False)\n        else:\n            logger.info(\"No checkpoint found\")\n\n            # initialize all variables\n            self.session.run(tf.global_variables_initializer())\n\n    def __restore_graph(self, checkpoint, restore_full):\n        logger.info(\"Restoring model from %s\", checkpoint)\n\n        if restore_full:\n            logger.info(\"...using a saver for all variables\")\n            self.full_saver.restore(self.session, checkpoint)\n\n        else:\n            # initialize all variables, such that non-basic variables are\n            # initialized\n            self.session.run(tf.global_variables_initializer())\n\n            logger.info(\"...using a saver for basic variables only\")\n            self.basic_saver.restore(self.session, checkpoint)\n\n        self.current_step = self.session.run(self.iteration)\n\n    def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        for output_name, array_key in self.gradients.items():\n            if array_key in request:\n                array_outputs[array_key] = self.tf_gradient[output_name]\n\n        return array_outputs\n\n    def __collect_provided_inputs(self, batch):\n        inputs = {}\n\n        for input_name, input_key in self.inputs.items():\n            if isinstance(input_key, ArrayKey):\n                if input_key in batch.arrays:\n                    inputs[input_name] = batch.arrays[input_key].data\n                else:\n                    logger.warn(\n                        \"batch does not contain %s, input %s will not \" \"be set\",\n                        input_key,\n                        input_name,\n                    )\n            elif isinstance(input_key, np.ndarray):\n                inputs[input_name] = input_key\n            elif isinstance(input_key, str):\n                inputs[input_name] = getattr(batch, input_key)\n            else:\n                raise Exception(\n                    \"Unknown network input key {}, can't be given to \"\n                    \"network\".format(input_key)\n                )\n\n        return inputs",
  "def __init__(\n        self,\n        graph,\n        optimizer,\n        loss,\n        inputs,\n        outputs,\n        gradients,\n        summary=None,\n        array_specs=None,\n        save_every=2000,\n        log_dir=\"./\",\n        log_every=1,\n    ):\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=False\n        )\n        self.meta_graph_filename = graph\n        self.optimizer_func = None\n        self.optimizer_loss_names = None\n        self.optimizer = None\n        self.loss = None\n        self.summary = summary\n        self.session = None\n        self.tf_gradient = {}\n        self.graph = None\n        self.basic_saver = None\n        self.full_saver = None\n        self.save_every = save_every\n        self.iteration = None\n        self.iteration_increment = None\n        self.summary_saver = None\n        self.log_dir = log_dir\n        self.log_every = log_every\n        # Check if optimizer is a str in python 2/3 compatible way.\n        if isinstance(optimizer, (\"\".__class__, \"\".__class__)):\n            self.optimizer_loss_names = (optimizer, loss)\n        else:\n            self.optimizer_func = optimizer\n\n        # at least for some versions of tensorflow, the checkpoint name has to\n        # start with a . if it is a relative path\n        if not os.path.isabs(self.meta_graph_filename):\n            self.meta_graph_filename = os.path.join(\".\", self.meta_graph_filename)",
  "def start(self):\n        target = LocalServer.get_target()\n        logger.info(\"Initializing tf session, connecting to %s...\", target)\n\n        self.graph = tf.Graph()\n        self.session = tf.Session(target=target, graph=self.graph)\n\n        with self.graph.as_default():\n            self.__read_meta_graph()\n\n        if self.summary is not None:\n            self.summary_saver = tf.summary.FileWriter(self.log_dir, self.graph)\n\n        if self.optimizer_func is None:\n            # get actual operations/tensors from names\n            self.optimizer = self.graph.get_operation_by_name(\n                self.optimizer_loss_names[0]\n            )\n            self.loss = self.graph.get_tensor_by_name(self.optimizer_loss_names[1])\n\n        # add symbolic gradients\n        for tensor_name in self.gradients:\n            tensor = self.graph.get_tensor_by_name(tensor_name)\n            self.tf_gradient[tensor_name] = tf.gradients(self.loss, [tensor])[0]",
  "def train_step(self, batch, request):\n        array_outputs = self.__collect_requested_outputs(request)\n        inputs = self.__collect_provided_inputs(batch)\n\n        to_compute = {\n            \"optimizer\": self.optimizer,\n            \"loss\": self.loss,\n            \"iteration\": self.iteration_increment,\n        }\n        to_compute.update(array_outputs)\n\n        # compute outputs, gradients, and update variables\n        if isinstance(self.summary, str):\n            to_compute[\"summaries\"] = self.summary\n        elif isinstance(self.summary, dict):\n            for k, (v, f) in self.summary.items():\n                if int(self.current_step + 1) % f == 0:\n                    to_compute[k] = v\n        outputs = self.session.run(to_compute, feed_dict=inputs)\n\n        for array_key in array_outputs:\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(outputs[array_key], spec)\n\n        batch.loss = outputs[\"loss\"]\n        batch.iteration = outputs[\"iteration\"][0]\n        self.current_step = batch.iteration\n        if self.summary is not None:\n            if isinstance(self.summary, str) and (\n                batch.iteration % self.log_every == 0 or batch.iteration == 1\n            ):\n                self.summary_saver.add_summary(outputs[\"summaries\"], batch.iteration)\n            else:\n                for k, (_, f) in self.summary.items():\n                    if int(self.current_step) % f == 0:\n                        self.summary_saver.add_summary(outputs[k], batch.iteration)\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = (\n                self.meta_graph_filename + \"_checkpoint_%i\" % batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            self.full_saver.save(self.session, checkpoint_name)",
  "def stop(self):\n        if self.session is not None:\n            self.optimizer = None\n            self.loss = None\n            if self.summary is not None:\n                self.summary_saver.close()\n            self.session.close()\n            self.graph = None\n            self.session = None",
  "def __read_meta_graph(self):\n        logger.info(\"Reading meta-graph...\")\n\n        # read the original meta-graph\n        tf.train.import_meta_graph(\n            self.meta_graph_filename + \".meta\", clear_devices=True\n        )\n\n        # add custom gunpowder variables\n        with tf.variable_scope(\"gunpowder\"):\n            self.iteration = tf.get_variable(\n                \"iteration\", shape=1, initializer=tf.zeros_initializer, trainable=False\n            )\n            self.iteration_increment = tf.assign(self.iteration, self.iteration + 1)\n\n        # Until now, only variables have been added to the graph that are part\n        # of every checkpoint. We create a 'basic_saver' for only those\n        # variables.\n        self.basic_saver = tf.train.Saver(max_to_keep=None)\n\n        # Add custom optimizer and loss, if requested. This potentially adds\n        # more variables, not covered by the basic_saver.\n        if self.optimizer_func is not None:\n            loss, optimizer = self.optimizer_func(self.graph)\n            self.loss = loss\n            self.optimizer = optimizer\n\n        # We create a 'full_saver' including those variables.\n        self.full_saver = tf.train.Saver(max_to_keep=None)\n\n        # find most recent checkpoint\n        checkpoint_dir = os.path.dirname(self.meta_graph_filename)\n        checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n\n        if checkpoint:\n            try:\n                # Try to restore the graph, including the custom optimizer\n                # state (if a custom optimizer was used).\n                self.__restore_graph(checkpoint, restore_full=True)\n\n            except tf.errors.NotFoundError:\n                # If that failed, we just transitioned from an earlier training\n                # without the custom optimizer. In this case, restore only the\n                # variables of the original meta-graph and 'gunpowder'\n                # variables. Custom optimizer variables will be default\n                # initialized.\n                logger.info(\"Checkpoint did not contain custom optimizer \" \"variables\")\n                self.__restore_graph(checkpoint, restore_full=False)\n        else:\n            logger.info(\"No checkpoint found\")\n\n            # initialize all variables\n            self.session.run(tf.global_variables_initializer())",
  "def __restore_graph(self, checkpoint, restore_full):\n        logger.info(\"Restoring model from %s\", checkpoint)\n\n        if restore_full:\n            logger.info(\"...using a saver for all variables\")\n            self.full_saver.restore(self.session, checkpoint)\n\n        else:\n            # initialize all variables, such that non-basic variables are\n            # initialized\n            self.session.run(tf.global_variables_initializer())\n\n            logger.info(\"...using a saver for basic variables only\")\n            self.basic_saver.restore(self.session, checkpoint)\n\n        self.current_step = self.session.run(self.iteration)",
  "def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        for output_name, array_key in self.gradients.items():\n            if array_key in request:\n                array_outputs[array_key] = self.tf_gradient[output_name]\n\n        return array_outputs",
  "def __collect_provided_inputs(self, batch):\n        inputs = {}\n\n        for input_name, input_key in self.inputs.items():\n            if isinstance(input_key, ArrayKey):\n                if input_key in batch.arrays:\n                    inputs[input_name] = batch.arrays[input_key].data\n                else:\n                    logger.warn(\n                        \"batch does not contain %s, input %s will not \" \"be set\",\n                        input_key,\n                        input_name,\n                    )\n            elif isinstance(input_key, np.ndarray):\n                inputs[input_name] = input_key\n            elif isinstance(input_key, str):\n                inputs[input_name] = getattr(batch, input_key)\n            else:\n                raise Exception(\n                    \"Unknown network input key {}, can't be given to \"\n                    \"network\".format(input_key)\n                )\n\n        return inputs",
  "class Predict(GenericPredict):\n    \"\"\"Tensorflow implementation of :class:`gunpowder.nodes.Predict`.\n\n    Args:\n\n        checkpoint (``string``):\n\n            Basename of a tensorflow checkpoint storing the tensorflow graph\n            and associated tensor values and metadata, as created by\n            :class:`gunpowder.nodes.Train`, for example.\n\n        inputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of input tensors in the network to\n            array keys.\n\n        outputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of output tensors in the network to array\n            keys. New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (``outputs``). This is\n            useful to set the ``voxel_size``, for example, if they differ from\n            the voxel size of the input arrays. Only fields that are not\n            ``None`` in the given :class:`ArraySpec` will be used.\n\n        graph: (``string``, optional):\n\n            An optional path to a tensorflow computation graph that should be\n            used for prediction. The checkpoint is used to restore the values\n            of matching variable names in the graph. Note that the graph\n            specified here can differ from the one associated to the\n            checkpoint.\n\n        skip_empty (``bool``, optional):\n\n            Skip prediction, if all inputs are empty (contain only 0). In this\n            case, outputs are simply set to 0.\n\n        max_shared_memory (``int``, optional):\n\n            The maximal amount of shared memory in bytes to allocate to send\n            batches to the GPU processes. Defaults to 1GB.\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint,\n        inputs,\n        outputs,\n        array_specs=None,\n        graph=None,\n        skip_empty=False,\n        max_shared_memory=1024 * 1024 * 1024,\n    ):\n        super(Predict, self).__init__(inputs, outputs, array_specs)\n\n        self.checkpoint = checkpoint\n        self.meta_graph = graph\n        self.session = None\n        self.graph = None\n        self.skip_empty = skip_empty\n\n        self.manager = mp.Manager()\n        self.max_shared_memory = max_shared_memory\n        self.shared_input_array_config = self.manager.dict()\n        self.shared_output_array_config = self.manager.dict()\n        self.shared_input_arrays = {}\n        self.shared_output_arrays = {}\n        self.shared_input_memory = mp.RawArray(ctypes.c_float, self.max_shared_memory)\n        self.shared_output_memory = mp.RawArray(ctypes.c_float, self.max_shared_memory)\n\n        self.send_lock = mp.Lock()\n        self.receive_lock = mp.Lock()\n        self.predict_process_initialized = mp.Event()\n        self.worker_sent_inputs = mp.Event()\n        self.predict_received_inputs = mp.Event()\n        self.predict_sent_outputs = mp.Event()\n\n        self.predict_process = mp.Process(target=self.__predict)\n        self.predict_process_crashed = mp.Value(\"i\", False)\n        self.predict_process.start()\n        self.predict_process_initialized.wait()\n\n    def predict(self, batch, request):\n        if not self.shared_output_arrays:\n            self.__init_shared_output_arrays()\n\n        if self.skip_empty:\n            can_skip = True\n            for array_key in self.inputs.values():\n                if batch[array_key].data.any():\n                    can_skip = False\n                    break\n\n            if can_skip:\n                logger.info(\"Skipping batch %i (all inputs are 0)\" % batch.id)\n\n                for name, array_key in self.outputs.items():\n                    shape = self.shared_output_arrays[name].shape\n                    dtype = self.shared_output_arrays[name].dtype\n\n                    spec = self.spec[array_key].copy()\n                    spec.roi = request[array_key].roi.copy()\n                    batch.arrays[array_key] = Array(np.zeros(shape, dtype=dtype), spec)\n\n                return\n\n        logger.debug(\"predicting in batch %i\", batch.id)\n\n        output_tensors = self.__collect_outputs(request)\n        input_data = self.__collect_provided_inputs(batch)\n\n        self.send_lock.acquire()\n\n        if not self.shared_input_arrays:\n            if not self.shared_input_array_config:\n                self.__create_shared_input_array_config(batch, request)\n            self.__init_shared_input_arrays()\n\n        self.__write_inputs_to_shared(input_data)\n        self.worker_sent_inputs.set()\n\n        self.receive_lock.acquire()\n\n        self.predict_received_inputs.wait()\n        self.__check_background_process([self.receive_lock, self.send_lock])\n\n        self.predict_received_inputs.clear()\n        self.send_lock.release()\n\n        self.predict_sent_outputs.wait()\n\n        self.predict_sent_outputs.clear()\n\n        output_data = self.__read_outputs_from_shared(output_tensors)\n\n        self.receive_lock.release()\n\n        for array_key in output_tensors:\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(output_data[array_key], spec)\n\n        logger.debug(\"predicted in batch %i\", batch.id)\n\n    def __predict(self):\n        \"\"\"The background predict process.\"\"\"\n\n        try:\n            # TODO: is the server still needed?\n            target = LocalServer.get_target()\n            logger.info(\"Initializing tf session, connecting to %s...\", target)\n\n            self.graph = tf.Graph()\n            self.session = tf.Session(target=target, graph=self.graph)\n\n            with self.graph.as_default():\n                self.__read_checkpoint()\n\n            if not self.shared_output_arrays:\n                if not self.shared_output_array_config:\n                    self.__create_shared_output_array_config()\n                self.__init_shared_output_arrays()\n\n            # from now on it is save to access the shared array configuration\n            self.predict_process_initialized.set()\n\n            # loop predict\n            while True:\n                # wait for inputs\n                self.worker_sent_inputs.wait()\n                self.worker_sent_inputs.clear()\n\n                if not self.shared_input_arrays:\n                    self.__init_shared_input_arrays()\n\n                # read inputs\n                input_data = self.__read_inputs_from_shared()\n                self.predict_received_inputs.set()\n\n                # compute outputs\n                output_data = self.session.run(\n                    {t: t for t in self.outputs.keys()}, feed_dict=input_data\n                )\n\n                # write outputs\n                self.__write_outputs_to_shared(output_data)\n                self.predict_sent_outputs.set()\n\n        except Exception as e:\n            self.predict_process_crashed.value = True\n\n            # release locks and events\n            self.predict_process_initialized.set()\n            self.worker_sent_inputs.clear()\n            self.predict_received_inputs.set()\n            self.predict_sent_outputs.set()\n            raise e\n\n    def teardown(self):\n        self.predict_process.terminate()\n        self.predict_process.join()\n\n    def __check_background_process(self, locks=[]):\n        if self.predict_process_crashed.value:\n            # release all locks before raising exception\n            for l in locks:\n                l.release()\n            raise RuntimeError(\"Background process died.\")\n\n    def __read_checkpoint(self):\n        # read the graph associated to the checkpoint\n        if self.meta_graph is None:\n            meta_graph_file = self.checkpoint + \".meta\"\n        # read alternative, custom graph\n        else:\n            meta_graph_file = self.meta_graph\n\n        logger.info(\n            \"Reading graph from %s and weights from %s...\",\n            meta_graph_file,\n            self.checkpoint,\n        )\n\n        saver = tf.train.import_meta_graph(meta_graph_file, clear_devices=True)\n\n        # restore variables from checkpoint\n        saver.restore(self.session, self.checkpoint)\n\n    def __collect_outputs(self, request=None):\n        \"\"\"Get a dict:\n\n            array key: tensor name\n\n        If request is not None, return only outputs that are in request.\n        \"\"\"\n\n        array_outputs = {}\n\n        for tensor_name, array_key in self.outputs.items():\n            if request is None or array_key in request:\n                array_outputs[array_key] = tensor_name\n\n        return array_outputs\n\n    def __collect_provided_inputs(self, batch):\n        \"\"\"Get a dict:\n\n        tensor name: ndarray\n        \"\"\"\n\n        inputs = {}\n\n        for input_name, input_key in self.inputs.items():\n            if isinstance(input_key, ArrayKey):\n                if input_key in batch.arrays:\n                    inputs[input_name] = batch.arrays[input_key].data\n                else:\n                    logger.warn(\n                        \"batch does not contain %s, input %s will not \" \"be set\",\n                        input_key,\n                        input_name,\n                    )\n            elif isinstance(input_key, np.ndarray):\n                inputs[input_name] = input_key\n            elif isinstance(input_key, str):\n                inputs[input_name] = getattr(batch, input_key)\n            else:\n                raise Exception(\n                    \"Unknown network input key {}, can't be given to \"\n                    \"network\".format(input_key)\n                )\n\n        return inputs\n\n    def __create_shared_input_array_config(self, batch, request):\n        \"\"\"Store the shared array config in a shared dictionary. Should be run\n        once by the first worker to submit a batch.\"\"\"\n\n        begin = 0\n        for name, array_key in self.inputs.items():\n            shape = batch[array_key].data.shape\n            size = reduce(mul, shape, 1)\n            dtype = batch[array_key].data.dtype\n\n            self.shared_input_array_config[name] = (begin, size, shape, dtype)\n\n            begin += size * np.dtype(dtype).itemsize\n            assert (\n                begin <= self.max_shared_memory\n            ), \"The input arrays exceed the max_shared_memory\"\n\n    def __create_shared_output_array_config(self):\n        \"\"\"To be called by predict process.\"\"\"\n\n        begin = 0\n        for name, array_key in self.outputs.items():\n            tensor = self.graph.get_tensor_by_name(name)\n            shape = tensor.get_shape().as_list()\n            size = reduce(mul, shape, 1)\n            dtype = tensor.dtype.as_numpy_dtype\n\n            self.shared_output_array_config[name] = (begin, size, tuple(shape), dtype)\n\n            begin += size * np.dtype(dtype).itemsize\n            assert (\n                begin <= self.max_shared_memory\n            ), \"The output arrays exceed the max_shared_memory\"\n\n    def __init_shared_input_arrays(self):\n        \"\"\"Assign the shared memory to numpy arrays.\"\"\"\n\n        for name, (begin, size, shape, dtype) in self.shared_input_array_config.items():\n            self.shared_input_arrays[name] = np.frombuffer(\n                self.shared_input_memory, dtype=dtype, offset=begin, count=size\n            ).reshape(shape)\n\n    def __init_shared_output_arrays(self):\n        \"\"\"Assign the shared memory to numpy arrays.\"\"\"\n\n        for name, (\n            begin,\n            size,\n            shape,\n            dtype,\n        ) in self.shared_output_array_config.items():\n            self.shared_output_arrays[name] = np.frombuffer(\n                self.shared_output_memory, dtype=dtype, offset=begin, count=size\n            ).reshape(shape)\n\n    def __write_inputs_to_shared(self, input_data):\n        for tensor_name, data in input_data.items():\n            self.shared_input_arrays[tensor_name][:] = data\n\n    def __read_inputs_from_shared(self):\n        return {\n            tensor_name: self.shared_input_arrays[tensor_name].copy()\n            for tensor_name in self.inputs.keys()\n        }\n\n    def __write_outputs_to_shared(self, output_data):\n        for tensor_name, data in output_data.items():\n            self.shared_output_arrays[tensor_name][:] = data\n\n    def __read_outputs_from_shared(self, output_tensors):\n        return {\n            array_key: self.shared_output_arrays[tensor_name].copy()\n            for array_key, tensor_name in output_tensors.items()\n        }",
  "def __init__(\n        self,\n        checkpoint,\n        inputs,\n        outputs,\n        array_specs=None,\n        graph=None,\n        skip_empty=False,\n        max_shared_memory=1024 * 1024 * 1024,\n    ):\n        super(Predict, self).__init__(inputs, outputs, array_specs)\n\n        self.checkpoint = checkpoint\n        self.meta_graph = graph\n        self.session = None\n        self.graph = None\n        self.skip_empty = skip_empty\n\n        self.manager = mp.Manager()\n        self.max_shared_memory = max_shared_memory\n        self.shared_input_array_config = self.manager.dict()\n        self.shared_output_array_config = self.manager.dict()\n        self.shared_input_arrays = {}\n        self.shared_output_arrays = {}\n        self.shared_input_memory = mp.RawArray(ctypes.c_float, self.max_shared_memory)\n        self.shared_output_memory = mp.RawArray(ctypes.c_float, self.max_shared_memory)\n\n        self.send_lock = mp.Lock()\n        self.receive_lock = mp.Lock()\n        self.predict_process_initialized = mp.Event()\n        self.worker_sent_inputs = mp.Event()\n        self.predict_received_inputs = mp.Event()\n        self.predict_sent_outputs = mp.Event()\n\n        self.predict_process = mp.Process(target=self.__predict)\n        self.predict_process_crashed = mp.Value(\"i\", False)\n        self.predict_process.start()\n        self.predict_process_initialized.wait()",
  "def predict(self, batch, request):\n        if not self.shared_output_arrays:\n            self.__init_shared_output_arrays()\n\n        if self.skip_empty:\n            can_skip = True\n            for array_key in self.inputs.values():\n                if batch[array_key].data.any():\n                    can_skip = False\n                    break\n\n            if can_skip:\n                logger.info(\"Skipping batch %i (all inputs are 0)\" % batch.id)\n\n                for name, array_key in self.outputs.items():\n                    shape = self.shared_output_arrays[name].shape\n                    dtype = self.shared_output_arrays[name].dtype\n\n                    spec = self.spec[array_key].copy()\n                    spec.roi = request[array_key].roi.copy()\n                    batch.arrays[array_key] = Array(np.zeros(shape, dtype=dtype), spec)\n\n                return\n\n        logger.debug(\"predicting in batch %i\", batch.id)\n\n        output_tensors = self.__collect_outputs(request)\n        input_data = self.__collect_provided_inputs(batch)\n\n        self.send_lock.acquire()\n\n        if not self.shared_input_arrays:\n            if not self.shared_input_array_config:\n                self.__create_shared_input_array_config(batch, request)\n            self.__init_shared_input_arrays()\n\n        self.__write_inputs_to_shared(input_data)\n        self.worker_sent_inputs.set()\n\n        self.receive_lock.acquire()\n\n        self.predict_received_inputs.wait()\n        self.__check_background_process([self.receive_lock, self.send_lock])\n\n        self.predict_received_inputs.clear()\n        self.send_lock.release()\n\n        self.predict_sent_outputs.wait()\n\n        self.predict_sent_outputs.clear()\n\n        output_data = self.__read_outputs_from_shared(output_tensors)\n\n        self.receive_lock.release()\n\n        for array_key in output_tensors:\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(output_data[array_key], spec)\n\n        logger.debug(\"predicted in batch %i\", batch.id)",
  "def __predict(self):\n        \"\"\"The background predict process.\"\"\"\n\n        try:\n            # TODO: is the server still needed?\n            target = LocalServer.get_target()\n            logger.info(\"Initializing tf session, connecting to %s...\", target)\n\n            self.graph = tf.Graph()\n            self.session = tf.Session(target=target, graph=self.graph)\n\n            with self.graph.as_default():\n                self.__read_checkpoint()\n\n            if not self.shared_output_arrays:\n                if not self.shared_output_array_config:\n                    self.__create_shared_output_array_config()\n                self.__init_shared_output_arrays()\n\n            # from now on it is save to access the shared array configuration\n            self.predict_process_initialized.set()\n\n            # loop predict\n            while True:\n                # wait for inputs\n                self.worker_sent_inputs.wait()\n                self.worker_sent_inputs.clear()\n\n                if not self.shared_input_arrays:\n                    self.__init_shared_input_arrays()\n\n                # read inputs\n                input_data = self.__read_inputs_from_shared()\n                self.predict_received_inputs.set()\n\n                # compute outputs\n                output_data = self.session.run(\n                    {t: t for t in self.outputs.keys()}, feed_dict=input_data\n                )\n\n                # write outputs\n                self.__write_outputs_to_shared(output_data)\n                self.predict_sent_outputs.set()\n\n        except Exception as e:\n            self.predict_process_crashed.value = True\n\n            # release locks and events\n            self.predict_process_initialized.set()\n            self.worker_sent_inputs.clear()\n            self.predict_received_inputs.set()\n            self.predict_sent_outputs.set()\n            raise e",
  "def teardown(self):\n        self.predict_process.terminate()\n        self.predict_process.join()",
  "def __check_background_process(self, locks=[]):\n        if self.predict_process_crashed.value:\n            # release all locks before raising exception\n            for l in locks:\n                l.release()\n            raise RuntimeError(\"Background process died.\")",
  "def __read_checkpoint(self):\n        # read the graph associated to the checkpoint\n        if self.meta_graph is None:\n            meta_graph_file = self.checkpoint + \".meta\"\n        # read alternative, custom graph\n        else:\n            meta_graph_file = self.meta_graph\n\n        logger.info(\n            \"Reading graph from %s and weights from %s...\",\n            meta_graph_file,\n            self.checkpoint,\n        )\n\n        saver = tf.train.import_meta_graph(meta_graph_file, clear_devices=True)\n\n        # restore variables from checkpoint\n        saver.restore(self.session, self.checkpoint)",
  "def __collect_outputs(self, request=None):\n        \"\"\"Get a dict:\n\n            array key: tensor name\n\n        If request is not None, return only outputs that are in request.\n        \"\"\"\n\n        array_outputs = {}\n\n        for tensor_name, array_key in self.outputs.items():\n            if request is None or array_key in request:\n                array_outputs[array_key] = tensor_name\n\n        return array_outputs",
  "def __collect_provided_inputs(self, batch):\n        \"\"\"Get a dict:\n\n        tensor name: ndarray\n        \"\"\"\n\n        inputs = {}\n\n        for input_name, input_key in self.inputs.items():\n            if isinstance(input_key, ArrayKey):\n                if input_key in batch.arrays:\n                    inputs[input_name] = batch.arrays[input_key].data\n                else:\n                    logger.warn(\n                        \"batch does not contain %s, input %s will not \" \"be set\",\n                        input_key,\n                        input_name,\n                    )\n            elif isinstance(input_key, np.ndarray):\n                inputs[input_name] = input_key\n            elif isinstance(input_key, str):\n                inputs[input_name] = getattr(batch, input_key)\n            else:\n                raise Exception(\n                    \"Unknown network input key {}, can't be given to \"\n                    \"network\".format(input_key)\n                )\n\n        return inputs",
  "def __create_shared_input_array_config(self, batch, request):\n        \"\"\"Store the shared array config in a shared dictionary. Should be run\n        once by the first worker to submit a batch.\"\"\"\n\n        begin = 0\n        for name, array_key in self.inputs.items():\n            shape = batch[array_key].data.shape\n            size = reduce(mul, shape, 1)\n            dtype = batch[array_key].data.dtype\n\n            self.shared_input_array_config[name] = (begin, size, shape, dtype)\n\n            begin += size * np.dtype(dtype).itemsize\n            assert (\n                begin <= self.max_shared_memory\n            ), \"The input arrays exceed the max_shared_memory\"",
  "def __create_shared_output_array_config(self):\n        \"\"\"To be called by predict process.\"\"\"\n\n        begin = 0\n        for name, array_key in self.outputs.items():\n            tensor = self.graph.get_tensor_by_name(name)\n            shape = tensor.get_shape().as_list()\n            size = reduce(mul, shape, 1)\n            dtype = tensor.dtype.as_numpy_dtype\n\n            self.shared_output_array_config[name] = (begin, size, tuple(shape), dtype)\n\n            begin += size * np.dtype(dtype).itemsize\n            assert (\n                begin <= self.max_shared_memory\n            ), \"The output arrays exceed the max_shared_memory\"",
  "def __init_shared_input_arrays(self):\n        \"\"\"Assign the shared memory to numpy arrays.\"\"\"\n\n        for name, (begin, size, shape, dtype) in self.shared_input_array_config.items():\n            self.shared_input_arrays[name] = np.frombuffer(\n                self.shared_input_memory, dtype=dtype, offset=begin, count=size\n            ).reshape(shape)",
  "def __init_shared_output_arrays(self):\n        \"\"\"Assign the shared memory to numpy arrays.\"\"\"\n\n        for name, (\n            begin,\n            size,\n            shape,\n            dtype,\n        ) in self.shared_output_array_config.items():\n            self.shared_output_arrays[name] = np.frombuffer(\n                self.shared_output_memory, dtype=dtype, offset=begin, count=size\n            ).reshape(shape)",
  "def __write_inputs_to_shared(self, input_data):\n        for tensor_name, data in input_data.items():\n            self.shared_input_arrays[tensor_name][:] = data",
  "def __read_inputs_from_shared(self):\n        return {\n            tensor_name: self.shared_input_arrays[tensor_name].copy()\n            for tensor_name in self.inputs.keys()\n        }",
  "def __write_outputs_to_shared(self, output_data):\n        for tensor_name, data in output_data.items():\n            self.shared_output_arrays[tensor_name][:] = data",
  "def __read_outputs_from_shared(self, output_tensors):\n        return {\n            array_key: self.shared_output_arrays[tensor_name].copy()\n            for array_key, tensor_name in output_tensors.items()\n        }",
  "def conv_pass(\n    fmaps_in,\n    kernel_size,\n    num_fmaps,\n    num_repetitions,\n    activation=\"relu\",\n    name=\"conv_pass\",\n):\n    \"\"\"Create a convolution pass::\n\n        f_in --> f_1 --> ... --> f_n\n\n    where each ``-->`` is a convolution followed by a (non-linear) activation\n    function and ``n`` ``num_repetitions``. Each convolution will decrease the\n    size of the feature maps by ``kernel_size-1``.\n\n    Args:\n\n        f_in:\n\n            The input tensor of shape ``(batch_size, channels, depth, height,\n            width)`` or ``(batch_size, channels, height, width)``.\n\n        kernel_size:\n\n            Size of the kernel. Forwarded to the tensorflow convolution layer.\n\n        num_fmaps:\n\n            The number of feature maps to produce with each convolution.\n\n        num_repetitions:\n\n            How many convolutions to apply.\n\n        activation:\n\n            Which activation to use after a convolution. Accepts the name of any\n            tensorflow activation function (e.g., ``relu`` for ``tf.nn.relu``).\n\n    \"\"\"\n\n    fmaps = fmaps_in\n    if activation is not None:\n        activation = getattr(tf.nn, activation)\n\n    conv_layer = getattr(\n        tf.layers, {2: \"conv2d\", 3: \"conv3d\"}[fmaps_in.get_shape().ndims - 2]\n    )\n\n    for i in range(num_repetitions):\n        fmaps = conv_layer(\n            inputs=fmaps,\n            filters=num_fmaps,\n            kernel_size=kernel_size,\n            padding=\"valid\",\n            data_format=\"channels_first\",\n            activation=activation,\n            name=name + \"_%i\" % i,\n        )\n\n    return fmaps",
  "def downsample(fmaps_in, factors, name=\"down\"):\n    pooling_layer = getattr(\n        tf.layers,\n        {2: \"max_pooling2d\", 3: \"max_pooling3d\"}[fmaps_in.get_shape().ndims - 2],\n    )\n\n    fmaps = pooling_layer(\n        fmaps_in,\n        pool_size=factors,\n        strides=factors,\n        padding=\"valid\",\n        data_format=\"channels_first\",\n        name=name,\n    )\n\n    return fmaps",
  "def upsample(fmaps_in, factors, num_fmaps, activation=\"relu\", name=\"up\"):\n    if activation is not None:\n        activation = getattr(tf.nn, activation)\n\n    conv_trans_layer = getattr(\n        tf.layers,\n        {2: \"conv2d_transpose\", 3: \"conv3d_transpose\"}[fmaps_in.get_shape().ndims - 2],\n    )\n\n    fmaps = conv_trans_layer(\n        fmaps_in,\n        filters=num_fmaps,\n        kernel_size=factors,\n        strides=factors,\n        padding=\"valid\",\n        data_format=\"channels_first\",\n        activation=activation,\n        name=name,\n    )\n\n    return fmaps",
  "def crop_spatial(fmaps_in, shape):\n    \"\"\"Crop only the spacial dimensions to match shape.\n\n    Args:\n\n        fmaps_in:\n\n            The input tensor.\n\n        shape:\n\n            A list (not a tensor) with the requested shape [_, _, z, y, x] or\n            [_, _, y, x].\n    \"\"\"\n\n    in_shape = fmaps_in.get_shape().as_list()\n\n    offset = [0, 0] + [(in_shape[i] - shape[i]) // 2 for i in range(2, len(shape))]\n    size = in_shape[0:2] + shape[2:]\n\n    fmaps = tf.slice(fmaps_in, offset, size)\n\n    return fmaps",
  "def unet(\n    fmaps_in, num_fmaps, fmap_inc_factor, downsample_factors, activation=\"relu\", layer=0\n):\n    \"\"\"Create a 2D or 3D U-Net::\n\n        f_in --> f_left --------------------------->> f_right--> f_out\n                    |                                   ^\n                    v                                   |\n                 g_in --> g_left ------->> g_right --> g_out\n                             |               ^\n                             v               |\n                                   ...\n\n    where each ``-->`` is a convolution pass (see ``conv_pass``), each `-->>` a\n    crop, and down and up arrows are max-pooling and transposed convolutions,\n    respectively.\n\n    The U-Net expects tensors to have shape ``(batch=1, channels, depth, height,\n    width)`` for 3D or ``(batch=1, channels, height, width)`` for 2D.\n\n    This U-Net performs only \"valid\" convolutions, i.e., sizes of the feature\n    maps decrease after each convolution.\n\n    Args:\n\n        fmaps_in:\n\n            The input tensor.\n\n        num_fmaps:\n\n            The number of feature maps in the first layer. This is also the\n            number of output feature maps.\n\n        fmap_inc_factor:\n\n            By how much to multiply the number of feature maps between layers.\n            If layer 0 has ``k`` feature maps, layer ``l`` will have\n            ``k*fmap_inc_factor**l``.\n\n        downsample_factors:\n\n            List of lists ``[z, y, x]`` or ``[y, x]`` to use to down- and\n            up-sample the feature maps between layers.\n\n        activation:\n\n            Which activation to use after a convolution. Accepts the name of any\n            tensorflow activation function (e.g., ``relu`` for ``tf.nn.relu``).\n\n        layer:\n\n            Used internally to build the U-Net recursively.\n    \"\"\"\n\n    prefix = \"    \" * layer\n    print(prefix + \"Creating U-Net layer %i\" % layer)\n    print(prefix + \"f_in: \" + str(fmaps_in.shape))\n\n    # convolve\n    f_left = conv_pass(\n        fmaps_in,\n        kernel_size=3,\n        num_fmaps=num_fmaps,\n        num_repetitions=2,\n        activation=activation,\n        name=\"unet_layer_%i_left\" % layer,\n    )\n\n    # last layer does not recurse\n    bottom_layer = layer == len(downsample_factors)\n    if bottom_layer:\n        print(prefix + \"bottom layer\")\n        print(prefix + \"f_out: \" + str(f_left.shape))\n        return f_left\n\n    # downsample\n    g_in = downsample(\n        f_left, downsample_factors[layer], \"unet_down_%i_to_%i\" % (layer, layer + 1)\n    )\n\n    # recursive U-net\n    g_out = unet(\n        g_in,\n        num_fmaps=num_fmaps * fmap_inc_factor,\n        fmap_inc_factor=fmap_inc_factor,\n        downsample_factors=downsample_factors,\n        activation=activation,\n        layer=layer + 1,\n    )\n\n    print(prefix + \"g_out: \" + str(g_out.shape))\n\n    # upsample\n    g_out_upsampled = upsample(\n        g_out,\n        downsample_factors[layer],\n        num_fmaps,\n        activation=activation,\n        name=\"unet_up_%i_to_%i\" % (layer + 1, layer),\n    )\n\n    print(prefix + \"g_out_upsampled: \" + str(g_out_upsampled.shape))\n\n    # copy-crop\n    f_left_cropped = crop_spatial(f_left, g_out_upsampled.get_shape().as_list())\n\n    print(prefix + \"f_left_cropped: \" + str(f_left_cropped.shape))\n\n    # concatenate along channel dimension\n    f_right = tf.concat([f_left_cropped, g_out_upsampled], 1)\n\n    print(prefix + \"f_right: \" + str(f_right.shape))\n\n    # convolve\n    f_out = conv_pass(\n        f_right,\n        kernel_size=3,\n        num_fmaps=num_fmaps,\n        num_repetitions=2,\n        name=\"unet_layer_%i_right\" % layer,\n    )\n\n    print(prefix + \"f_out: \" + str(f_out.shape))\n\n    return f_out",
  "class NoSuchModule(object):\n    def __init__(self, name):\n        self.__name = name\n        self.__traceback_str = traceback.format_tb(sys.exc_info()[2])\n        errtype, value = sys.exc_info()[:2]\n        self.__exception = errtype(value)\n\n    def __getattr__(self, item):\n        raise self.__exception",
  "def __init__(self, name):\n        self.__name = name\n        self.__traceback_str = traceback.format_tb(sys.exc_info()[2])\n        errtype, value = sys.exc_info()[:2]\n        self.__exception = errtype(value)",
  "def __getattr__(self, item):\n        raise self.__exception",
  "class ZarrFile:\n    \"\"\"To be used as a context manager, similar to h5py.File.\"\"\"\n\n    def __init__(self, store: Union[BaseStore, MutableMapping, str], mode=\"a\"):\n        self.store = store\n        self.mode = mode\n\n    def __enter__(self):\n        return zarr.open(self.store, mode=self.mode)\n\n    def __exit__(self, *args):\n        pass",
  "def __init__(self, store: Union[BaseStore, MutableMapping, str], mode=\"a\"):\n        self.store = store\n        self.mode = mode",
  "def __enter__(self):\n        return zarr.open(self.store, mode=self.mode)",
  "def __exit__(self, *args):\n        pass",
  "class GenericJaxModel:\n    \"\"\"An interface for models to follow in order to train or predict. A model\n    implementing this interface will need to contain not only the forward\n    model but also loss and update fn. Some examples can be found in\n    https://github.com/funkelab/funlib.learn.jax\n\n    Args:\n\n        is_training (``bool``):\n\n            Indicating whether the model will be used for training\n            or inferencing.\n    \"\"\"\n\n    def __init__(self, is_training):\n        pass\n\n    def initialize(self, rng_key, inputs):\n        \"\"\"Initialize parameters for training.\n\n        Args:\n\n            rng_key (jax.random.PRNGKey):\n\n                Seed for parameter initialization\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs, provided to initialize parameters\n                with the correct dimensions.\n\n        Return:\n\n            params (Any):\n\n                Function should return an object encapsulating different\n                parameters of the model.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")\n\n    def forward(self, params, inputs):\n        \"\"\"Run the forward model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n        Return:\n\n            outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of outputs.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")\n\n    def train_step(self, params, inputs, pmapped):\n        \"\"\"Run one iteration of training on the model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n            pmapped (``bool``):\n\n                Whether the function is run with `jax.pmap` or not.\n                If pmapped across devices, the function should take care to\n                synchronize gradients during the train step.\n                The `axis_name` is set to the ``string`` \"num_devices\"\n\n        Return:\n\n            Tuple(new_params, outputs, loss)\n\n                new_params (Any):\n\n                    Updated model parameters.\n\n                outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                    Dictionary of outputs.\n\n                loss (Union[``float``, (``dict``, ``string`` -> ``float``)]):\n\n                    Loss value of this iteration. Value can either be a single\n                    ``float`` or a dictionary of multiple losses.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")",
  "def __init__(self, is_training):\n        pass",
  "def initialize(self, rng_key, inputs):\n        \"\"\"Initialize parameters for training.\n\n        Args:\n\n            rng_key (jax.random.PRNGKey):\n\n                Seed for parameter initialization\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs, provided to initialize parameters\n                with the correct dimensions.\n\n        Return:\n\n            params (Any):\n\n                Function should return an object encapsulating different\n                parameters of the model.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")",
  "def forward(self, params, inputs):\n        \"\"\"Run the forward model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n        Return:\n\n            outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of outputs.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")",
  "def train_step(self, params, inputs, pmapped):\n        \"\"\"Run one iteration of training on the model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n            pmapped (``bool``):\n\n                Whether the function is run with `jax.pmap` or not.\n                If pmapped across devices, the function should take care to\n                synchronize gradients during the train step.\n                The `axis_name` is set to the ``string`` \"num_devices\"\n\n        Return:\n\n            Tuple(new_params, outputs, loss)\n\n                new_params (Any):\n\n                    Updated model parameters.\n\n                outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                    Dictionary of outputs.\n\n                loss (Union[``float``, (``dict``, ``string`` -> ``float``)]):\n\n                    Loss value of this iteration. Value can either be a single\n                    ``float`` or a dictionary of multiple losses.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")",
  "class Train(GenericTrain):\n    \"\"\"JAX implementation of :class:`gunpowder.nodes.GenericTrain`.\n\n    Args:\n\n        model (subclass of ``gunpowder.jax.GenericJaxModel``):\n\n            The model to train. This model encapsulates the forward model,\n            loss, and optimizer.\n\n        inputs (``dict``, ``string`` -> Union[np.ndarray, ArrayKey]):\n\n            Dictionary from the names of input tensors expected by the\n            ``train_step`` method to array keys or ndarray.\n\n        outputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of tensors in the network to array\n            keys. If the key is a string, the tensor will be retrieved\n            by checking the model for an attribute with they key as its name.\n            If the key is an integer, it is interpreted as a tuple index of\n            the outputs of the network.\n            New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (at the moment only\n            ``output``). This is useful to set the ``voxel_size``, for example,\n            if they differ from the voxel size of the input arrays. Only fields\n            that are not ``None`` in the given :class:`ArraySpec` will be used.\n\n        checkpoint_basename (``string``, optional):\n\n            The basename used for checkpoint files. Defaults to ``model``.\n\n        save_every (``int``, optional):\n\n            After how many iterations to create a checkpoint to store the\n            learnt weights.\n\n        keep_n_checkpoints (``int``, optional):\n\n            Number of checkpoints to keep. Node will attempt to delete older\n            checkpoints. Default is `None` (no deletion).\n\n        log_dir (``string``, optional):\n\n            Directory for saving tensorboard summaries.\n\n        log_every (``int``, optional):\n\n            After how many iterations to write out tensorboard summaries.\n\n        spawn_subprocess (``bool``, optional):\n\n            Whether to run the ``train_step`` in a separate process. Default is\n            false.\n\n        n_devices (``int``, optional):\n\n            Number of GPU devices to train on concurrently using `jax.pmap`. If\n            `None`, the number of available GPUs will be automatically detected\n            and used.\n\n        validate_fn (function -> Union[``float``, (``dict``, ``string`` -> ``float``)] , optional):\n\n            Function to run validation on, which should has the form of\n\n                def validate_fn(model, params)\n\n            where `model` is the same provided `GenericJaxModel` model and\n            `params` is the parameter of this model, and returns either a\n            ``float`` (one loss) or a dictionary of losses to record in\n            tensorboard.\n\n        validate_every (``int``, optional):\n\n            After how many iterations to run `validate_fn`.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: GenericJaxModel,\n        inputs: Dict[str, Union[np.ndarray, ArrayKey]],\n        outputs: Dict[Union[int, str], ArrayKey],\n        gradients: Dict[Union[int, str], ArrayKey] = {},\n        array_specs: Optional[Dict[ArrayKey, ArraySpec]] = None,\n        checkpoint_basename: str = \"model\",\n        save_every: int = 2000,\n        keep_n_checkpoints: Optional[int] = None,\n        log_dir: str = None,\n        log_every: int = 1,\n        spawn_subprocess: bool = False,\n        n_devices: Optional[int] = None,\n        validate_fn=None,\n        validate_every=None,\n    ):\n        # not yet implemented\n        gradients = gradients\n\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.checkpoint_basename = checkpoint_basename\n        self.save_every = save_every\n        if n_devices is None:\n            n_devices = jax.local_device_count()  # autodetect available GPUs\n        self.n_devices = n_devices\n        self.local_devices = jax.local_devices()\n        self.keep_n_checkpoints = keep_n_checkpoints\n\n        self.iteration = 0\n\n        if not isinstance(tensorboardX, NoSuchModule) and log_dir is not None:\n            self.summary_writer = tensorboardX.SummaryWriter(log_dir)\n            self.log_every = log_every\n        else:\n            self.summary_writer = None\n            if log_dir is not None:\n                logger.warning(\"log_dir given, but tensorboardX is not installed\")\n\n        self.intermediate_layers = {}\n\n        self.validate_fn = validate_fn\n        self.validate_every = validate_every\n\n    def replicate_params(self, params):\n        return jax.tree_map(lambda x: jnp.array([x] * self.n_devices), params)\n\n    def start(self):\n        checkpoint, self.iteration = self._get_latest_checkpoint(\n            self.checkpoint_basename\n        )\n\n        if checkpoint is not None:\n            logger.info(\"Resuming training from iteration %d\", self.iteration)\n\n            with open(checkpoint, \"rb\") as f:\n                self.model_params = pickle.load(f)\n                if self.n_devices > 1:\n                    self.model_params = self.replicate_params(self.model_params)\n        else:\n            logger.info(\"Starting training from scratch\")\n            self.model_params = None\n\n    def split_inputs(self, inputs):\n        for k, arr in inputs.items():\n            assert arr.shape[0] % self.n_devices == 0, (\n                f\"Batch size should be evenly divisible by the number of \"\n                f\"devices. Input array shape is {arr.shape} but n_device is\"\n                f\" {self.n_devices}\"\n            )\n            inputs[k] = arr.reshape(\n                self.n_devices, arr.shape[0] // self.n_devices, *arr.shape[1:]\n            )\n            inputs[k] = [x for x in inputs[k]]  # make a sequence for put_sharded\n        return inputs\n\n    def unstack_device_outputs(self, outputs):\n        for k, arr in outputs.items():\n            outputs[k] = arr.reshape(arr.shape[0] * arr.shape[1], *arr.shape[2:])\n        return outputs\n\n    def train_step(self, batch, request):\n        inputs = self.__collect_provided_inputs(batch)\n        if self.n_devices > 1:\n            inputs = self.split_inputs(inputs)\n\n        # put to device for max performance\n        if self.n_devices > 1:\n            for k, v in inputs.items():\n                inputs[k] = jax.device_put_sharded(v, jax.local_devices())\n        else:\n            for k, v in inputs.items():\n                inputs[k] = jax.device_put(v)\n\n        # initialize model if necessary\n        if self.model_params is None:\n            # Use the random seed of first request to initialize model's weight\n            rng = jax.random.PRNGKey(request.random_seed)\n            if self.n_devices > 1:\n                rng = jnp.broadcast_to(rng, (self.n_devices,) + rng.shape)\n                self.model_params = jax.pmap(self.model.initialize)(rng, inputs)\n            else:\n                self.model_params = self.model.initialize(rng, inputs)\n\n        requested_outputs = self.__collect_requested_outputs(request)\n\n        if self.n_devices > 1:\n            self.model_params, outputs, loss = jax.pmap(\n                self.model.train_step,\n                axis_name=\"num_devices\",\n                donate_argnums=(0,),\n                static_broadcasted_argnums=(2,),\n            )(self.model_params, inputs, True)\n            loss = loss.mean()\n            outputs = self.unstack_device_outputs(outputs)  # stack by batch\n        else:\n            self.model_params, outputs, loss = jax.jit(\n                self.model.train_step, donate_argnums=(0,), static_argnums=(2,)\n            )(self.model_params, inputs, False)\n\n        logger.debug(\"model outputs: %s\", {k: v.shape for k, v in outputs.items()})\n\n        # add requested model outputs to batch\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(outputs[array_name], spec)\n\n        if isinstance(loss, dict):\n            total = 0.0\n            for k, v in loss.items():\n                total += v\n            batch.loss = total\n        else:\n            batch.loss = loss\n        self.iteration += 1\n        batch.iteration = self.iteration\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = self._checkpoint_name(\n                self.checkpoint_basename, batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            model_state = self.model_params\n            if self.n_devices > 1:\n                # get only a single copy of param for saving\n                model_state = jax.tree_map(lambda x: x[0], model_state)\n            with open(checkpoint_name, \"wb\") as f:\n                pickle.dump(model_state, f)\n\n            if self.keep_n_checkpoints:\n                checkpoint_name = self._checkpoint_name(\n                    self.checkpoint_basename,\n                    batch.iteration - self.keep_n_checkpoints * self.save_every,\n                )\n                try:\n                    os.remove(checkpoint_name)\n                    logger.info(\"Removed checkpoint %s\", checkpoint_name)\n                except FileNotFoundError:\n                    pass\n\n        if self.summary_writer and batch.iteration % self.log_every == 0:\n            if isinstance(loss, dict):\n                for k, v in loss.items():\n                    self.summary_writer.add_scalar(k, v, batch.iteration)\n            else:\n                self.summary_writer.add_scalar(\"loss\", loss, batch.iteration)\n\n        # run validate\n        if self.validate_fn is not None and batch.iteration % self.validate_every == 0:\n            val_ret = self.validate_fn(self.model, self.model_params)\n            if isinstance(val_ret, dict):\n                for k, v in val_ret.items():\n                    self.summary_writer.add_scalar(k, v, batch.iteration)\n            else:\n                self.summary_writer.add_scalar(\"validate\", val_ret, batch.iteration)\n\n    def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        return array_outputs\n\n    def __collect_provided_inputs(self, batch):\n        return self.__collect_provided_arrays(self.inputs, batch)\n\n    def __collect_provided_arrays(self, reference, batch):\n        arrays = {}\n\n        for array_name, array_key in reference.items():\n            if isinstance(array_key, ArrayKey):\n                msg = f\"batch does not contain {array_key}, array {array_name} will not be set\"\n                if array_key in batch.arrays:\n                    arrays[array_name] = batch.arrays[array_key].data\n                else:\n                    logger.warn(msg)\n            elif isinstance(array_key, np.ndarray):\n                arrays[array_name] = array_key\n            else:\n                raise Exception(\n                    \"Unknown network array key {}, can't be given to \"\n                    \"network\".format(array_key)\n                )\n\n        return arrays",
  "def __init__(\n        self,\n        model: GenericJaxModel,\n        inputs: Dict[str, Union[np.ndarray, ArrayKey]],\n        outputs: Dict[Union[int, str], ArrayKey],\n        gradients: Dict[Union[int, str], ArrayKey] = {},\n        array_specs: Optional[Dict[ArrayKey, ArraySpec]] = None,\n        checkpoint_basename: str = \"model\",\n        save_every: int = 2000,\n        keep_n_checkpoints: Optional[int] = None,\n        log_dir: str = None,\n        log_every: int = 1,\n        spawn_subprocess: bool = False,\n        n_devices: Optional[int] = None,\n        validate_fn=None,\n        validate_every=None,\n    ):\n        # not yet implemented\n        gradients = gradients\n\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.checkpoint_basename = checkpoint_basename\n        self.save_every = save_every\n        if n_devices is None:\n            n_devices = jax.local_device_count()  # autodetect available GPUs\n        self.n_devices = n_devices\n        self.local_devices = jax.local_devices()\n        self.keep_n_checkpoints = keep_n_checkpoints\n\n        self.iteration = 0\n\n        if not isinstance(tensorboardX, NoSuchModule) and log_dir is not None:\n            self.summary_writer = tensorboardX.SummaryWriter(log_dir)\n            self.log_every = log_every\n        else:\n            self.summary_writer = None\n            if log_dir is not None:\n                logger.warning(\"log_dir given, but tensorboardX is not installed\")\n\n        self.intermediate_layers = {}\n\n        self.validate_fn = validate_fn\n        self.validate_every = validate_every",
  "def replicate_params(self, params):\n        return jax.tree_map(lambda x: jnp.array([x] * self.n_devices), params)",
  "def start(self):\n        checkpoint, self.iteration = self._get_latest_checkpoint(\n            self.checkpoint_basename\n        )\n\n        if checkpoint is not None:\n            logger.info(\"Resuming training from iteration %d\", self.iteration)\n\n            with open(checkpoint, \"rb\") as f:\n                self.model_params = pickle.load(f)\n                if self.n_devices > 1:\n                    self.model_params = self.replicate_params(self.model_params)\n        else:\n            logger.info(\"Starting training from scratch\")\n            self.model_params = None",
  "def split_inputs(self, inputs):\n        for k, arr in inputs.items():\n            assert arr.shape[0] % self.n_devices == 0, (\n                f\"Batch size should be evenly divisible by the number of \"\n                f\"devices. Input array shape is {arr.shape} but n_device is\"\n                f\" {self.n_devices}\"\n            )\n            inputs[k] = arr.reshape(\n                self.n_devices, arr.shape[0] // self.n_devices, *arr.shape[1:]\n            )\n            inputs[k] = [x for x in inputs[k]]  # make a sequence for put_sharded\n        return inputs",
  "def unstack_device_outputs(self, outputs):\n        for k, arr in outputs.items():\n            outputs[k] = arr.reshape(arr.shape[0] * arr.shape[1], *arr.shape[2:])\n        return outputs",
  "def train_step(self, batch, request):\n        inputs = self.__collect_provided_inputs(batch)\n        if self.n_devices > 1:\n            inputs = self.split_inputs(inputs)\n\n        # put to device for max performance\n        if self.n_devices > 1:\n            for k, v in inputs.items():\n                inputs[k] = jax.device_put_sharded(v, jax.local_devices())\n        else:\n            for k, v in inputs.items():\n                inputs[k] = jax.device_put(v)\n\n        # initialize model if necessary\n        if self.model_params is None:\n            # Use the random seed of first request to initialize model's weight\n            rng = jax.random.PRNGKey(request.random_seed)\n            if self.n_devices > 1:\n                rng = jnp.broadcast_to(rng, (self.n_devices,) + rng.shape)\n                self.model_params = jax.pmap(self.model.initialize)(rng, inputs)\n            else:\n                self.model_params = self.model.initialize(rng, inputs)\n\n        requested_outputs = self.__collect_requested_outputs(request)\n\n        if self.n_devices > 1:\n            self.model_params, outputs, loss = jax.pmap(\n                self.model.train_step,\n                axis_name=\"num_devices\",\n                donate_argnums=(0,),\n                static_broadcasted_argnums=(2,),\n            )(self.model_params, inputs, True)\n            loss = loss.mean()\n            outputs = self.unstack_device_outputs(outputs)  # stack by batch\n        else:\n            self.model_params, outputs, loss = jax.jit(\n                self.model.train_step, donate_argnums=(0,), static_argnums=(2,)\n            )(self.model_params, inputs, False)\n\n        logger.debug(\"model outputs: %s\", {k: v.shape for k, v in outputs.items()})\n\n        # add requested model outputs to batch\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(outputs[array_name], spec)\n\n        if isinstance(loss, dict):\n            total = 0.0\n            for k, v in loss.items():\n                total += v\n            batch.loss = total\n        else:\n            batch.loss = loss\n        self.iteration += 1\n        batch.iteration = self.iteration\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = self._checkpoint_name(\n                self.checkpoint_basename, batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            model_state = self.model_params\n            if self.n_devices > 1:\n                # get only a single copy of param for saving\n                model_state = jax.tree_map(lambda x: x[0], model_state)\n            with open(checkpoint_name, \"wb\") as f:\n                pickle.dump(model_state, f)\n\n            if self.keep_n_checkpoints:\n                checkpoint_name = self._checkpoint_name(\n                    self.checkpoint_basename,\n                    batch.iteration - self.keep_n_checkpoints * self.save_every,\n                )\n                try:\n                    os.remove(checkpoint_name)\n                    logger.info(\"Removed checkpoint %s\", checkpoint_name)\n                except FileNotFoundError:\n                    pass\n\n        if self.summary_writer and batch.iteration % self.log_every == 0:\n            if isinstance(loss, dict):\n                for k, v in loss.items():\n                    self.summary_writer.add_scalar(k, v, batch.iteration)\n            else:\n                self.summary_writer.add_scalar(\"loss\", loss, batch.iteration)\n\n        # run validate\n        if self.validate_fn is not None and batch.iteration % self.validate_every == 0:\n            val_ret = self.validate_fn(self.model, self.model_params)\n            if isinstance(val_ret, dict):\n                for k, v in val_ret.items():\n                    self.summary_writer.add_scalar(k, v, batch.iteration)\n            else:\n                self.summary_writer.add_scalar(\"validate\", val_ret, batch.iteration)",
  "def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        return array_outputs",
  "def __collect_provided_inputs(self, batch):\n        return self.__collect_provided_arrays(self.inputs, batch)",
  "def __collect_provided_arrays(self, reference, batch):\n        arrays = {}\n\n        for array_name, array_key in reference.items():\n            if isinstance(array_key, ArrayKey):\n                msg = f\"batch does not contain {array_key}, array {array_name} will not be set\"\n                if array_key in batch.arrays:\n                    arrays[array_name] = batch.arrays[array_key].data\n                else:\n                    logger.warn(msg)\n            elif isinstance(array_key, np.ndarray):\n                arrays[array_name] = array_key\n            else:\n                raise Exception(\n                    \"Unknown network array key {}, can't be given to \"\n                    \"network\".format(array_key)\n                )\n\n        return arrays",
  "class Predict(GenericPredict):\n    \"\"\"JAX implementation of :class:`gunpowder.nodes.Predict`.\n\n    Args:\n\n        model (subclass of ``gunpowder.jax.GenericJaxModel``):\n\n            The model to use for prediction.\n\n        inputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of input tensors in the network to\n            array keys.\n\n        outputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of output tensors in the network to array\n            keys. New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (``outputs``). This is\n            useful to set the ``voxel_size``, for example, if they differ from\n            the voxel size of the input arrays. Only fields that are not\n            ``None`` in the given :class:`ArraySpec` will be used.\n\n        checkpoint: (``string``, optional):\n\n            An optional path to the saved parameters for your jax module.\n            These will be loaded and used for prediction if provided.\n\n        spawn_subprocess (bool, optional): Whether to run ``predict`` in a\n            separate process. Default is false.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: GenericJaxModel,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[str, int], ArrayKey],\n        array_specs: Dict[ArrayKey, ArraySpec] = None,\n        checkpoint: str = None,\n        spawn_subprocess=False,\n    ):\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        super(Predict, self).__init__(\n            inputs, outputs, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.checkpoint = checkpoint\n        self.model_params = None\n\n    def start(self):\n        if self.checkpoint is not None:\n            with open(self.checkpoint, \"rb\") as f:\n                self.model_params = pickle.load(f)\n\n    def predict(self, batch, request):\n        inputs = self.get_inputs(batch)\n\n        if self.model_params is None:\n            # need to init model first\n            rng = jax.random.PRNGKey(request.random_seed)\n            self.model_params = self.model.initialize(rng, inputs)\n\n        out = jax.jit(self.model.forward)(self.model_params, inputs)\n        outputs = self.get_outputs(out, request)\n        self.update_batch(batch, request, outputs)\n\n    def get_inputs(self, batch):\n        model_inputs = {\n            key: jax.device_put(batch[value].data) for key, value in self.inputs.items()\n        }\n        return model_inputs\n\n    def get_outputs(self, module_out, request):\n        outputs = {}\n        for key, value in self.outputs.items():\n            if value in request:\n                outputs[value] = module_out[key]\n        return outputs\n\n    def update_batch(self, batch, request, requested_outputs):\n        for array_key, tensor in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor, spec)\n\n    def stop(self):\n        pass",
  "def __init__(\n        self,\n        model: GenericJaxModel,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[str, int], ArrayKey],\n        array_specs: Dict[ArrayKey, ArraySpec] = None,\n        checkpoint: str = None,\n        spawn_subprocess=False,\n    ):\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        super(Predict, self).__init__(\n            inputs, outputs, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.checkpoint = checkpoint\n        self.model_params = None",
  "def start(self):\n        if self.checkpoint is not None:\n            with open(self.checkpoint, \"rb\") as f:\n                self.model_params = pickle.load(f)",
  "def predict(self, batch, request):\n        inputs = self.get_inputs(batch)\n\n        if self.model_params is None:\n            # need to init model first\n            rng = jax.random.PRNGKey(request.random_seed)\n            self.model_params = self.model.initialize(rng, inputs)\n\n        out = jax.jit(self.model.forward)(self.model_params, inputs)\n        outputs = self.get_outputs(out, request)\n        self.update_batch(batch, request, outputs)",
  "def get_inputs(self, batch):\n        model_inputs = {\n            key: jax.device_put(batch[value].data) for key, value in self.inputs.items()\n        }\n        return model_inputs",
  "def get_outputs(self, module_out, request):\n        outputs = {}\n        for key, value in self.outputs.items():\n            if value in request:\n                outputs[value] = module_out[key]\n        return outputs",
  "def update_batch(self, batch, request, requested_outputs):\n        for array_key, tensor in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor, spec)",
  "def stop(self):\n        pass",
  "class Train(GenericTrain):\n    \"\"\"Torch implementation of :class:`gunpowder.nodes.GenericTrain`.\n\n    Args:\n\n        model (subclass of ``torch.nn.Module``):\n\n            The model to train.\n\n        loss:\n\n            The torch loss to use.\n\n        optimizer:\n\n            The torch optimizer to use.\n\n        inputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of input tensors (argument names of the\n            ``forward`` method) in the model to array keys.\n\n        loss_inputs (``dict``, ``string`` or ``int`` -> :class:`ArrayKey`):\n\n            Dictionary with the names of input variables to the loss function as\n            keys, and ArrayKeys containing the desired data as values. Keys can\n            be either strings or integers. If the key is an integer, it will\n            be treated as a positional argument to the loss function, a\n            string will be used as a named argument\n\n        outputs (``dict``, ``string`` or ``int`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of tensors in the network to array\n            keys. If the key is a string, the tensor will be retrieved\n            by checking the model for an attribute with they key as its name.\n            If the key is an integer, it is interpreted as a tuple index of\n            the outputs of the network.\n            New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (at the moment only\n            ``output``). This is useful to set the ``voxel_size``, for example,\n            if they differ from the voxel size of the input arrays. Only fields\n            that are not ``None`` in the given :class:`ArraySpec` will be used.\n\n        checkpoint_basename (``string``, optional):\n\n            The basename used for checkpoint files. Defaults to ``model``.\n\n        save_every (``int``, optional):\n\n            After how many iterations to create a checkpoint to store the\n            learnt weights.\n\n        log_dir (``string``, optional):\n\n            Directory for saving tensorboard summaries.\n\n        log_every (``int``, optional):\n\n            After how many iterations to write out tensorboard summaries.\n\n        spawn_subprocess (``bool``, optional):\n\n            Whether to run the ``train_step`` in a separate process. Default is false.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        loss,\n        optimizer,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[int, str], ArrayKey],\n        loss_inputs: Dict[Union[int, str], ArrayKey],\n        gradients: Dict[Union[int, str], ArrayKey] = {},\n        array_specs: Optional[Dict[ArrayKey, ArraySpec]] = None,\n        checkpoint_basename: str = \"model\",\n        save_every: int = 2000,\n        log_dir: str = None,\n        log_every: int = 1,\n        spawn_subprocess: bool = False,\n    ):\n        if not model.training:\n            logger.warning(\n                \"Model is in evaluation mode during training. \"\n                \"Consider using model.train()\"\n            )\n\n        # not yet implemented\n        gradients = gradients\n        inputs.update(\n            {k: v for k, v in loss_inputs.items() if v not in outputs.values()}\n        )\n\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.loss = loss\n        self.optimizer = optimizer\n        self.loss_inputs = loss_inputs\n        self.checkpoint_basename = checkpoint_basename\n        self.save_every = save_every\n\n        self.iteration = 0\n\n        if not isinstance(tensorboardX, NoSuchModule) and log_dir is not None:\n            self.summary_writer = tensorboardX.SummaryWriter(log_dir)\n            self.log_every = log_every\n        else:\n            self.summary_writer = None\n            if log_dir is not None:\n                logger.warning(\"log_dir given, but tensorboardX is not installed\")\n\n        self.intermediate_layers = {}\n        self.register_hooks()\n\n    def register_hooks(self):\n        for key in self.outputs:\n            if isinstance(key, str):\n                layer = getattr(self.model, key)\n                layer.register_forward_hook(self.create_hook(key))\n\n    def create_hook(self, key):\n        def save_layer(module, input, output):\n            self.intermediate_layers[key] = output\n\n        return save_layer\n\n    def retain_gradients(self, request, outputs):\n        for array_name, array_key in self.gradients.items():\n            if array_key not in request:\n                continue\n            if isinstance(array_name, int):\n                tensor = outputs[array_name]\n            elif isinstance(array_name, str):\n                tensor = getattr(self.model, array_name)\n            else:\n                raise RuntimeError(\n                    \"only ints and strings are supported as gradients keys\"\n                )\n            tensor.retain_grad()\n\n    def start(self):\n        self.use_cuda = torch.cuda.is_available()\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n        try:\n            self.model = self.model.to(self.device)\n        except RuntimeError as e:\n            raise RuntimeError(\n                \"Failed to move model to device. If you are using a child process \"\n                \"to run your model, maybe you already initialized CUDA by sending \"\n                \"your model to device in the main process.\"\n            ) from e\n        if isinstance(self.loss, torch.nn.Module):\n            self.loss = self.loss.to(self.device)\n\n        checkpoint, self.iteration = self._get_latest_checkpoint(\n            self.checkpoint_basename\n        )\n\n        if checkpoint is not None:\n            logger.info(\"Resuming training from iteration %d\", self.iteration)\n            logger.info(\"Loading %s\", checkpoint)\n\n            checkpoint = torch.load(checkpoint, map_location=self.device)\n            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n        else:\n            logger.info(\"Starting training from scratch\")\n\n        logger.info(\"Using device %s\", self.device)\n\n    def train_step(self, batch, request):\n        inputs = self.__collect_provided_inputs(batch)\n        requested_outputs = self.__collect_requested_outputs(request)\n\n        # keys are argument names of model forward pass\n        device_inputs = {\n            k: torch.as_tensor(v, device=self.device) for k, v in inputs.items()\n        }\n\n        # get outputs. Keys are tuple indices or model attr names as in self.outputs\n        self.optimizer.zero_grad()\n        model_outputs = self.model(**device_inputs)\n        if isinstance(model_outputs, tuple):\n            outputs = {i: model_outputs[i] for i in range(len(model_outputs))}\n        elif isinstance(model_outputs, torch.Tensor):\n            outputs = {0: model_outputs}\n        else:\n            raise RuntimeError(\n                \"Torch train node only supports return types of tuple\",\n                f\"and torch.Tensor from model.forward(). not {type(model_outputs)}\",\n            )\n        outputs.update(self.intermediate_layers)\n\n        # Some inputs to the loss should come from the batch, not the model\n        provided_loss_inputs = self.__collect_provided_loss_inputs(batch)\n\n        device_loss_inputs = {\n            k: torch.as_tensor(v, device=self.device)\n            for k, v in provided_loss_inputs.items()\n        }\n\n        # Some inputs to the loss function should come from the outputs of the model\n        # Update device loss inputs with tensors from outputs if available\n        flipped_outputs = {v: outputs[k] for k, v in self.outputs.items()}\n        device_loss_inputs = {\n            k: flipped_outputs.get(v, device_loss_inputs.get(k))\n            for k, v in self.loss_inputs.items()\n        }\n\n        device_loss_args = []\n        for i in range(len(device_loss_inputs)):\n            if i in device_loss_inputs:\n                device_loss_args.append(device_loss_inputs.pop(i))\n            else:\n                break\n        device_loss_kwargs = {}\n        for k, v in list(device_loss_inputs.items()):\n            if isinstance(k, str):\n                device_loss_kwargs[k] = device_loss_inputs.pop(k)\n        assert (\n            len(device_loss_inputs) == 0\n        ), f\"Not all loss inputs could be interpreted. Failed keys: {device_loss_inputs.keys()}\"\n\n        self.retain_gradients(request, outputs)\n\n        logger.debug(\"model outputs: %s\", {k: v.shape for k, v in outputs.items()})\n        logger.debug(\n            \"loss inputs: %s %s\",\n            [v.shape for v in device_loss_args],\n            {k: v.shape for k, v in device_loss_kwargs.items()},\n        )\n        loss = self.loss(*device_loss_args, **device_loss_kwargs)\n        loss.backward()\n        self.optimizer.step()\n\n        # add requested model outputs to batch\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(\n                outputs[array_name].cpu().detach().numpy(), spec\n            )\n\n        for array_name, array_key in self.gradients.items():\n            if array_key not in request:\n                continue\n            if isinstance(array_name, int):\n                tensor = outputs[array_name]\n            elif isinstance(array_name, str):\n                tensor = getattr(self.model, array_name)\n            else:\n                raise RuntimeError(\n                    \"only ints and strings are supported as gradients keys\"\n                )\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor.grad.cpu().detach().numpy(), spec)\n\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(\n                outputs[array_name].cpu().detach().numpy(), spec\n            )\n\n        batch.loss = loss.cpu().detach().numpy()\n        self.iteration += 1\n        batch.iteration = self.iteration\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = self._checkpoint_name(\n                self.checkpoint_basename, batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            torch.save(\n                {\n                    \"model_state_dict\": self.model.state_dict(),\n                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n                },\n                checkpoint_name,\n            )\n\n        if self.summary_writer and batch.iteration % self.log_every == 0:\n            self.summary_writer.add_scalar(\"loss\", batch.loss, batch.iteration)\n\n    def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        return array_outputs\n\n    def __collect_provided_inputs(self, batch):\n        return self.__collect_provided_arrays(\n            {k: v for k, v in self.inputs.items() if k not in self.loss_inputs}, batch\n        )\n\n    def __collect_provided_loss_inputs(self, batch):\n        return self.__collect_provided_arrays(\n            self.loss_inputs, batch, expect_missing_arrays=True\n        )\n\n    def __collect_provided_arrays(self, reference, batch, expect_missing_arrays=False):\n        arrays = {}\n\n        for array_name, array_key in reference.items():\n            if isinstance(array_key, ArrayKey):\n                msg = f\"batch does not contain {array_key}, array {array_name} will not be set\"\n                if array_key in batch.arrays:\n                    arrays[array_name] = batch.arrays[array_key].data\n                elif not expect_missing_arrays:\n                    logger.warn(msg)\n                else:\n                    logger.debug(msg)\n            elif isinstance(array_key, np.ndarray):\n                arrays[array_name] = array_key\n            elif isinstance(array_key, str):\n                arrays[array_name] = getattr(batch, array_key)\n            else:\n                raise Exception(\n                    \"Unknown network array key {}, can't be given to \"\n                    \"network\".format(array_key)\n                )\n\n        return arrays",
  "def __init__(\n        self,\n        model,\n        loss,\n        optimizer,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[int, str], ArrayKey],\n        loss_inputs: Dict[Union[int, str], ArrayKey],\n        gradients: Dict[Union[int, str], ArrayKey] = {},\n        array_specs: Optional[Dict[ArrayKey, ArraySpec]] = None,\n        checkpoint_basename: str = \"model\",\n        save_every: int = 2000,\n        log_dir: str = None,\n        log_every: int = 1,\n        spawn_subprocess: bool = False,\n    ):\n        if not model.training:\n            logger.warning(\n                \"Model is in evaluation mode during training. \"\n                \"Consider using model.train()\"\n            )\n\n        # not yet implemented\n        gradients = gradients\n        inputs.update(\n            {k: v for k, v in loss_inputs.items() if v not in outputs.values()}\n        )\n\n        super(Train, self).__init__(\n            inputs, outputs, gradients, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.model = model\n        self.loss = loss\n        self.optimizer = optimizer\n        self.loss_inputs = loss_inputs\n        self.checkpoint_basename = checkpoint_basename\n        self.save_every = save_every\n\n        self.iteration = 0\n\n        if not isinstance(tensorboardX, NoSuchModule) and log_dir is not None:\n            self.summary_writer = tensorboardX.SummaryWriter(log_dir)\n            self.log_every = log_every\n        else:\n            self.summary_writer = None\n            if log_dir is not None:\n                logger.warning(\"log_dir given, but tensorboardX is not installed\")\n\n        self.intermediate_layers = {}\n        self.register_hooks()",
  "def register_hooks(self):\n        for key in self.outputs:\n            if isinstance(key, str):\n                layer = getattr(self.model, key)\n                layer.register_forward_hook(self.create_hook(key))",
  "def create_hook(self, key):\n        def save_layer(module, input, output):\n            self.intermediate_layers[key] = output\n\n        return save_layer",
  "def retain_gradients(self, request, outputs):\n        for array_name, array_key in self.gradients.items():\n            if array_key not in request:\n                continue\n            if isinstance(array_name, int):\n                tensor = outputs[array_name]\n            elif isinstance(array_name, str):\n                tensor = getattr(self.model, array_name)\n            else:\n                raise RuntimeError(\n                    \"only ints and strings are supported as gradients keys\"\n                )\n            tensor.retain_grad()",
  "def start(self):\n        self.use_cuda = torch.cuda.is_available()\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n        try:\n            self.model = self.model.to(self.device)\n        except RuntimeError as e:\n            raise RuntimeError(\n                \"Failed to move model to device. If you are using a child process \"\n                \"to run your model, maybe you already initialized CUDA by sending \"\n                \"your model to device in the main process.\"\n            ) from e\n        if isinstance(self.loss, torch.nn.Module):\n            self.loss = self.loss.to(self.device)\n\n        checkpoint, self.iteration = self._get_latest_checkpoint(\n            self.checkpoint_basename\n        )\n\n        if checkpoint is not None:\n            logger.info(\"Resuming training from iteration %d\", self.iteration)\n            logger.info(\"Loading %s\", checkpoint)\n\n            checkpoint = torch.load(checkpoint, map_location=self.device)\n            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n        else:\n            logger.info(\"Starting training from scratch\")\n\n        logger.info(\"Using device %s\", self.device)",
  "def train_step(self, batch, request):\n        inputs = self.__collect_provided_inputs(batch)\n        requested_outputs = self.__collect_requested_outputs(request)\n\n        # keys are argument names of model forward pass\n        device_inputs = {\n            k: torch.as_tensor(v, device=self.device) for k, v in inputs.items()\n        }\n\n        # get outputs. Keys are tuple indices or model attr names as in self.outputs\n        self.optimizer.zero_grad()\n        model_outputs = self.model(**device_inputs)\n        if isinstance(model_outputs, tuple):\n            outputs = {i: model_outputs[i] for i in range(len(model_outputs))}\n        elif isinstance(model_outputs, torch.Tensor):\n            outputs = {0: model_outputs}\n        else:\n            raise RuntimeError(\n                \"Torch train node only supports return types of tuple\",\n                f\"and torch.Tensor from model.forward(). not {type(model_outputs)}\",\n            )\n        outputs.update(self.intermediate_layers)\n\n        # Some inputs to the loss should come from the batch, not the model\n        provided_loss_inputs = self.__collect_provided_loss_inputs(batch)\n\n        device_loss_inputs = {\n            k: torch.as_tensor(v, device=self.device)\n            for k, v in provided_loss_inputs.items()\n        }\n\n        # Some inputs to the loss function should come from the outputs of the model\n        # Update device loss inputs with tensors from outputs if available\n        flipped_outputs = {v: outputs[k] for k, v in self.outputs.items()}\n        device_loss_inputs = {\n            k: flipped_outputs.get(v, device_loss_inputs.get(k))\n            for k, v in self.loss_inputs.items()\n        }\n\n        device_loss_args = []\n        for i in range(len(device_loss_inputs)):\n            if i in device_loss_inputs:\n                device_loss_args.append(device_loss_inputs.pop(i))\n            else:\n                break\n        device_loss_kwargs = {}\n        for k, v in list(device_loss_inputs.items()):\n            if isinstance(k, str):\n                device_loss_kwargs[k] = device_loss_inputs.pop(k)\n        assert (\n            len(device_loss_inputs) == 0\n        ), f\"Not all loss inputs could be interpreted. Failed keys: {device_loss_inputs.keys()}\"\n\n        self.retain_gradients(request, outputs)\n\n        logger.debug(\"model outputs: %s\", {k: v.shape for k, v in outputs.items()})\n        logger.debug(\n            \"loss inputs: %s %s\",\n            [v.shape for v in device_loss_args],\n            {k: v.shape for k, v in device_loss_kwargs.items()},\n        )\n        loss = self.loss(*device_loss_args, **device_loss_kwargs)\n        loss.backward()\n        self.optimizer.step()\n\n        # add requested model outputs to batch\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(\n                outputs[array_name].cpu().detach().numpy(), spec\n            )\n\n        for array_name, array_key in self.gradients.items():\n            if array_key not in request:\n                continue\n            if isinstance(array_name, int):\n                tensor = outputs[array_name]\n            elif isinstance(array_name, str):\n                tensor = getattr(self.model, array_name)\n            else:\n                raise RuntimeError(\n                    \"only ints and strings are supported as gradients keys\"\n                )\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor.grad.cpu().detach().numpy(), spec)\n\n        for array_key, array_name in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(\n                outputs[array_name].cpu().detach().numpy(), spec\n            )\n\n        batch.loss = loss.cpu().detach().numpy()\n        self.iteration += 1\n        batch.iteration = self.iteration\n\n        if batch.iteration % self.save_every == 0:\n            checkpoint_name = self._checkpoint_name(\n                self.checkpoint_basename, batch.iteration\n            )\n\n            logger.info(\"Creating checkpoint %s\", checkpoint_name)\n\n            torch.save(\n                {\n                    \"model_state_dict\": self.model.state_dict(),\n                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n                },\n                checkpoint_name,\n            )\n\n        if self.summary_writer and batch.iteration % self.log_every == 0:\n            self.summary_writer.add_scalar(\"loss\", batch.loss, batch.iteration)",
  "def __collect_requested_outputs(self, request):\n        array_outputs = {}\n\n        for output_name, array_key in self.outputs.items():\n            if array_key in request:\n                array_outputs[array_key] = output_name\n\n        return array_outputs",
  "def __collect_provided_inputs(self, batch):\n        return self.__collect_provided_arrays(\n            {k: v for k, v in self.inputs.items() if k not in self.loss_inputs}, batch\n        )",
  "def __collect_provided_loss_inputs(self, batch):\n        return self.__collect_provided_arrays(\n            self.loss_inputs, batch, expect_missing_arrays=True\n        )",
  "def __collect_provided_arrays(self, reference, batch, expect_missing_arrays=False):\n        arrays = {}\n\n        for array_name, array_key in reference.items():\n            if isinstance(array_key, ArrayKey):\n                msg = f\"batch does not contain {array_key}, array {array_name} will not be set\"\n                if array_key in batch.arrays:\n                    arrays[array_name] = batch.arrays[array_key].data\n                elif not expect_missing_arrays:\n                    logger.warn(msg)\n                else:\n                    logger.debug(msg)\n            elif isinstance(array_key, np.ndarray):\n                arrays[array_name] = array_key\n            elif isinstance(array_key, str):\n                arrays[array_name] = getattr(batch, array_key)\n            else:\n                raise Exception(\n                    \"Unknown network array key {}, can't be given to \"\n                    \"network\".format(array_key)\n                )\n\n        return arrays",
  "def save_layer(module, input, output):\n            self.intermediate_layers[key] = output",
  "class Predict(GenericPredict):\n    \"\"\"Torch implementation of :class:`gunpowder.nodes.Predict`.\n\n    Args:\n\n        model (subclass of ``torch.nn.Module``):\n\n            The model to use for prediction.\n\n        inputs (``dict``, ``string`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of input tensors (argument names of the\n            ``forward`` method) in the model to array keys.\n\n        outputs (``dict``, ``string`` or ``int`` -> :class:`ArrayKey`):\n\n            Dictionary from the names of tensors in the network to array\n            keys. If the key is a string, the tensor will be retrieved\n            by checking the model for an attribute with the key as its name.\n            If the key is an integer, it is interpreted as a tuple index of\n            the outputs of the network.\n            New arrays will be generated by this node for each entry (if\n            requested downstream).\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            Used to set the specs of generated arrays (``outputs``). This is\n            useful to set the ``voxel_size``, for example, if they differ from\n            the voxel size of the input arrays. Only fields that are not\n            ``None`` in the given :class:`ArraySpec` will be used.\n\n        checkpoint: (``string``, optional):\n\n            An optional path to the saved parameters for your torch module.\n            These will be loaded and used for prediction if provided.\n\n        device (``string``, optional):\n\n            Which device to use for prediction (``\"cpu\"`` or ``\"cuda\"``).\n            Default is ``\"cuda\"``, which falls back to CPU if CUDA is not\n            available.\n\n        spawn_subprocess (bool, optional): Whether to run ``predict`` in a\n            separate process. Default is false.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[str, int], ArrayKey],\n        array_specs: Dict[ArrayKey, ArraySpec] = None,\n        checkpoint: str = None,\n        device=\"cuda\",\n        spawn_subprocess=False,\n    ):\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        if model.training:\n            logger.warning(\n                \"Model is in training mode during prediction. \"\n                \"Consider using model.eval()\"\n            )\n\n        super(Predict, self).__init__(\n            inputs, outputs, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.device_string = device\n        self.device = None  # to be set in start()\n        self.model = model\n        self.checkpoint = checkpoint\n\n        self.intermediate_layers = {}\n        self.register_hooks()\n\n    def start(self):\n        self.use_cuda = torch.cuda.is_available() and self.device_string == \"cuda\"\n        logger.info(f\"Predicting on {'gpu' if self.use_cuda else 'cpu'}\")\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n        try:\n            self.model = self.model.to(self.device)\n        except RuntimeError as e:\n            raise RuntimeError(\n                \"Failed to move model to device. If you are using a child process \"\n                \"to run your model, maybe you already initialized CUDA by sending \"\n                \"your model to device in the main process.\"\n            ) from e\n\n        if self.checkpoint is not None:\n            checkpoint = torch.load(self.checkpoint, map_location=self.device)\n            if \"model_state_dict\" in checkpoint:\n                self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n            else:\n                self.model.load_state_dict(checkpoint)\n\n    def predict(self, batch, request):\n        inputs = self.get_inputs(batch)\n        with torch.no_grad():\n            out = self.model.forward(**inputs)\n        outputs = self.get_outputs(out, request)\n        self.update_batch(batch, request, outputs)\n\n    def get_inputs(self, batch):\n        model_inputs = {\n            key: torch.as_tensor(batch[value].data, device=self.device)\n            for key, value in self.inputs.items()\n        }\n        return model_inputs\n\n    def register_hooks(self):\n        for key in self.outputs:\n            if isinstance(key, str):\n                layer = getattr(self.model, key)\n                layer.register_forward_hook(self.create_hook(key))\n\n    def create_hook(self, key):\n        def save_layer(module, input, output):\n            self.intermediate_layers[key] = output\n\n        return save_layer\n\n    def get_outputs(self, module_out, request):\n        outputs = {}\n        if isinstance(module_out, tuple):\n            module_outs = module_out\n        else:\n            module_outs = (module_out,)\n        for key, value in self.outputs.items():\n            if value in request:\n                if isinstance(key, str):\n                    outputs[value] = self.intermediate_layers[key]\n                elif isinstance(key, int):\n                    outputs[value] = module_outs[key]\n        return outputs\n\n    def update_batch(self, batch, request, requested_outputs):\n        for array_key, tensor in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor.cpu().detach().numpy(), spec)\n\n    def stop(self):\n        pass",
  "def __init__(\n        self,\n        model,\n        inputs: Dict[str, ArrayKey],\n        outputs: Dict[Union[str, int], ArrayKey],\n        array_specs: Dict[ArrayKey, ArraySpec] = None,\n        checkpoint: str = None,\n        device=\"cuda\",\n        spawn_subprocess=False,\n    ):\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        if model.training:\n            logger.warning(\n                \"Model is in training mode during prediction. \"\n                \"Consider using model.eval()\"\n            )\n\n        super(Predict, self).__init__(\n            inputs, outputs, array_specs, spawn_subprocess=spawn_subprocess\n        )\n\n        self.device_string = device\n        self.device = None  # to be set in start()\n        self.model = model\n        self.checkpoint = checkpoint\n\n        self.intermediate_layers = {}\n        self.register_hooks()",
  "def start(self):\n        self.use_cuda = torch.cuda.is_available() and self.device_string == \"cuda\"\n        logger.info(f\"Predicting on {'gpu' if self.use_cuda else 'cpu'}\")\n        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n\n        try:\n            self.model = self.model.to(self.device)\n        except RuntimeError as e:\n            raise RuntimeError(\n                \"Failed to move model to device. If you are using a child process \"\n                \"to run your model, maybe you already initialized CUDA by sending \"\n                \"your model to device in the main process.\"\n            ) from e\n\n        if self.checkpoint is not None:\n            checkpoint = torch.load(self.checkpoint, map_location=self.device)\n            if \"model_state_dict\" in checkpoint:\n                self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n            else:\n                self.model.load_state_dict(checkpoint)",
  "def predict(self, batch, request):\n        inputs = self.get_inputs(batch)\n        with torch.no_grad():\n            out = self.model.forward(**inputs)\n        outputs = self.get_outputs(out, request)\n        self.update_batch(batch, request, outputs)",
  "def get_inputs(self, batch):\n        model_inputs = {\n            key: torch.as_tensor(batch[value].data, device=self.device)\n            for key, value in self.inputs.items()\n        }\n        return model_inputs",
  "def register_hooks(self):\n        for key in self.outputs:\n            if isinstance(key, str):\n                layer = getattr(self.model, key)\n                layer.register_forward_hook(self.create_hook(key))",
  "def create_hook(self, key):\n        def save_layer(module, input, output):\n            self.intermediate_layers[key] = output\n\n        return save_layer",
  "def get_outputs(self, module_out, request):\n        outputs = {}\n        if isinstance(module_out, tuple):\n            module_outs = module_out\n        else:\n            module_outs = (module_out,)\n        for key, value in self.outputs.items():\n            if value in request:\n                if isinstance(key, str):\n                    outputs[value] = self.intermediate_layers[key]\n                elif isinstance(key, int):\n                    outputs[value] = module_outs[key]\n        return outputs",
  "def update_batch(self, batch, request, requested_outputs):\n        for array_key, tensor in requested_outputs.items():\n            spec = self.spec[array_key].copy()\n            spec.roi = request[array_key].roi\n            batch.arrays[array_key] = Array(tensor.cpu().detach().numpy(), spec)",
  "def stop(self):\n        pass",
  "def save_layer(module, input, output):\n            self.intermediate_layers[key] = output",
  "class AddBoundaryDistanceGradients(BatchFilter):\n    \"\"\"Add an array with vectors pointing away from the closest boundary.\n\n    The vectors are the spacial gradients of the distance transform, i.e., the\n    distance to the boundary between labels or the background label (0).\n\n    Args:\n\n        label_array_key(:class:``ArrayKey``): The array to read the labels\n            from.\n\n        gradient_array_key(:class:``ArrayKey``): The array to generate\n            containing the gradients.\n\n        distance_array_key(:class:``ArrayKey``, optional): The array to\n            generate containing the values of the distance transform.\n\n        boundary_array_key(:class:``ArrayKey``, optional): The array to\n            generate containing a boundary labeling. Note this array will be\n            doubled as it encodes boundaries between voxels.\n\n        normalize(string, optional): ``None``, ``'l1'``, or ``'l2'``. Specifies\n            if and how to normalize the gradients.\n\n        scale(string, optional): ``None`` or ``exp``. If ``exp``, distance\n            gradients will be scaled by ``beta*e**(-d*alpha)``, where ``d`` is\n            the distance to the boundary.\n\n        scale_args(tuple, optional): For ``exp`` a tuple with the values of\n            ``alpha`` and ``beta``.\n    \"\"\"\n\n    def __init__(\n        self,\n        label_array_key,\n        gradient_array_key,\n        distance_array_key=None,\n        boundary_array_key=None,\n        normalize=None,\n        scale=None,\n        scale_args=None,\n    ):\n        self.label_array_key = label_array_key\n        self.gradient_array_key = gradient_array_key\n        self.distance_array_key = distance_array_key\n        self.boundary_array_key = boundary_array_key\n        self.normalize = normalize\n        self.scale = scale\n        self.scale_args = scale_args\n\n    def setup(self):\n        assert self.label_array_key in self.spec, (\n            \"Upstream does not provide %s needed by \"\n            \"AddBoundaryDistanceGradients\" % self.label_array_key\n        )\n\n        spec = self.spec[self.label_array_key].copy()\n        spec.dtype = np.float32\n        self.provides(self.gradient_array_key, spec)\n        if self.distance_array_key is not None:\n            self.provides(self.distance_array_key, spec)\n        if self.boundary_array_key is not None:\n            spec.voxel_size /= 2\n            self.provides(self.boundary_array_key, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.label_array_key] = request[self.gradient_array_key]\n\n        return deps\n\n    def process(self, batch, request):\n        if not self.gradient_array_key in request:\n            return\n\n        labels = batch.arrays[self.label_array_key].data\n        voxel_size = self.spec[self.label_array_key].voxel_size\n\n        # get boundaries between label regions\n        boundaries = self.__find_boundaries(labels)\n\n        # mark boundaries with 0 (not 1)\n        boundaries = 1.0 - boundaries\n\n        if np.sum(boundaries == 0) == 0:\n            # no boundary -- no distance to compute\n            distances = np.zeros(labels.shape, dtype=np.float32)\n\n        else:\n            # get distances (voxel_size/2 because image is doubled)\n            distances = distance_transform_edt(\n                boundaries, sampling=tuple(float(v) / 2 for v in voxel_size)\n            )\n            distances = distances.astype(np.float32)\n\n            # restore original shape\n            downsample = (slice(None, None, 2),) * len(voxel_size)\n            distances = distances[downsample]\n\n            # set distances in background to 0\n            distances[labels == 0] = 0\n\n        gradients = np.asarray(np.gradient(distances, *voxel_size))\n\n        # set gradients on background voxels to 0\n        for d in range(len(voxel_size)):\n            gradients[d, labels == 0] = 0\n\n        if self.normalize is not None:\n            self.__normalize(gradients, self.normalize)\n\n        if self.scale is not None:\n            self.__scale(gradients, distances, self.scale, self.scale_args)\n\n        spec = self.spec[self.gradient_array_key].copy()\n        spec.roi = request[self.gradient_array_key].roi\n        batch.arrays[self.gradient_array_key] = Array(gradients, spec)\n\n        if self.distance_array_key is not None and self.distance_array_key in request:\n            batch.arrays[self.distance_array_key] = Array(distances, spec)\n\n        if self.boundary_array_key is not None and self.boundary_array_key in request:\n            # add one more face at each dimension, as boundary map has shape\n            # 2*s - 1 of original shape s\n            grown = np.ones(tuple(s + 1 for s in boundaries.shape))\n            grown[tuple(slice(0, s) for s in boundaries.shape)] = boundaries\n            spec.voxel_size = voxel_size / 2\n            logger.debug(\"voxel size of boundary array: %s\", spec.voxel_size)\n            batch.arrays[self.boundary_array_key] = Array(grown, spec)\n\n    def __find_boundaries(self, labels):\n        # labels: 1 1 1 1 0 0 2 2 2 2 3 3       n\n        # shift :   1 1 1 1 0 0 2 2 2 2 3       n - 1\n        # diff  :   0 0 0 1 0 1 0 0 0 1 0       n - 1\n        # bound.: 00000001000100000001000      2n - 1\n\n        logger.debug(\"computing boundaries for %s\", labels.shape)\n\n        dims = len(labels.shape)\n        in_shape = labels.shape\n        out_shape = tuple(2 * s - 1 for s in in_shape)\n        out_slices = tuple(slice(0, s) for s in out_shape)\n\n        boundaries = np.zeros(out_shape, dtype=bool)\n\n        logger.debug(\"boundaries shape is %s\", boundaries.shape)\n\n        for d in range(dims):\n            logger.debug(\"processing dimension %d\", d)\n\n            shift_p = [slice(None)] * dims\n            shift_p[d] = slice(1, in_shape[d])\n\n            shift_n = [slice(None)] * dims\n            shift_n[d] = slice(0, in_shape[d] - 1)\n\n            diff = (labels[tuple(shift_p)] - labels[tuple(shift_n)]) != 0\n\n            logger.debug(\"diff shape is %s\", diff.shape)\n\n            target = [slice(None, None, 2)] * dims\n            target[d] = slice(1, out_shape[d], 2)\n\n            logger.debug(\"target slices are %s\", target)\n\n            boundaries[tuple(target)] = diff\n\n        return boundaries\n\n    def __normalize(self, gradients, norm):\n        dims = gradients.shape[0]\n\n        if norm == \"l1\":\n            factors = sum([np.abs(gradients[d]) for d in range(dims)])\n        elif norm == \"l2\":\n            factors = np.sqrt(sum([np.square(gradients[d]) for d in range(dims)]))\n        else:\n            raise RuntimeError(\"norm %s not supported\" % norm)\n\n        factors[factors < 1e-5] = 1\n        gradients /= factors\n\n    def __scale(self, gradients, distances, scale, scale_args):\n        dims = gradients.shape[0]\n\n        if scale == \"exp\":\n            alpha, beta = self.scale_args\n            factors = np.exp(-distances * alpha) * beta\n\n        gradients *= factors",
  "def __init__(\n        self,\n        label_array_key,\n        gradient_array_key,\n        distance_array_key=None,\n        boundary_array_key=None,\n        normalize=None,\n        scale=None,\n        scale_args=None,\n    ):\n        self.label_array_key = label_array_key\n        self.gradient_array_key = gradient_array_key\n        self.distance_array_key = distance_array_key\n        self.boundary_array_key = boundary_array_key\n        self.normalize = normalize\n        self.scale = scale\n        self.scale_args = scale_args",
  "def setup(self):\n        assert self.label_array_key in self.spec, (\n            \"Upstream does not provide %s needed by \"\n            \"AddBoundaryDistanceGradients\" % self.label_array_key\n        )\n\n        spec = self.spec[self.label_array_key].copy()\n        spec.dtype = np.float32\n        self.provides(self.gradient_array_key, spec)\n        if self.distance_array_key is not None:\n            self.provides(self.distance_array_key, spec)\n        if self.boundary_array_key is not None:\n            spec.voxel_size /= 2\n            self.provides(self.boundary_array_key, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.label_array_key] = request[self.gradient_array_key]\n\n        return deps",
  "def process(self, batch, request):\n        if not self.gradient_array_key in request:\n            return\n\n        labels = batch.arrays[self.label_array_key].data\n        voxel_size = self.spec[self.label_array_key].voxel_size\n\n        # get boundaries between label regions\n        boundaries = self.__find_boundaries(labels)\n\n        # mark boundaries with 0 (not 1)\n        boundaries = 1.0 - boundaries\n\n        if np.sum(boundaries == 0) == 0:\n            # no boundary -- no distance to compute\n            distances = np.zeros(labels.shape, dtype=np.float32)\n\n        else:\n            # get distances (voxel_size/2 because image is doubled)\n            distances = distance_transform_edt(\n                boundaries, sampling=tuple(float(v) / 2 for v in voxel_size)\n            )\n            distances = distances.astype(np.float32)\n\n            # restore original shape\n            downsample = (slice(None, None, 2),) * len(voxel_size)\n            distances = distances[downsample]\n\n            # set distances in background to 0\n            distances[labels == 0] = 0\n\n        gradients = np.asarray(np.gradient(distances, *voxel_size))\n\n        # set gradients on background voxels to 0\n        for d in range(len(voxel_size)):\n            gradients[d, labels == 0] = 0\n\n        if self.normalize is not None:\n            self.__normalize(gradients, self.normalize)\n\n        if self.scale is not None:\n            self.__scale(gradients, distances, self.scale, self.scale_args)\n\n        spec = self.spec[self.gradient_array_key].copy()\n        spec.roi = request[self.gradient_array_key].roi\n        batch.arrays[self.gradient_array_key] = Array(gradients, spec)\n\n        if self.distance_array_key is not None and self.distance_array_key in request:\n            batch.arrays[self.distance_array_key] = Array(distances, spec)\n\n        if self.boundary_array_key is not None and self.boundary_array_key in request:\n            # add one more face at each dimension, as boundary map has shape\n            # 2*s - 1 of original shape s\n            grown = np.ones(tuple(s + 1 for s in boundaries.shape))\n            grown[tuple(slice(0, s) for s in boundaries.shape)] = boundaries\n            spec.voxel_size = voxel_size / 2\n            logger.debug(\"voxel size of boundary array: %s\", spec.voxel_size)\n            batch.arrays[self.boundary_array_key] = Array(grown, spec)",
  "def __find_boundaries(self, labels):\n        # labels: 1 1 1 1 0 0 2 2 2 2 3 3       n\n        # shift :   1 1 1 1 0 0 2 2 2 2 3       n - 1\n        # diff  :   0 0 0 1 0 1 0 0 0 1 0       n - 1\n        # bound.: 00000001000100000001000      2n - 1\n\n        logger.debug(\"computing boundaries for %s\", labels.shape)\n\n        dims = len(labels.shape)\n        in_shape = labels.shape\n        out_shape = tuple(2 * s - 1 for s in in_shape)\n        out_slices = tuple(slice(0, s) for s in out_shape)\n\n        boundaries = np.zeros(out_shape, dtype=bool)\n\n        logger.debug(\"boundaries shape is %s\", boundaries.shape)\n\n        for d in range(dims):\n            logger.debug(\"processing dimension %d\", d)\n\n            shift_p = [slice(None)] * dims\n            shift_p[d] = slice(1, in_shape[d])\n\n            shift_n = [slice(None)] * dims\n            shift_n[d] = slice(0, in_shape[d] - 1)\n\n            diff = (labels[tuple(shift_p)] - labels[tuple(shift_n)]) != 0\n\n            logger.debug(\"diff shape is %s\", diff.shape)\n\n            target = [slice(None, None, 2)] * dims\n            target[d] = slice(1, out_shape[d], 2)\n\n            logger.debug(\"target slices are %s\", target)\n\n            boundaries[tuple(target)] = diff\n\n        return boundaries",
  "def __normalize(self, gradients, norm):\n        dims = gradients.shape[0]\n\n        if norm == \"l1\":\n            factors = sum([np.abs(gradients[d]) for d in range(dims)])\n        elif norm == \"l2\":\n            factors = np.sqrt(sum([np.square(gradients[d]) for d in range(dims)]))\n        else:\n            raise RuntimeError(\"norm %s not supported\" % norm)\n\n        factors[factors < 1e-5] = 1\n        gradients /= factors",
  "def __scale(self, gradients, distances, scale, scale_args):\n        dims = gradients.shape[0]\n\n        if scale == \"exp\":\n            alpha, beta = self.scale_args\n            factors = np.exp(-distances * alpha) * beta\n\n        gradients *= factors",
  "class PrepareMalis(BatchFilter):\n    \"\"\"Creates a component label array needed for two-phase malis training.\n\n    Args:\n\n        labels_array_key(:class:`ArrayKey`): The label array to use.\n\n        malis_comp_array_key(:class:`ArrayKey`): The malis component array\n            to generate.\n\n        ignore_array_key(:class:`ArrayKey`, optional): An ignore mask to\n            use.\n    \"\"\"\n\n    def __init__(self, labels_array_key, malis_comp_array_key, ignore_array_key=None):\n        self.labels_array_key = labels_array_key\n        self.malis_comp_array_key = malis_comp_array_key\n        self.ignore_array_key = ignore_array_key\n\n    def setup(self):\n        spec = self.spec[self.labels_array_key].copy()\n        self.provides(self.malis_comp_array_key, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.labels_array_key] = copy.deepcopy(request[self.labels_array_key])\n        if self.ignore_array_key is not None:\n            deps[self.ignore_array_key] = copy.deepcopy(request[self.labels_array_key])\n        return deps\n\n    def process(self, batch, request):\n        gt_labels = batch.arrays[self.labels_array_key]\n        next_id = gt_labels.data.max() + 1\n\n        gt_pos_pass = gt_labels.data\n\n        if self.ignore_array_key is not None:\n            gt_neg_pass = np.array(gt_labels.data)\n            gt_neg_pass[batch.arrays[self.ignore_array_key].data == 0] = next_id\n\n        else:\n            gt_neg_pass = gt_pos_pass\n\n        spec = self.spec[self.malis_comp_array_key].copy()\n        spec.roi = request[self.labels_array_key].roi\n        batch.arrays[self.malis_comp_array_key] = Array(\n            np.array([gt_neg_pass, gt_pos_pass]), spec\n        )",
  "def __init__(self, labels_array_key, malis_comp_array_key, ignore_array_key=None):\n        self.labels_array_key = labels_array_key\n        self.malis_comp_array_key = malis_comp_array_key\n        self.ignore_array_key = ignore_array_key",
  "def setup(self):\n        spec = self.spec[self.labels_array_key].copy()\n        self.provides(self.malis_comp_array_key, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.labels_array_key] = copy.deepcopy(request[self.labels_array_key])\n        if self.ignore_array_key is not None:\n            deps[self.ignore_array_key] = copy.deepcopy(request[self.labels_array_key])\n        return deps",
  "def process(self, batch, request):\n        gt_labels = batch.arrays[self.labels_array_key]\n        next_id = gt_labels.data.max() + 1\n\n        gt_pos_pass = gt_labels.data\n\n        if self.ignore_array_key is not None:\n            gt_neg_pass = np.array(gt_labels.data)\n            gt_neg_pass[batch.arrays[self.ignore_array_key].data == 0] = next_id\n\n        else:\n            gt_neg_pass = gt_pos_pass\n\n        spec = self.spec[self.malis_comp_array_key].copy()\n        spec.roi = request[self.labels_array_key].roi\n        batch.arrays[self.malis_comp_array_key] = Array(\n            np.array([gt_neg_pass, gt_pos_pass]), spec\n        )",
  "class AddNonsymmetricAffinities(BatchFilter):\n    def __init__(\n        self,\n        affinity_vectors,\n        array_key_1,\n        array_key_2,\n        affinity_array_key_1,\n        affinity_array_key_2,\n    ):\n        self.array_key_1 = array_key_1\n        self.array_key_2 = array_key_2\n        self.affinity_array_key_1 = affinity_array_key_1\n        self.affinity_array_key_2 = affinity_array_key_2\n        self.affinity_vectors = affinity_vectors\n\n    def setup(self):\n        assert self.array_key_1 in self.spec, (\n            \"Upstream does not provide %s needed by \\\n        AddNonsymmetricAffinities\"\n            % self.array_key_1\n        )\n        assert self.array_key_2 in self.spec, (\n            \"Upstream does not provide %s needed by \\\n        AddNonsymmetricAffinities\"\n            % self.array_key_2\n        )\n\n        voxel_size = self.spec[self.array_key_1].voxel_size\n\n        self.upstream_spec = self.get_upstream_provider().spec\n        self.upstream_roi = self.upstream_spec.get_total_roi()\n\n        # get maximum offset in each dimension\n        self.padding = np.max(np.abs(self.affinity_vectors), axis=0)\n        self.padding = tuple(round_to_voxel_size(self.padding, voxel_size))\n\n        logger.debug(\"padding neg: %s\" % np.asarray(self.padding))\n\n        spec = self.spec[self.array_key_1].copy()\n        # if spec.roi is not None:\n\n        self.provides(self.affinity_array_key_1, spec)\n        self.provides(self.affinity_array_key_2, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        array_1_roi = request[self.array_key_1].roi\n        logger.debug(\"downstream %s request: \" % self.array_key_1 + str(array_1_roi))\n\n        array_2_roi = request[self.array_key_2].roi\n        logger.debug(\"downstream %s request: \" % self.array_key_2 + str(array_2_roi))\n\n        # grow labels ROI to accomodate padding TODO: vol 2\n        array_1_roi = array_1_roi.grow(self.padding, self.padding)\n        array_2_roi = array_2_roi.grow(self.padding, self.padding)\n\n        request[self.array_key_1].roi = array_1_roi\n        request[self.array_key_2].roi = array_2_roi\n\n        logger.debug(\"upstream %s request: \" % self.array_key_1 + str(array_1_roi))\n        logger.debug(\"upstream %s request: \" % self.array_key_2 + str(array_2_roi))\n\n        # pdb.set_trace()\n\n    def process(self, batch, request):\n        full_vol1 = batch.arrays[self.array_key_1]\n        full_vol2 = batch.arrays[self.array_key_2]\n\n        # Both full_vol1 should match\n        assert (\n            full_vol1.spec.dtype == full_vol2.spec.dtype\n        ), \"data type of array 1(%s) and array 2(%s) should match\" % (\n            full_vol1.spec.dtype,\n            full_vol2.spec.dtype,\n        )\n\n        assert (\n            full_vol1.spec.voxel_size == full_vol2.spec.voxel_size\n        ), \"data type of array 1(%s) and array 2(%s) should match\" % (\n            full_vol1.spec.voxel_size,\n            full_vol2.spec.voxel_size,\n        )\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        # Calculate affinities 1: from vol2 onto vol1\n\n        # Initialize affinity map\n        request_vol = request[self.affinity_array_key_1]\n        affinity_map = np.zeros(\n            (len(self.affinity_vectors),)\n            + tuple(request_vol.roi.shape / request_vol.voxel_size),\n            dtype=full_vol1.spec.dtype,\n        )\n\n        # calculate affinities\n        vol1 = full_vol1.crop(request_vol.roi)\n        for i, vector in enumerate(self.affinity_vectors):\n            vol2 = full_vol2.crop(request_vol.roi.shift(tuple(-vector)))\n            affinity_map[i, :, :, :] = np.bitwise_and(vol1.data, vol2.data)\n\n        batch.arrays[self.affinity_array_key_1] = Array(\n            affinity_map, spec=request[self.affinity_array_key_1].copy()\n        )\n\n        batch.arrays[self.affinity_array_key_1].attrs[\n            \"affinity_vectors\"\n        ] = self.affinity_vectors\n\n        # Calculate affinities 2: from vol1 onto vol2\n\n        # Initialize affinity map\n        request_vol = request[self.affinity_array_key_2]\n        affinity_map = np.zeros(\n            (len(self.affinity_vectors),)\n            + tuple(request_vol.roi.shape / request_vol.voxel_size),\n            dtype=full_vol1.spec.dtype,\n        )\n\n        # calculate affinities\n        vol2 = full_vol2.crop(request_vol.roi)\n        for i, vector in enumerate(self.affinity_vectors):\n            vol1 = full_vol1.crop(request_vol.roi.shift(tuple(vector)))\n            affinity_map[i, :, :, :] = np.bitwise_and(vol1.data, vol2.data)\n\n        batch.arrays[self.affinity_array_key_2] = Array(\n            affinity_map, spec=request[self.affinity_array_key_2].copy()\n        )\n\n        batch.arrays[self.affinity_array_key_2].attrs[\n            \"affinity_vectors\"\n        ] = self.affinity_vectors\n\n        # Crop all other requests\n        for array_key, array in request.array_specs.items():\n            batch.arrays[array_key] = batch.arrays[array_key].crop(array.roi)\n\n        for points_key, points in request.points_specs.items():\n            recropped = batch.points[points_key].spec.roi = points.roi\n            batch.points[points_key] = recropped",
  "def round_to_voxel_size(shape, voxel_size):\n    voxel_size = np.asarray(voxel_size, dtype=float)\n    shape = np.ceil(shape / voxel_size) * voxel_size\n    return np.array(shape, dtype=\"int32\")",
  "def __init__(\n        self,\n        affinity_vectors,\n        array_key_1,\n        array_key_2,\n        affinity_array_key_1,\n        affinity_array_key_2,\n    ):\n        self.array_key_1 = array_key_1\n        self.array_key_2 = array_key_2\n        self.affinity_array_key_1 = affinity_array_key_1\n        self.affinity_array_key_2 = affinity_array_key_2\n        self.affinity_vectors = affinity_vectors",
  "def setup(self):\n        assert self.array_key_1 in self.spec, (\n            \"Upstream does not provide %s needed by \\\n        AddNonsymmetricAffinities\"\n            % self.array_key_1\n        )\n        assert self.array_key_2 in self.spec, (\n            \"Upstream does not provide %s needed by \\\n        AddNonsymmetricAffinities\"\n            % self.array_key_2\n        )\n\n        voxel_size = self.spec[self.array_key_1].voxel_size\n\n        self.upstream_spec = self.get_upstream_provider().spec\n        self.upstream_roi = self.upstream_spec.get_total_roi()\n\n        # get maximum offset in each dimension\n        self.padding = np.max(np.abs(self.affinity_vectors), axis=0)\n        self.padding = tuple(round_to_voxel_size(self.padding, voxel_size))\n\n        logger.debug(\"padding neg: %s\" % np.asarray(self.padding))\n\n        spec = self.spec[self.array_key_1].copy()\n        # if spec.roi is not None:\n\n        self.provides(self.affinity_array_key_1, spec)\n        self.provides(self.affinity_array_key_2, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        array_1_roi = request[self.array_key_1].roi\n        logger.debug(\"downstream %s request: \" % self.array_key_1 + str(array_1_roi))\n\n        array_2_roi = request[self.array_key_2].roi\n        logger.debug(\"downstream %s request: \" % self.array_key_2 + str(array_2_roi))\n\n        # grow labels ROI to accomodate padding TODO: vol 2\n        array_1_roi = array_1_roi.grow(self.padding, self.padding)\n        array_2_roi = array_2_roi.grow(self.padding, self.padding)\n\n        request[self.array_key_1].roi = array_1_roi\n        request[self.array_key_2].roi = array_2_roi\n\n        logger.debug(\"upstream %s request: \" % self.array_key_1 + str(array_1_roi))\n        logger.debug(\"upstream %s request: \" % self.array_key_2 + str(array_2_roi))",
  "def process(self, batch, request):\n        full_vol1 = batch.arrays[self.array_key_1]\n        full_vol2 = batch.arrays[self.array_key_2]\n\n        # Both full_vol1 should match\n        assert (\n            full_vol1.spec.dtype == full_vol2.spec.dtype\n        ), \"data type of array 1(%s) and array 2(%s) should match\" % (\n            full_vol1.spec.dtype,\n            full_vol2.spec.dtype,\n        )\n\n        assert (\n            full_vol1.spec.voxel_size == full_vol2.spec.voxel_size\n        ), \"data type of array 1(%s) and array 2(%s) should match\" % (\n            full_vol1.spec.voxel_size,\n            full_vol2.spec.voxel_size,\n        )\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        # Calculate affinities 1: from vol2 onto vol1\n\n        # Initialize affinity map\n        request_vol = request[self.affinity_array_key_1]\n        affinity_map = np.zeros(\n            (len(self.affinity_vectors),)\n            + tuple(request_vol.roi.shape / request_vol.voxel_size),\n            dtype=full_vol1.spec.dtype,\n        )\n\n        # calculate affinities\n        vol1 = full_vol1.crop(request_vol.roi)\n        for i, vector in enumerate(self.affinity_vectors):\n            vol2 = full_vol2.crop(request_vol.roi.shift(tuple(-vector)))\n            affinity_map[i, :, :, :] = np.bitwise_and(vol1.data, vol2.data)\n\n        batch.arrays[self.affinity_array_key_1] = Array(\n            affinity_map, spec=request[self.affinity_array_key_1].copy()\n        )\n\n        batch.arrays[self.affinity_array_key_1].attrs[\n            \"affinity_vectors\"\n        ] = self.affinity_vectors\n\n        # Calculate affinities 2: from vol1 onto vol2\n\n        # Initialize affinity map\n        request_vol = request[self.affinity_array_key_2]\n        affinity_map = np.zeros(\n            (len(self.affinity_vectors),)\n            + tuple(request_vol.roi.shape / request_vol.voxel_size),\n            dtype=full_vol1.spec.dtype,\n        )\n\n        # calculate affinities\n        vol2 = full_vol2.crop(request_vol.roi)\n        for i, vector in enumerate(self.affinity_vectors):\n            vol1 = full_vol1.crop(request_vol.roi.shift(tuple(vector)))\n            affinity_map[i, :, :, :] = np.bitwise_and(vol1.data, vol2.data)\n\n        batch.arrays[self.affinity_array_key_2] = Array(\n            affinity_map, spec=request[self.affinity_array_key_2].copy()\n        )\n\n        batch.arrays[self.affinity_array_key_2].attrs[\n            \"affinity_vectors\"\n        ] = self.affinity_vectors\n\n        # Crop all other requests\n        for array_key, array in request.array_specs.items():\n            batch.arrays[array_key] = batch.arrays[array_key].crop(array.roi)\n\n        for points_key, points in request.points_specs.items():\n            recropped = batch.points[points_key].spec.roi = points.roi\n            batch.points[points_key] = recropped",
  "class AddBlobsFromPoints(BatchFilter):\n    \"\"\"Add an array with blobs at locations given by a specified points\n    collection. The blobs are also restricted to stay within the same class in\n    the restrictive_mask array that corresponds to the center voxel of the\n    blob.\n\n    Args:\n\n        blob_settings(dict):\n\n            Where each desired output blob map should have it's own entry\n            consisting of the following format:\n\n            `blob_name` : dict (\n\n                'points_key' : Desired point type to use for blob locations\n\n                'output_array_key': Desired array type name for output map\n\n                'output_array_dtype': Desired array dtype name for output map\n\n                'radius': Desired radius of blobs, since blobs are restricted\n                by the restricting mask, this radius should be thought of as\n                the maximum radius of the blobs.\n\n                'output_voxel_size': voxel_size of output array. Voxel size of\n                restrictive mask\n\n                'restrictive_mask_key': Array type of restrictive mask\n\n                'id_mapper': Functor (class with a __call__ function) that can\n                take an ID and map it to some other value. This class should\n                also have a 'make_map' method that will be called at the\n                beggining of each process step and given all ids in all arrays\n                to be processed for that batch.\n            )\n\n            This is an example blob_setting for presynaptic blobs in the cremi\n            dataset::\n\n              add_blob_data = {\n                'PRESYN': {\n                  'points_key': PointsTypes.PRESYN,\n                  'output_array_key': ArrayTypes.PRESYN_BLOB,\n                  'output_array_dtype': 'int64',\n                  'radius': 60,\n                  'output_voxel_size': voxel_size,\n                  'restrictive_mask_key': ArrayTypes.GT_LABELS,\n                  'max_desired_overlap': 0.05\n                }\n              }\n    \"\"\"\n\n    def __init__(self, blob_settings):\n        self.blob_settings = blob_settings\n\n        for points_key, settings in self.blob_settings.items():\n            blob_settings[points_key][\"blob_placer\"] = BlobPlacer(\n                radius=settings[\"radius\"],\n                voxel_size=settings[\"output_voxel_size\"],\n                dtype=settings[\"output_array_dtype\"],\n            )\n\n    def setup(self):\n        for blob_name, settings in self.blob_settings.items():\n            self.provides(\n                settings[\"output_array_key\"],\n                self.spec[settings[\"restrictive_mask_key\"]],\n            )\n\n    def prepare(self, request):\n        for blob_name, settings in self.blob_settings.items():\n            array_key = settings[\"output_array_key\"]\n            if array_key in request:\n                points_key = settings[\"points_key\"]\n                request_roi = request[array_key].roi\n\n                # If point is not already requested, add to request\n                if points_key not in request.points_specs:\n                    request.add(points_key, request_roi.shape)\n                else:\n                    request[points_key].roi = request[points_key].roi.union(request_roi)\n\n                # Get correct size for restrictive_mask_key\n                restrictive_mask_key = settings[\"restrictive_mask_key\"]\n                if restrictive_mask_key not in request.array_specs:\n                    request.add(restrictive_mask_key, request_roi.shape)\n                else:\n                    request[restrictive_mask_key].roi = request[\n                        restrictive_mask_key\n                    ].roi.union(request_roi)\n            else:\n                # do nothing if no blobs of this type were requested\n                logger.warning(\n                    \"%s output array type for %s never requested. \\\n                    Deleting entry...\"\n                    % (settings[\"output_array_key\"], blob_name)\n                )\n                del self.blob_settings[blob_name]\n\n    def process(self, batch, request):\n        # check arrays and gather all IDs and synapse IDs\n        all_points = {}\n        all_synapse_ids = {}\n\n        for blob_name, settings in self.blob_settings.items():\n            # Unpack settings\n            points_key = settings[\"points_key\"]\n            restrictive_mask_key = settings[\"restrictive_mask_key\"]\n\n            # Make sure both the necesary point types and arrays are present\n            assert points_key in batch.points, (\n                \"Upstream does not provide required point type\\\n            : %s\"\n                % points_key\n            )\n\n            assert restrictive_mask_key in batch.arrays, (\n                \"Upstream does not provide required \\\n            array type: %s\"\n                % restrictive_mask_key\n            )\n\n            # Get point data\n            points = batch.points[points_key]\n\n            # If point doesn't have it's corresponding partner, delete it\n            if (\n                \"partner_points\" in settings.keys()\n                and settings[\"partner_points\"] is not None\n            ):\n                partner_points = batch.points[settings[\"partner_points\"]]\n                synapse_ids = []\n                for point_id, point in points.data.items():\n                    # pdb.set_trace()\n                    if not point.partner_ids[0] in partner_points.data.keys():\n                        logger.warning(\n                            \"Point %s has no partner. Deleting...\" % point_id\n                        )\n                        del points.data[point_id]\n                    else:\n                        synapse_ids.append(point.synapse_id)\n\n            all_synapse_ids[points_key] = synapse_ids\n            all_points[points_key] = points\n\n        for blob_name, settings in self.blob_settings.items():\n            # Unpack settings\n            points_key = settings[\"points_key\"]\n            array_key = settings[\"output_array_key\"]\n            voxel_size = settings[\"output_voxel_size\"]\n            restrictive_mask_key = settings[\"restrictive_mask_key\"]\n            restrictive_mask = batch.arrays[restrictive_mask_key].crop(\n                request[array_key].roi\n            )\n\n            id_mapper = settings[\"id_mapper\"]\n            dtype = settings[\"output_array_dtype\"]\n\n            if id_mapper is not None:\n                id_mapper.make_map(all_points)\n\n            # Initialize output array\n            shape_array = np.asarray(request[array_key].roi.shape) / voxel_size\n            blob_map = np.zeros(shape_array, dtype=dtype)\n\n            # Get point data\n            points = batch.points[points_key]\n\n            offset = np.asarray(points.spec.roi.offset)\n            for point_id, point_data in points.data.items():\n                voxel_location = np.round(\n                    ((point_data.location - offset) / (voxel_size))\n                ).astype(\"int32\")\n\n                synapse_id = point_data.synapse_id\n                # if mapping exists, do it\n                if id_mapper is not None:\n                    synapse_id = id_mapper(synapse_id)\n\n                settings[\"blob_placer\"].place(\n                    blob_map, voxel_location, synapse_id, restrictive_mask.data\n                )\n\n            # Provide array\n            batch.arrays[array_key] = Array(blob_map, spec=request[array_key].copy())\n            batch.arrays[array_key].spec.dtype = dtype\n\n            # add id_mapping to attributes\n            if id_mapper is not None:\n                id_map_list = np.array(list(id_mapper.get_map().items()))\n                batch.arrays[array_key].attrs[\"id_mapping\"] = id_map_list\n\n            batch.arrays[array_key].attrs[\"point_ids\"] = points.data.keys()\n            batch.arrays[array_key].attrs[\"synapse_ids\"] = all_synapse_ids[points_key]\n\n            # Crop all other requests\n        for array_key, array in request.array_specs.items():\n            batch.arrays[array_key] = batch.arrays[array_key].crop(array.roi)\n\n        for points_key, points in request.points_specs.items():\n            batch.points[points_key] = batch.points[points_key].spec.roi = points.roi",
  "class BlobPlacer:\n    \"\"\"Places synapse array blobs from location data.\n    Args:\n        radius: int - that desired radius of synaptic blobs\n        voxel_size: array, list, tuple - voxel size in physical\n    \"\"\"\n\n    def __init__(self, radius, voxel_size, dtype=\"uint64\"):\n        self.voxel_size = voxel_size\n        if isinstance(self.voxel_size, (list, tuple)):\n            self.voxel_size = np.asarray(self.voxel_size)\n\n        self.radius = radius / self.voxel_size\n        self.sphere_map = np.zeros(self.radius * 2, dtype=dtype)\n        self.center = (np.asarray(self.sphere_map.shape)) / 2\n\n        ranges = [\n            range(0, self.radius[0] * 2),\n            range(0, self.radius[1] * 2),\n            range(0, self.radius[2] * 2),\n        ]\n\n        for index in np.asarray(list(itertools.product(*ranges))):\n            # if distance less than r, place a 1\n            if np.linalg.norm((self.center - index) * self.voxel_size) <= radius:\n                self.sphere_map[tuple(index)] = 1\n\n        self.sphere_voxel_array = np.sum(self.sphere_map, axis=(0, 1, 2))\n\n    def place(self, matrix, location, marker, mask):\n        \"\"\"Places synapse\n        Args:\n            matrix: 4D np array - 1st dim are for layers to avoid overlap\n            (3 should be more than enough)\n            location: np array - location where to place synaptic blob within given matrix\n            marker: int - the ID used to mark this paricular synapse in the matrix\n            mask:   3D np array - when placing a blob, will sample mask at\n        center location and only place blob in interection where mask has\n        the same ID. Usually used to restrict synaptic blobs inside their\n        respective cells (using segmentation)\n        \"\"\"\n        # Calculate cube circumscribing the sphere to place\n        start = location - self.radius\n        end = location + self.radius\n\n        # check if sphere fits in matrix\n        if np.all(start >= 0) and np.all(np.asarray(matrix.shape) - end >= 0):\n            # calculate actual synapse shape from intersection between sphere and restrictive mask\n            restricting_label = mask[location[0], location[1], location[2]]\n\n            restricting_mask = (\n                mask[start[0] : end[0], start[1] : end[1], start[2] : end[2]]\n                == restricting_label\n            )\n\n            shape = self.sphere_map * restricting_mask\n\n            # place shape in chosen layer\n            matrix[start[0] : end[0], start[1] : end[1], start[2] : end[2]] += (\n                shape * marker\n            )\n            return matrix, True\n\n        logger.warning(\"Location %s out of bounds\" % (location))\n        return matrix, False",
  "def __init__(self, blob_settings):\n        self.blob_settings = blob_settings\n\n        for points_key, settings in self.blob_settings.items():\n            blob_settings[points_key][\"blob_placer\"] = BlobPlacer(\n                radius=settings[\"radius\"],\n                voxel_size=settings[\"output_voxel_size\"],\n                dtype=settings[\"output_array_dtype\"],\n            )",
  "def setup(self):\n        for blob_name, settings in self.blob_settings.items():\n            self.provides(\n                settings[\"output_array_key\"],\n                self.spec[settings[\"restrictive_mask_key\"]],\n            )",
  "def prepare(self, request):\n        for blob_name, settings in self.blob_settings.items():\n            array_key = settings[\"output_array_key\"]\n            if array_key in request:\n                points_key = settings[\"points_key\"]\n                request_roi = request[array_key].roi\n\n                # If point is not already requested, add to request\n                if points_key not in request.points_specs:\n                    request.add(points_key, request_roi.shape)\n                else:\n                    request[points_key].roi = request[points_key].roi.union(request_roi)\n\n                # Get correct size for restrictive_mask_key\n                restrictive_mask_key = settings[\"restrictive_mask_key\"]\n                if restrictive_mask_key not in request.array_specs:\n                    request.add(restrictive_mask_key, request_roi.shape)\n                else:\n                    request[restrictive_mask_key].roi = request[\n                        restrictive_mask_key\n                    ].roi.union(request_roi)\n            else:\n                # do nothing if no blobs of this type were requested\n                logger.warning(\n                    \"%s output array type for %s never requested. \\\n                    Deleting entry...\"\n                    % (settings[\"output_array_key\"], blob_name)\n                )\n                del self.blob_settings[blob_name]",
  "def process(self, batch, request):\n        # check arrays and gather all IDs and synapse IDs\n        all_points = {}\n        all_synapse_ids = {}\n\n        for blob_name, settings in self.blob_settings.items():\n            # Unpack settings\n            points_key = settings[\"points_key\"]\n            restrictive_mask_key = settings[\"restrictive_mask_key\"]\n\n            # Make sure both the necesary point types and arrays are present\n            assert points_key in batch.points, (\n                \"Upstream does not provide required point type\\\n            : %s\"\n                % points_key\n            )\n\n            assert restrictive_mask_key in batch.arrays, (\n                \"Upstream does not provide required \\\n            array type: %s\"\n                % restrictive_mask_key\n            )\n\n            # Get point data\n            points = batch.points[points_key]\n\n            # If point doesn't have it's corresponding partner, delete it\n            if (\n                \"partner_points\" in settings.keys()\n                and settings[\"partner_points\"] is not None\n            ):\n                partner_points = batch.points[settings[\"partner_points\"]]\n                synapse_ids = []\n                for point_id, point in points.data.items():\n                    # pdb.set_trace()\n                    if not point.partner_ids[0] in partner_points.data.keys():\n                        logger.warning(\n                            \"Point %s has no partner. Deleting...\" % point_id\n                        )\n                        del points.data[point_id]\n                    else:\n                        synapse_ids.append(point.synapse_id)\n\n            all_synapse_ids[points_key] = synapse_ids\n            all_points[points_key] = points\n\n        for blob_name, settings in self.blob_settings.items():\n            # Unpack settings\n            points_key = settings[\"points_key\"]\n            array_key = settings[\"output_array_key\"]\n            voxel_size = settings[\"output_voxel_size\"]\n            restrictive_mask_key = settings[\"restrictive_mask_key\"]\n            restrictive_mask = batch.arrays[restrictive_mask_key].crop(\n                request[array_key].roi\n            )\n\n            id_mapper = settings[\"id_mapper\"]\n            dtype = settings[\"output_array_dtype\"]\n\n            if id_mapper is not None:\n                id_mapper.make_map(all_points)\n\n            # Initialize output array\n            shape_array = np.asarray(request[array_key].roi.shape) / voxel_size\n            blob_map = np.zeros(shape_array, dtype=dtype)\n\n            # Get point data\n            points = batch.points[points_key]\n\n            offset = np.asarray(points.spec.roi.offset)\n            for point_id, point_data in points.data.items():\n                voxel_location = np.round(\n                    ((point_data.location - offset) / (voxel_size))\n                ).astype(\"int32\")\n\n                synapse_id = point_data.synapse_id\n                # if mapping exists, do it\n                if id_mapper is not None:\n                    synapse_id = id_mapper(synapse_id)\n\n                settings[\"blob_placer\"].place(\n                    blob_map, voxel_location, synapse_id, restrictive_mask.data\n                )\n\n            # Provide array\n            batch.arrays[array_key] = Array(blob_map, spec=request[array_key].copy())\n            batch.arrays[array_key].spec.dtype = dtype\n\n            # add id_mapping to attributes\n            if id_mapper is not None:\n                id_map_list = np.array(list(id_mapper.get_map().items()))\n                batch.arrays[array_key].attrs[\"id_mapping\"] = id_map_list\n\n            batch.arrays[array_key].attrs[\"point_ids\"] = points.data.keys()\n            batch.arrays[array_key].attrs[\"synapse_ids\"] = all_synapse_ids[points_key]\n\n            # Crop all other requests\n        for array_key, array in request.array_specs.items():\n            batch.arrays[array_key] = batch.arrays[array_key].crop(array.roi)\n\n        for points_key, points in request.points_specs.items():\n            batch.points[points_key] = batch.points[points_key].spec.roi = points.roi",
  "def __init__(self, radius, voxel_size, dtype=\"uint64\"):\n        self.voxel_size = voxel_size\n        if isinstance(self.voxel_size, (list, tuple)):\n            self.voxel_size = np.asarray(self.voxel_size)\n\n        self.radius = radius / self.voxel_size\n        self.sphere_map = np.zeros(self.radius * 2, dtype=dtype)\n        self.center = (np.asarray(self.sphere_map.shape)) / 2\n\n        ranges = [\n            range(0, self.radius[0] * 2),\n            range(0, self.radius[1] * 2),\n            range(0, self.radius[2] * 2),\n        ]\n\n        for index in np.asarray(list(itertools.product(*ranges))):\n            # if distance less than r, place a 1\n            if np.linalg.norm((self.center - index) * self.voxel_size) <= radius:\n                self.sphere_map[tuple(index)] = 1\n\n        self.sphere_voxel_array = np.sum(self.sphere_map, axis=(0, 1, 2))",
  "def place(self, matrix, location, marker, mask):\n        \"\"\"Places synapse\n        Args:\n            matrix: 4D np array - 1st dim are for layers to avoid overlap\n            (3 should be more than enough)\n            location: np array - location where to place synaptic blob within given matrix\n            marker: int - the ID used to mark this paricular synapse in the matrix\n            mask:   3D np array - when placing a blob, will sample mask at\n        center location and only place blob in interection where mask has\n        the same ID. Usually used to restrict synaptic blobs inside their\n        respective cells (using segmentation)\n        \"\"\"\n        # Calculate cube circumscribing the sphere to place\n        start = location - self.radius\n        end = location + self.radius\n\n        # check if sphere fits in matrix\n        if np.all(start >= 0) and np.all(np.asarray(matrix.shape) - end >= 0):\n            # calculate actual synapse shape from intersection between sphere and restrictive mask\n            restricting_label = mask[location[0], location[1], location[2]]\n\n            restricting_mask = (\n                mask[start[0] : end[0], start[1] : end[1], start[2] : end[2]]\n                == restricting_label\n            )\n\n            shape = self.sphere_map * restricting_mask\n\n            # place shape in chosen layer\n            matrix[start[0] : end[0], start[1] : end[1], start[2] : end[2]] += (\n                shape * marker\n            )\n            return matrix, True\n\n        logger.warning(\"Location %s out of bounds\" % (location))\n        return matrix, False",
  "class Hdf5PointsSource(BatchProvider):\n    \"\"\"An HDF5 data source for :class:``Graph``. Currently only supports a\n    specific case where graphs represent pre- and post-synaptic markers.\n\n    Args:\n\n        filename (string): The HDF5 file.\n\n        datasets (dict): Dictionary of :class:``GraphKey`` -> dataset names\n            that this source offers.\n\n        rois (dict): Dictionary of :class:``GraphKey`` -> :class:``Roi`` to\n            set the ROI for each point set provided by this source.\n\n    \"\"\"\n\n    def __init__(self, filename, datasets, rois):\n        self.filename = filename\n        self.datasets = datasets\n        self.rois = rois\n\n        self.ndims = None\n\n    def setup(self):\n        hdf_file = h5py.File(self.filename, \"r\")\n\n        for points_key, ds_name in self.datasets.items():\n            if ds_name not in hdf_file:\n                raise RuntimeError(\"%s not in %s\" % (ds_name, self.filename))\n\n            spec = PointsSpec()\n            spec.roi = self.rois[points_key]\n\n            self.provides(points_key, spec)\n\n        hdf_file.close()\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with h5py.File(self.filename, \"r\") as hdf_file:\n            # if pre and postsynaptic locations required, their id\n            # SynapseLocation dictionaries should be created together s.t. ids\n            # are unique and allow to find partner locations\n            if (\n                PointsKeys.PRESYN in request.points_specs\n                or PointsKeys.POSTSYN in request.points_specs\n            ):\n                assert (\n                    request.points_specs[PointsKeys.PRESYN].roi\n                    == request.points_specs[PointsKeys.POSTSYN].roi\n                )\n                # Cremi specific, ROI offset corresponds to offset present in the\n                # synapse location relative to the raw data.\n                dataset_offset = self.spec[PointsKeys.PRESYN].roi.offset\n                presyn_points, postsyn_points = self.__get_syn_points(\n                    roi=request.points_specs[PointsKeys.PRESYN].roi,\n                    syn_file=hdf_file,\n                    dataset_offset=dataset_offset,\n                )\n\n            for points_key, request_spec in request.points_specs.items():\n                logger.debug(\"Reading %s in %s...\", points_key, request_spec.roi)\n                id_to_point = {\n                    PointsKeys.PRESYN: presyn_points,\n                    PointsKeys.POSTSYN: postsyn_points,\n                }[points_key]\n\n                points_spec = self.spec[points_key].copy()\n                points_spec.roi = request_spec.roi\n                batch.points[points_key] = Points(data=id_to_point, spec=points_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def __get_syn_points(self, roi, syn_file, dataset_offset=None):\n        presyn_points_dict, postsyn_points_dict = {}, {}\n        presyn_node_ids = syn_file[\"annotations/presynaptic_site/partners\"][\n            :, 0\n        ].tolist()\n        postsyn_node_ids = syn_file[\"annotations/presynaptic_site/partners\"][\n            :, 1\n        ].tolist()\n\n        for node_nr, node_id in enumerate(syn_file[\"annotations/ids\"]):\n            location = syn_file[\"annotations/locations\"][node_nr]\n            if dataset_offset is not None:\n                logging.debug(\n                    \"adding global offset to points %i %i %i\"\n                    % (dataset_offset[0], dataset_offset[1], dataset_offset[2])\n                )\n                location += dataset_offset\n\n            # cremi synapse locations are in physical space\n            if roi.contains(Coordinate(location)):\n                if node_id in presyn_node_ids:\n                    kind = \"PreSyn\"\n                    assert syn_file[\"annotations/types\"][node_nr] == \"presynaptic_site\"\n                    syn_id = int(np.where(presyn_node_ids == node_id)[0])\n                    partner_node_id = postsyn_node_ids[syn_id]\n                elif node_id in postsyn_node_ids:\n                    kind = \"PostSyn\"\n                    assert syn_file[\"annotations/types\"][node_nr] == \"postsynaptic_site\"\n                    syn_id = int(np.where(postsyn_node_ids == node_id)[0])\n                    partner_node_id = presyn_node_ids[syn_id]\n                else:\n                    raise Exception(\"Node id neither pre- no post-synaptic\")\n\n                partners_ids = [int(partner_node_id)]\n                location_id = int(node_id)\n\n                props = {}\n                if node_id in syn_file[\"annotations/comments/target_ids\"]:\n                    props = {\"unsure\": True}\n\n                # create synpaseLocation & add to dict\n                if kind == \"PreSyn\":\n                    syn_point = PreSynPoint(\n                        location=location,\n                        location_id=location_id,\n                        synapse_id=syn_id,\n                        partner_ids=partners_ids,\n                        props=props,\n                    )\n                    presyn_points_dict[int(node_id)] = copy.deepcopy(syn_point)\n                elif kind == \"PostSyn\":\n                    syn_point = PostSynPoint(\n                        location=location,\n                        location_id=location_id,\n                        synapse_id=syn_id,\n                        partner_ids=partners_ids,\n                        props=props,\n                    )\n                    postsyn_points_dict[int(node_id)] = copy.deepcopy(syn_point)\n\n        return presyn_points_dict, postsyn_points_dict\n\n    def __repr__(self):\n        return self.filename",
  "def __init__(self, filename, datasets, rois):\n        self.filename = filename\n        self.datasets = datasets\n        self.rois = rois\n\n        self.ndims = None",
  "def setup(self):\n        hdf_file = h5py.File(self.filename, \"r\")\n\n        for points_key, ds_name in self.datasets.items():\n            if ds_name not in hdf_file:\n                raise RuntimeError(\"%s not in %s\" % (ds_name, self.filename))\n\n            spec = PointsSpec()\n            spec.roi = self.rois[points_key]\n\n            self.provides(points_key, spec)\n\n        hdf_file.close()",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with h5py.File(self.filename, \"r\") as hdf_file:\n            # if pre and postsynaptic locations required, their id\n            # SynapseLocation dictionaries should be created together s.t. ids\n            # are unique and allow to find partner locations\n            if (\n                PointsKeys.PRESYN in request.points_specs\n                or PointsKeys.POSTSYN in request.points_specs\n            ):\n                assert (\n                    request.points_specs[PointsKeys.PRESYN].roi\n                    == request.points_specs[PointsKeys.POSTSYN].roi\n                )\n                # Cremi specific, ROI offset corresponds to offset present in the\n                # synapse location relative to the raw data.\n                dataset_offset = self.spec[PointsKeys.PRESYN].roi.offset\n                presyn_points, postsyn_points = self.__get_syn_points(\n                    roi=request.points_specs[PointsKeys.PRESYN].roi,\n                    syn_file=hdf_file,\n                    dataset_offset=dataset_offset,\n                )\n\n            for points_key, request_spec in request.points_specs.items():\n                logger.debug(\"Reading %s in %s...\", points_key, request_spec.roi)\n                id_to_point = {\n                    PointsKeys.PRESYN: presyn_points,\n                    PointsKeys.POSTSYN: postsyn_points,\n                }[points_key]\n\n                points_spec = self.spec[points_key].copy()\n                points_spec.roi = request_spec.roi\n                batch.points[points_key] = Points(data=id_to_point, spec=points_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __get_syn_points(self, roi, syn_file, dataset_offset=None):\n        presyn_points_dict, postsyn_points_dict = {}, {}\n        presyn_node_ids = syn_file[\"annotations/presynaptic_site/partners\"][\n            :, 0\n        ].tolist()\n        postsyn_node_ids = syn_file[\"annotations/presynaptic_site/partners\"][\n            :, 1\n        ].tolist()\n\n        for node_nr, node_id in enumerate(syn_file[\"annotations/ids\"]):\n            location = syn_file[\"annotations/locations\"][node_nr]\n            if dataset_offset is not None:\n                logging.debug(\n                    \"adding global offset to points %i %i %i\"\n                    % (dataset_offset[0], dataset_offset[1], dataset_offset[2])\n                )\n                location += dataset_offset\n\n            # cremi synapse locations are in physical space\n            if roi.contains(Coordinate(location)):\n                if node_id in presyn_node_ids:\n                    kind = \"PreSyn\"\n                    assert syn_file[\"annotations/types\"][node_nr] == \"presynaptic_site\"\n                    syn_id = int(np.where(presyn_node_ids == node_id)[0])\n                    partner_node_id = postsyn_node_ids[syn_id]\n                elif node_id in postsyn_node_ids:\n                    kind = \"PostSyn\"\n                    assert syn_file[\"annotations/types\"][node_nr] == \"postsynaptic_site\"\n                    syn_id = int(np.where(postsyn_node_ids == node_id)[0])\n                    partner_node_id = presyn_node_ids[syn_id]\n                else:\n                    raise Exception(\"Node id neither pre- no post-synaptic\")\n\n                partners_ids = [int(partner_node_id)]\n                location_id = int(node_id)\n\n                props = {}\n                if node_id in syn_file[\"annotations/comments/target_ids\"]:\n                    props = {\"unsure\": True}\n\n                # create synpaseLocation & add to dict\n                if kind == \"PreSyn\":\n                    syn_point = PreSynPoint(\n                        location=location,\n                        location_id=location_id,\n                        synapse_id=syn_id,\n                        partner_ids=partners_ids,\n                        props=props,\n                    )\n                    presyn_points_dict[int(node_id)] = copy.deepcopy(syn_point)\n                elif kind == \"PostSyn\":\n                    syn_point = PostSynPoint(\n                        location=location,\n                        location_id=location_id,\n                        synapse_id=syn_id,\n                        partner_ids=partners_ids,\n                        props=props,\n                    )\n                    postsyn_points_dict[int(node_id)] = copy.deepcopy(syn_point)\n\n        return presyn_points_dict, postsyn_points_dict",
  "def __repr__(self):\n        return self.filename",
  "class ZeroOutConstSections(BatchFilter):\n    \"\"\"Every z-section that has constant values only will be set to 0.\n\n    This is to handle blank (missing) sections in a less invasive way: Instead\n    of leaving them at -1 (which is \"black\", the lowest possible input to the\n    CNN), 0 (\"gray\") might be easier to ignore.\n\n    For that you should call this filter after you are done with all other\n    intensity manipulations.\n    \"\"\"\n\n    def __init__(self, intensities):\n        self.intensities = intensities\n\n    def process(self, batch, request):\n        assert batch.get_total_roi().dims == 3, \"This filter only works on 3D data.\"\n\n        raw = batch.arrays[self.intensities]\n\n        for z in range(\n            (raw.spec.roi / self.spec[self.intensities].voxel_size).get_shape()[0]\n        ):\n            if raw.data[z].min() == raw.data[z].max():\n                raw.data[z] = 0",
  "def __init__(self, intensities):\n        self.intensities = intensities",
  "def process(self, batch, request):\n        assert batch.get_total_roi().dims == 3, \"This filter only works on 3D data.\"\n\n        raw = batch.arrays[self.intensities]\n\n        for z in range(\n            (raw.spec.roi / self.spec[self.intensities].voxel_size).get_shape()[0]\n        ):\n            if raw.data[z].min() == raw.data[z].max():\n                raw.data[z] = 0",
  "class DvidPartnerAnnoationSourceReadException(Exception):\n    pass",
  "class MaskNotProvidedException(Exception):\n    pass",
  "class DvidPartnerAnnotationSource(BatchProvider):\n    \"\"\"\n    :param hostname: hostname for DVID server\n    :type hostname: str\n\n    :param port: port for DVID server\n    :type port: int\n\n    :param uuid: UUID of node on DVID server\n    :type uuid: str\n\n    :param datasets: dict {GraphKey: DVID data instance}\n    \"\"\"\n\n    def __init__(self, hostname, port, uuid, datasets=None, rois=None):\n        self.hostname = hostname\n        self.port = port\n        self.url = \"http://{}:{}\".format(self.hostname, self.port)\n        self.uuid = uuid\n\n        self.datasets = datasets if datasets is not None else {}\n        self.rois = rois if rois is not None else {}\n\n        self.node_service = None\n        self.dims = 0\n\n    def setup(self):\n        for points_key, points_name in self.datasets.items():\n            self.provides(points_key, GraphSpec(roi=self.points_rois[points_key]))\n\n        logger.info(\"DvidPartnerAnnotationSource.spec:\\n{}\".format(self.spec))\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        # if pre and postsynaptic locations requested, their id : SynapseLocation dictionaries should be created\n        # together s.t. the ids are unique and allow to find partner locations\n        if GraphKey.PRESYN in request.points or GraphKey.POSTSYN in request.points:\n            try:  # either both have the same roi, or only one of them is requested\n                assert (\n                    request.points[GraphKey.PRESYN] == request.points[GraphKey.POSTSYN]\n                )\n            except AssertionError:\n                assert (\n                    GraphKey.PRESYN not in request.points\n                    or GraphKey.POSTSYN not in request.points\n                )\n            if GraphKey.PRESYN in request.points:\n                presyn_points, postsyn_points = self.__read_syn_points(\n                    roi=request.points[GraphKey.PRESYN]\n                )\n            elif GraphKey.POSTSYN in request.points:\n                presyn_points, postsyn_points = self.__read_syn_points(\n                    roi=request.points[GraphKey.POSTSYN]\n                )\n\n        for points_key, roi in request.points.items():\n            # check if requested points can be provided\n            if points_key not in self.spec:\n                raise RuntimeError(\n                    \"Asked for %s which this source does not provide\" % points_key\n                )\n            # check if request roi lies within provided roi\n            if not self.spec[points_key].roi.contains(roi):\n                raise RuntimeError(\n                    \"%s's ROI %s outside of my ROI %s\"\n                    % (points_key, roi, self.spec[points_key].roi)\n                )\n\n            logger.debug(\"Reading %s in %s...\" % (points_key, roi))\n            id_to_point = {\n                GraphKey.PRESYN: presyn_points,\n                GraphKey.POSTSYN: postsyn_points,\n            }[points_key]\n\n            batch.points[points_key] = Graph(data=id_to_point, spec=GraphSpec(roi=roi))\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def __load_json_annotations(\n        self, array_shape_voxel, array_offset_voxel, array_name\n    ):\n        url = (\n            \"http://\"\n            + str(self.hostname)\n            + \":\"\n            + str(self.port)\n            + \"/api/node/\"\n            + str(self.uuid)\n            + \"/\"\n            + str(array_name)\n            + \"/elements/{}_{}_{}/{}_{}_{}\".format(\n                array_shape_voxel[2],\n                array_shape_voxel[1],\n                array_shape_voxel[0],\n                array_offset_voxel[2],\n                array_offset_voxel[1],\n                array_offset_voxel[0],\n            )\n        )\n        annotations_file = requests.get(url)\n        json_annotations = annotations_file.json()\n        if json_annotations is None:\n            json_annotations = []  # create empty_dummy_json_annotations\n            # raise Exception ('No synapses found in region defined by array_offset {} and array_shape {}'.format(array_offset, array_shape))\n        return json_annotations\n\n    def __read_syn_points(self, roi):\n        \"\"\"read json file from dvid source, in json format to create for every location given\"\"\"\n\n        if GraphKey.PRESYN in self.points_voxel_size:\n            voxel_size = self.points_voxel_size[GraphKey.PRESYN]\n        elif GraphKey.POSTSYN in self.points_voxel_size:\n            voxel_size = self.points_voxel_size[GraphKey.POSTSYN]\n\n        syn_file_json = self.__load_json_annotations(\n            array_shape_voxel=roi.shape // voxel_size,\n            array_offset_voxel=roi.offset // voxel_size,\n            array_name=self.datasets[GraphKey.PRESYN],\n        )\n\n        presyn_points_dict, postsyn_points_dict = {}, {}\n        location_to_location_id_dict, location_id_to_partner_locations = {}, {}\n        for node_nr, node in enumerate(syn_file_json):\n            # collect information\n            kind = str(node[\"Kind\"])\n            location = (\n                np.asarray((node[\"Pos\"][2], node[\"Pos\"][1], node[\"Pos\"][0]))\n                * voxel_size\n            )\n            location_id = int(node_nr)\n            # some synapses are wrongly annotated in dvid source, have 'Tag': null ???, they are skipped\n            try:\n                syn_id = int(node[\"Tags\"][0][3:])\n            except:\n                continue\n            location_to_location_id_dict[str(location)] = location_id\n\n            partner_locations = []\n            try:\n                for relation in node[\"Rels\"]:\n                    partner_locations.append(\n                        (\n                            np.asarray(\n                                [\n                                    relation[\"To\"][2],\n                                    relation[\"To\"][1],\n                                    relation[\"To\"][0],\n                                ]\n                            )\n                        )\n                        * voxel_size\n                    )\n            except:\n                partner_locations = []\n            location_id_to_partner_locations[int(node_nr)] = partner_locations\n\n            # check if property given, not always given\n            props = {}\n            if \"conf\" in node[\"Prop\"]:\n                props[\"conf\"] = float(node[\"Prop\"][\"conf\"])\n            if \"agent\" in node[\"Prop\"]:\n                props[\"agent\"] = str(node[\"Prop\"][\"agent\"])\n            if \"flagged\" in node[\"Prop\"]:\n                str_value_flagged = str(node[\"Prop\"][\"flagged\"])\n                props[\"flagged\"] = bool(distutils.util.strtobool(str_value_flagged))\n            if \"multi\" in node[\"Prop\"]:\n                str_value_multi = str(node[\"Prop\"][\"multi\"])\n                props[\"multi\"] = bool(distutils.util.strtobool(str_value_multi))\n\n            # create synPoint with information collected so far (partner_ids not completed yet)\n            if kind == \"PreSyn\":\n                syn_point = Node(\n                    location=location,\n                    location_id=location_id,\n                    synapse_id=syn_id,\n                    partner_ids=[],\n                    props=props,\n                )\n                presyn_points_dict[int(node_nr)] = deepcopy(syn_point)\n            elif kind == \"PostSyn\":\n                syn_(\n                    location=location,\n                    location_id=location_id,\n                    synapse_id=syn_id,\n                    partner_ids=[],\n                    props=props,\n                )\n                postsyn_points_dict[int(node_nr)] = deepcopy(syn_point)\n\n        # add partner ids\n        last_node_nr = len(syn_file_json) - 1\n        for current_syn_point_id in location_id_to_partner_locations.keys():\n            all_partner_ids = []\n            for partner_loc in location_id_to_partner_locations[current_syn_point_id]:\n                if location_to_location_id_dict.has_key(str(partner_loc)):\n                    all_partner_ids.append(\n                        int(location_to_location_id_dict[str(partner_loc)])\n                    )\n                else:\n                    last_node_nr = last_node_nr + 1\n                    assert not location_to_location_id_dict.has_key(str(partner_loc))\n                    all_partner_ids.append(int(last_node_nr))\n\n            if current_syn_point_id in presyn_points_dict:\n                presyn_points_dict[current_syn_point_id].partner_ids = all_partner_ids\n            elif current_syn_point_id in postsyn_points_dict:\n                postsyn_points_dict[current_syn_point_id].partner_ids = all_partner_ids\n            else:\n                raise Exception(\"current syn_point id not found in any dictionary\")\n\n        return presyn_points_dict, postsyn_points_dict\n\n    def __repr__(self):\n        return \"DvidPartnerAnnoationSource(hostname={}, port={}, uuid={}, raw_array_name={}, gt_array_name={}\".format(\n            self.hostname,\n            self.port,\n            self.uuid,\n            self.array_names[ArrayKeys.RAW],\n            self.array_names[ArrayKeys.GT_LABELS],\n        )",
  "def __init__(self, hostname, port, uuid, datasets=None, rois=None):\n        self.hostname = hostname\n        self.port = port\n        self.url = \"http://{}:{}\".format(self.hostname, self.port)\n        self.uuid = uuid\n\n        self.datasets = datasets if datasets is not None else {}\n        self.rois = rois if rois is not None else {}\n\n        self.node_service = None\n        self.dims = 0",
  "def setup(self):\n        for points_key, points_name in self.datasets.items():\n            self.provides(points_key, GraphSpec(roi=self.points_rois[points_key]))\n\n        logger.info(\"DvidPartnerAnnotationSource.spec:\\n{}\".format(self.spec))",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        # if pre and postsynaptic locations requested, their id : SynapseLocation dictionaries should be created\n        # together s.t. the ids are unique and allow to find partner locations\n        if GraphKey.PRESYN in request.points or GraphKey.POSTSYN in request.points:\n            try:  # either both have the same roi, or only one of them is requested\n                assert (\n                    request.points[GraphKey.PRESYN] == request.points[GraphKey.POSTSYN]\n                )\n            except AssertionError:\n                assert (\n                    GraphKey.PRESYN not in request.points\n                    or GraphKey.POSTSYN not in request.points\n                )\n            if GraphKey.PRESYN in request.points:\n                presyn_points, postsyn_points = self.__read_syn_points(\n                    roi=request.points[GraphKey.PRESYN]\n                )\n            elif GraphKey.POSTSYN in request.points:\n                presyn_points, postsyn_points = self.__read_syn_points(\n                    roi=request.points[GraphKey.POSTSYN]\n                )\n\n        for points_key, roi in request.points.items():\n            # check if requested points can be provided\n            if points_key not in self.spec:\n                raise RuntimeError(\n                    \"Asked for %s which this source does not provide\" % points_key\n                )\n            # check if request roi lies within provided roi\n            if not self.spec[points_key].roi.contains(roi):\n                raise RuntimeError(\n                    \"%s's ROI %s outside of my ROI %s\"\n                    % (points_key, roi, self.spec[points_key].roi)\n                )\n\n            logger.debug(\"Reading %s in %s...\" % (points_key, roi))\n            id_to_point = {\n                GraphKey.PRESYN: presyn_points,\n                GraphKey.POSTSYN: postsyn_points,\n            }[points_key]\n\n            batch.points[points_key] = Graph(data=id_to_point, spec=GraphSpec(roi=roi))\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __load_json_annotations(\n        self, array_shape_voxel, array_offset_voxel, array_name\n    ):\n        url = (\n            \"http://\"\n            + str(self.hostname)\n            + \":\"\n            + str(self.port)\n            + \"/api/node/\"\n            + str(self.uuid)\n            + \"/\"\n            + str(array_name)\n            + \"/elements/{}_{}_{}/{}_{}_{}\".format(\n                array_shape_voxel[2],\n                array_shape_voxel[1],\n                array_shape_voxel[0],\n                array_offset_voxel[2],\n                array_offset_voxel[1],\n                array_offset_voxel[0],\n            )\n        )\n        annotations_file = requests.get(url)\n        json_annotations = annotations_file.json()\n        if json_annotations is None:\n            json_annotations = []  # create empty_dummy_json_annotations\n            # raise Exception ('No synapses found in region defined by array_offset {} and array_shape {}'.format(array_offset, array_shape))\n        return json_annotations",
  "def __read_syn_points(self, roi):\n        \"\"\"read json file from dvid source, in json format to create for every location given\"\"\"\n\n        if GraphKey.PRESYN in self.points_voxel_size:\n            voxel_size = self.points_voxel_size[GraphKey.PRESYN]\n        elif GraphKey.POSTSYN in self.points_voxel_size:\n            voxel_size = self.points_voxel_size[GraphKey.POSTSYN]\n\n        syn_file_json = self.__load_json_annotations(\n            array_shape_voxel=roi.shape // voxel_size,\n            array_offset_voxel=roi.offset // voxel_size,\n            array_name=self.datasets[GraphKey.PRESYN],\n        )\n\n        presyn_points_dict, postsyn_points_dict = {}, {}\n        location_to_location_id_dict, location_id_to_partner_locations = {}, {}\n        for node_nr, node in enumerate(syn_file_json):\n            # collect information\n            kind = str(node[\"Kind\"])\n            location = (\n                np.asarray((node[\"Pos\"][2], node[\"Pos\"][1], node[\"Pos\"][0]))\n                * voxel_size\n            )\n            location_id = int(node_nr)\n            # some synapses are wrongly annotated in dvid source, have 'Tag': null ???, they are skipped\n            try:\n                syn_id = int(node[\"Tags\"][0][3:])\n            except:\n                continue\n            location_to_location_id_dict[str(location)] = location_id\n\n            partner_locations = []\n            try:\n                for relation in node[\"Rels\"]:\n                    partner_locations.append(\n                        (\n                            np.asarray(\n                                [\n                                    relation[\"To\"][2],\n                                    relation[\"To\"][1],\n                                    relation[\"To\"][0],\n                                ]\n                            )\n                        )\n                        * voxel_size\n                    )\n            except:\n                partner_locations = []\n            location_id_to_partner_locations[int(node_nr)] = partner_locations\n\n            # check if property given, not always given\n            props = {}\n            if \"conf\" in node[\"Prop\"]:\n                props[\"conf\"] = float(node[\"Prop\"][\"conf\"])\n            if \"agent\" in node[\"Prop\"]:\n                props[\"agent\"] = str(node[\"Prop\"][\"agent\"])\n            if \"flagged\" in node[\"Prop\"]:\n                str_value_flagged = str(node[\"Prop\"][\"flagged\"])\n                props[\"flagged\"] = bool(distutils.util.strtobool(str_value_flagged))\n            if \"multi\" in node[\"Prop\"]:\n                str_value_multi = str(node[\"Prop\"][\"multi\"])\n                props[\"multi\"] = bool(distutils.util.strtobool(str_value_multi))\n\n            # create synPoint with information collected so far (partner_ids not completed yet)\n            if kind == \"PreSyn\":\n                syn_point = Node(\n                    location=location,\n                    location_id=location_id,\n                    synapse_id=syn_id,\n                    partner_ids=[],\n                    props=props,\n                )\n                presyn_points_dict[int(node_nr)] = deepcopy(syn_point)\n            elif kind == \"PostSyn\":\n                syn_(\n                    location=location,\n                    location_id=location_id,\n                    synapse_id=syn_id,\n                    partner_ids=[],\n                    props=props,\n                )\n                postsyn_points_dict[int(node_nr)] = deepcopy(syn_point)\n\n        # add partner ids\n        last_node_nr = len(syn_file_json) - 1\n        for current_syn_point_id in location_id_to_partner_locations.keys():\n            all_partner_ids = []\n            for partner_loc in location_id_to_partner_locations[current_syn_point_id]:\n                if location_to_location_id_dict.has_key(str(partner_loc)):\n                    all_partner_ids.append(\n                        int(location_to_location_id_dict[str(partner_loc)])\n                    )\n                else:\n                    last_node_nr = last_node_nr + 1\n                    assert not location_to_location_id_dict.has_key(str(partner_loc))\n                    all_partner_ids.append(int(last_node_nr))\n\n            if current_syn_point_id in presyn_points_dict:\n                presyn_points_dict[current_syn_point_id].partner_ids = all_partner_ids\n            elif current_syn_point_id in postsyn_points_dict:\n                postsyn_points_dict[current_syn_point_id].partner_ids = all_partner_ids\n            else:\n                raise Exception(\"current syn_point id not found in any dictionary\")\n\n        return presyn_points_dict, postsyn_points_dict",
  "def __repr__(self):\n        return \"DvidPartnerAnnoationSource(hostname={}, port={}, uuid={}, raw_array_name={}, gt_array_name={}\".format(\n            self.hostname,\n            self.port,\n            self.uuid,\n            self.array_names[ArrayKeys.RAW],\n            self.array_names[ArrayKeys.GT_LABELS],\n        )",
  "class AddGtMaskExclusiveZone(BatchFilter):\n    \"\"\"Create ExclusizeZone mask for a binary map in batch and add it as\n    array to batch.\n\n    An ExclusiveZone mask is a bianry mask [0,1] where locations which lie\n    within a given distance to the ON (=1) regions (surrounding the ON regions)\n    of the given binary map are set to 0, whereas all the others are set to 1.\n\n    Args:\n\n        EZ_masks_to_binary_map(dict, :class:``ArrayKey``->:class:``ArrayKey``):\n            Arrays of exclusive zones (keys of dict) to create for which\n            binary mask (values of dict).\n\n        gaussian_sigma_for_zone(float, optional): Defines extend of exclusive\n            zone around ON region in binary map. Defaults to 1.\n\n        rasterization_setting(:class:``RasterizationSettings``, optional): Which\n            rasterization setting to use.\n    \"\"\"\n\n    def __init__(\n        self,\n        EZ_masks_to_binary_map,\n        gaussian_sigma_for_zone=1,\n        rasterization_setting=None,\n    ):\n        self.EZ_masks_to_binary_map = EZ_masks_to_binary_map\n        self.gaussian_sigma_for_zone = gaussian_sigma_for_zone\n        if rasterization_setting is None:\n            self.rasterization_setting = RasterizationSettings()\n        else:\n            self.rasterization_setting = rasterization_setting\n        self.skip_next = False\n\n    def setup(self):\n        self.upstream_spec = self.get_upstream_provider().get_spec()\n        self.spec = copy.deepcopy(self.upstream_spec)\n\n        for EZ_mask_type, binary_map_type in self.EZ_masks_to_binary_map.items():\n            if binary_map_type in self.upstream_spec.arrays:\n                self.spec.arrays[EZ_mask_type] = self.spec.arrays[binary_map_type]\n\n    def get_spec(self):\n        return self.spec\n\n    def prepare(self, request):\n        self.EZ_masks_to_create = []\n        for EZ_mask_type, binary_map_type in self.EZ_masks_to_binary_map.items():\n            # do nothing if binary mask to create EZ mask around is not requested as well\n            if EZ_mask_type in request.arrays:\n                # assert that binary mask for which EZ mask is created for is requested\n                assert (\n                    binary_map_type in request.arrays\n                ), \"ExclusiveZone Mask for {}, can only be created if {} also requested.\".format(\n                    EZ_mask_type, binary_map_type\n                )\n                # assert that ROI of EZ lies within ROI of binary mask\n                assert request.arrays[binary_map_type].contains(\n                    request.arrays[EZ_mask_type]\n                ), \"EZ mask for {} requested for ROI outside of source's ({}) ROI.\".format(\n                    EZ_mask_type, binary_map_type\n                )\n\n                self.EZ_masks_to_create.append(EZ_mask_type)\n                del request.arrays[EZ_mask_type]\n\n        if len(self.EZ_masks_to_create) == 0:\n            logger.warn(\"no ExclusiveZone Masks requested, will do nothing\")\n            self.skip_next = True\n\n    def process(self, batch, request):\n        # do nothing if no gt binary maps were requested\n        if self.skip_next:\n            self.skip_next = False\n            return\n\n        for EZ_mask_type in self.EZ_masks_to_create:\n            binary_map_type = self.EZ_masks_to_binary_map[EZ_mask_type]\n            binary_map = batch.arrays[binary_map_type].data\n            resolution = batch.arrays[binary_map_type].resolution\n            EZ_mask = self.__get_exclusivezone_mask(\n                binary_map,\n                shape_EZ_mask=request.arrays[EZ_mask_type].get_shape(),\n                resolution=resolution,\n            )\n\n            batch.arrays[EZ_mask_type] = Array(\n                data=EZ_mask, roi=request.arrays[EZ_mask_type], resolution=resolution\n            )\n\n    def __get_exclusivezone_mask(self, binary_map, shape_EZ_mask, resolution=None):\n        \"\"\"Exclusive zone surrounds every synapse. Created by enlarging the ON regions of given binary map\n        with different gaussian filter, make it binary and subtract the original binary map from it\n        \"\"\"\n\n        shape_diff = np.asarray(binary_map.shape - np.asarray(shape_EZ_mask))\n        slices = [\n            slice(diff, shape - diff)\n            for diff, shape in zip(shape_diff, binary_map.shape)\n        ]\n        relevant_binary_map = binary_map[slices]\n\n        BM_enlarged_binary = enlarge_binary_map(\n            relevant_binary_map,\n            marker_size_voxel=self.rasterization_setting.marker_size_voxel,\n            voxel_size=resolution,\n            marker_size_physical=self.rasterization_setting.marker_size_physical,\n        )\n\n        exclusive_zone = np.ones_like(BM_enlarged_binary) - (\n            BM_enlarged_binary - relevant_binary_map\n        )\n        return exclusive_zone",
  "def __init__(\n        self,\n        EZ_masks_to_binary_map,\n        gaussian_sigma_for_zone=1,\n        rasterization_setting=None,\n    ):\n        self.EZ_masks_to_binary_map = EZ_masks_to_binary_map\n        self.gaussian_sigma_for_zone = gaussian_sigma_for_zone\n        if rasterization_setting is None:\n            self.rasterization_setting = RasterizationSettings()\n        else:\n            self.rasterization_setting = rasterization_setting\n        self.skip_next = False",
  "def setup(self):\n        self.upstream_spec = self.get_upstream_provider().get_spec()\n        self.spec = copy.deepcopy(self.upstream_spec)\n\n        for EZ_mask_type, binary_map_type in self.EZ_masks_to_binary_map.items():\n            if binary_map_type in self.upstream_spec.arrays:\n                self.spec.arrays[EZ_mask_type] = self.spec.arrays[binary_map_type]",
  "def get_spec(self):\n        return self.spec",
  "def prepare(self, request):\n        self.EZ_masks_to_create = []\n        for EZ_mask_type, binary_map_type in self.EZ_masks_to_binary_map.items():\n            # do nothing if binary mask to create EZ mask around is not requested as well\n            if EZ_mask_type in request.arrays:\n                # assert that binary mask for which EZ mask is created for is requested\n                assert (\n                    binary_map_type in request.arrays\n                ), \"ExclusiveZone Mask for {}, can only be created if {} also requested.\".format(\n                    EZ_mask_type, binary_map_type\n                )\n                # assert that ROI of EZ lies within ROI of binary mask\n                assert request.arrays[binary_map_type].contains(\n                    request.arrays[EZ_mask_type]\n                ), \"EZ mask for {} requested for ROI outside of source's ({}) ROI.\".format(\n                    EZ_mask_type, binary_map_type\n                )\n\n                self.EZ_masks_to_create.append(EZ_mask_type)\n                del request.arrays[EZ_mask_type]\n\n        if len(self.EZ_masks_to_create) == 0:\n            logger.warn(\"no ExclusiveZone Masks requested, will do nothing\")\n            self.skip_next = True",
  "def process(self, batch, request):\n        # do nothing if no gt binary maps were requested\n        if self.skip_next:\n            self.skip_next = False\n            return\n\n        for EZ_mask_type in self.EZ_masks_to_create:\n            binary_map_type = self.EZ_masks_to_binary_map[EZ_mask_type]\n            binary_map = batch.arrays[binary_map_type].data\n            resolution = batch.arrays[binary_map_type].resolution\n            EZ_mask = self.__get_exclusivezone_mask(\n                binary_map,\n                shape_EZ_mask=request.arrays[EZ_mask_type].get_shape(),\n                resolution=resolution,\n            )\n\n            batch.arrays[EZ_mask_type] = Array(\n                data=EZ_mask, roi=request.arrays[EZ_mask_type], resolution=resolution\n            )",
  "def __get_exclusivezone_mask(self, binary_map, shape_EZ_mask, resolution=None):\n        \"\"\"Exclusive zone surrounds every synapse. Created by enlarging the ON regions of given binary map\n        with different gaussian filter, make it binary and subtract the original binary map from it\n        \"\"\"\n\n        shape_diff = np.asarray(binary_map.shape - np.asarray(shape_EZ_mask))\n        slices = [\n            slice(diff, shape - diff)\n            for diff, shape in zip(shape_diff, binary_map.shape)\n        ]\n        relevant_binary_map = binary_map[slices]\n\n        BM_enlarged_binary = enlarge_binary_map(\n            relevant_binary_map,\n            marker_size_voxel=self.rasterization_setting.marker_size_voxel,\n            voxel_size=resolution,\n            marker_size_physical=self.rasterization_setting.marker_size_physical,\n        )\n\n        exclusive_zone = np.ones_like(BM_enlarged_binary) - (\n            BM_enlarged_binary - relevant_binary_map\n        )\n        return exclusive_zone",
  "class AddVectorMap(BatchFilter):\n    def __init__(\n        self,\n        src_and_trg_points,\n        voxel_sizes,\n        radius_phys,\n        partner_criterion,\n        stayinside_array_keys=None,\n        pad_for_partners=(0, 0, 0),\n    ):\n        \"\"\"Creates a vector map of shape [dim_vector, [shape_of_array]] (e.g. [3, 50,50,50] for an array of\n            shape (50,50,50)) where every voxel which is close to a any source point location has a vector which points to\n            one of the source point location's target location.\n            Close to a point location in src_point includes all voxels which\n                1) lie within distance radius_phys of the considered src point location\n                2) (if stayinside_array_keys is not None), lie within the same segment as the src location in the\n                    mask provided in stayinside_array_keys.\n            The partner_criterion decides to which target location of the source point location that the vector of a\n            voxel points (the different criterions are described below).\n\n        Args:\n            src_and_trg_points (dict):      Dictionary from :class:``ArrayKey`` of the vector map to be created\n                                            to a tuple (:class:``GraphKey`` of the source points, :class:``GraphKey``\n                                            of the target points) which define the source and target points.\n            voxel_sizes (dict):             Dictionary from\n                                            :class:``ArrayKey`` of the vector\n                                            map to be created to a\n                                            :class:`Coordinate` for the voxel\n                                            size of the array.\n            stayinside_array_keys (dict):  Dictionary from :class:``ArrayKey`` of the vector map to be created to\n                                            :class:``ArrayKey`` of the stayinside_array.\n                                            The stayinside_array is assumed to contain discrete objects labeled with\n                                            different object ids. The object id at the specific source location is used\n                                            to restrict the region where vectors are created around a source location.\n                                            Voxels that are located outside of this object are set to zero.\n                                            If stayinside_array_keys is None, all the voxels within distance\n                                            radius_phys to the source location receive a vector.\n            pad_for_partners (tuple):       n-dim tuple which defines padding of trg_points request in all dimensions\n                                            (in world units).\n                                            This might be used s.t. also partner locations which lie within the padded\n                                            region, hence slightly outside of the vector map's roi, are considered.\n            radius_phys (int):              Radius (in world units) to restrict region where vectors are created around\n                                            a source location.\n            partner_criterion(str):         'min_distance' or 'all'\n                                            'min_distance': the vectors of all the voxels around a source location\n                                            point to the same target location, namely the location which has the\n                                            minimal distance to the considered source location.\n                                            'all': all partner locations of a given source location are considered.\n                                            The region around a source location is split up into (num_partners)-subregions\n                                            where each voxel points to the target location for which this subregion\n                                            is the closest.\n        \"\"\"\n\n        self.array_to_src_trg_points = src_and_trg_points\n        self.voxel_sizes = voxel_sizes\n        self.array_keys_to_stayinside_array_keys = stayinside_array_keys\n        self.pad_for_partners = pad_for_partners\n        self.radius_phys = radius_phys\n        self.partner_criterion = partner_criterion\n\n    def setup(self):\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            for points_key in [src_points_key, trg_points_key]:\n                assert (\n                    points_key in self.spec\n                ), \"Asked for {} in AddVectorMap from {}, where {} is not provided.\".format(\n                    array_key, points_key, points_key\n                )\n            neg_pad_for_partners = Coordinate(\n                (self.pad_for_partners * np.asarray([-1])).tolist()\n            )\n            self.provides(\n                array_key,\n                ArraySpec(\n                    roi=self.spec[src_points_key].roi.grow(\n                        neg_pad_for_partners, neg_pad_for_partners\n                    ),\n                    voxel_size=self.voxel_sizes[array_key],\n                    interpolatable=False,\n                    dtype=np.float32,\n                ),\n            )\n\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            if array_key in request:\n                # increase or set request for points to be array roi + padding for partners outside roi for target points\n                deps[src_points_key] = GraphSpec(request[array_key].roi)\n                padded_roi = request[array_key].roi.grow(\n                    (self.pad_for_partners), (self.pad_for_partners)\n                )\n                deps[trg_points_key] = GraphSpec(padded_roi)\n\n        for (\n            array_key,\n            stayinside_array_key,\n        ) in self.array_keys_to_stayinside_array_keys.items():\n            if array_key in request:\n                deps[stayinside_array_key] = copy.deepcopy(request[array_key])\n\n        return deps\n\n    def process(self, batch, request):\n        # create vector map and add it to batch\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            if array_key in request:\n                vector_map = self.__get_vector_map(\n                    batch=batch, request=request, vector_map_array_key=array_key\n                )\n                spec = self.spec[array_key].copy()\n                spec.roi = request[array_key].roi\n                batch.arrays[array_key] = Array(data=vector_map, spec=spec)\n\n    def __get_vector_map(self, batch, request, vector_map_array_key):\n        src_points_key, trg_points_key = self.array_to_src_trg_points[\n            vector_map_array_key\n        ]\n        dim_vectors = len(request[vector_map_array_key].roi.shape)\n        voxel_size_vm = self.voxel_sizes[vector_map_array_key]\n        offset_vector_map_phys = request[vector_map_array_key].roi.offset\n        vector_map_total = np.zeros(\n            (dim_vectors,) + (request[vector_map_array_key].roi.shape // voxel_size_vm),\n            dtype=np.float32,\n        )\n\n        if batch.graphs[src_points_key].num_vertices() == 0:\n            return vector_map_total\n\n        for node in batch.graphs[src_points_key].nodes:\n            if request[vector_map_array_key].roi.contains(Coordinate(node.location)):\n                # get all partner locations which should be considered\n                relevant_partner_loc = self.__get_relevant_partner_locations(\n                    batch, node, trg_points_key\n                )\n                if len(relevant_partner_loc) > 0:\n                    # get locations where to set vectors around source location\n                    # (look only at region close to src location (to avoid np.nonzero() over entire array))\n                    mask = self.__get_mask(\n                        batch, request, vector_map_array_key, node.location\n                    )\n                    offset_vx_considered_mask = [\n                        (\n                            (\n                                node.location[dim]\n                                - self.radius_phys\n                                - offset_vector_map_phys[dim]\n                            )\n                            // voxel_size_vm[dim]\n                        )\n                        for dim in range(dim_vectors)\n                    ]\n                    clipped_offset_vx_considered_mask = np.clip(\n                        offset_vx_considered_mask, a_min=0, a_max=np.inf\n                    )\n                    slices = tuple(\n                        slice(\n                            int(np.max((0, offset_vx_considered_mask[dim]))),\n                            int(\n                                np.min(\n                                    (\n                                        offset_vx_considered_mask[dim]\n                                        + (2 * self.radius_phys // voxel_size_vm[dim]),\n                                        ((mask.shape[dim])),\n                                    )\n                                )\n                            ),\n                        )\n                        for dim in range(dim_vectors)\n                    )\n                    considered_region_mask = mask[slices]\n                    locations_to_fill_vx = np.reshape(\n                        np.nonzero(considered_region_mask), [dim_vectors, -1]\n                    ).T\n                    locations_to_fill_abs_phys = (\n                        (\n                            (locations_to_fill_vx + clipped_offset_vx_considered_mask)\n                            * voxel_size_vm\n                        )\n                        + offset_vector_map_phys\n                    ).tolist()\n\n                    # check for target node with largest distance first and add vector pointing to it to vector_map_total\n                    num_src_vectors_per_trg_loc = len(\n                        locations_to_fill_abs_phys\n                    ) // len(relevant_partner_loc)\n                    if num_src_vectors_per_trg_loc > 0:\n                        dist_to_locs = {}\n                        for phys_loc in relevant_partner_loc:\n                            dist_to_locs[\n                                np.linalg.norm(node.location - phys_loc)\n                            ] = phys_loc\n                        for nr, dist in enumerate(\n                            reversed(np.sort(list(dist_to_locs.keys())))\n                        ):\n                            trg_loc_abs_phys = dist_to_locs[dist]\n                            kdtree_locs_vector_map = KDTree(locations_to_fill_abs_phys)\n                            if nr == len(relevant_partner_loc) - 1:\n                                num_src_vectors_per_trg_loc = len(\n                                    locations_to_fill_abs_phys\n                                )\n                            distances, ids = kdtree_locs_vector_map.query(\n                                trg_loc_abs_phys, k=num_src_vectors_per_trg_loc\n                            )\n\n                            try:\n                                len(ids)\n                            except TypeError:\n                                ids = [ids]\n\n                            for src_voxel_id in ids:\n                                # remove node from list which are taken as neighbors of THIS target location\n                                neighbor_loc_abs_phys = kdtree_locs_vector_map.data[\n                                    src_voxel_id\n                                ]\n                                locations_to_fill_abs_phys.remove(\n                                    neighbor_loc_abs_phys.tolist()\n                                )\n\n                                # get vector for THIS neighbor to THIS target location, get its location and place it\n                                vector = trg_loc_abs_phys - neighbor_loc_abs_phys\n                                neighbor_loc_shifted_vx = (\n                                    neighbor_loc_abs_phys - offset_vector_map_phys\n                                ) // voxel_size_vm\n                                for dim in range(dim_vectors):\n                                    vector_map_total[dim][\n                                        tuple([int(l)] for l in neighbor_loc_shifted_vx)\n                                    ] = vector[dim]\n        return vector_map_total\n\n    def __get_relevant_partner_locations(self, batch, node, trg_points_key):\n        # criterions: 'min_distance' or 'all'\n\n        # get all partner locations\n        all_partners_locations = []\n        for partner_id in node.attrs[\"partner_ids\"]:\n            if batch.graphs[trg_points_key].contains(partner_id):\n                all_partners_locations.append(\n                    batch.graphs[trg_points_key].node(partner_id).location\n                )\n\n        # if only one partner location, return this one for any given criterion\n        if len(all_partners_locations) <= 1:\n            return all_partners_locations\n\n        # return all partner locations\n        elif self.partner_criterion == \"all\":\n            return all_partners_locations\n\n        # return partner with minimal euclidean distance to src_location\n        elif self.partner_criterion == \"min_distance\":\n            min_distance, stored_pos = np.inf, []\n            for partner_loc in all_partners_locations:\n                distance = np.linalg.norm(partner_loc - node.location)\n                if distance < min_distance:\n                    min_distance = distance.copy()\n                    stored_pos = partner_loc.copy()\n            return [stored_pos]\n\n    def __get_mask(self, batch, request, vector_map_array_key, src_location):\n        \"\"\"create binary mask encoding where to place vectors for in region around considered src_location\"\"\"\n\n        voxel_size = self.voxel_sizes[vector_map_array_key]\n\n        offset_bm_phys = request[vector_map_array_key].roi.offset\n        shape_bm_array_vx = request[vector_map_array_key].roi.shape // voxel_size\n        binary_map = np.zeros(shape_bm_array_vx, dtype=\"uint8\")\n\n        if self.array_keys_to_stayinside_array_keys is None:\n            mask = np.ones_like(binary_map)\n        else:\n            stayinside_array_key = self.array_keys_to_stayinside_array_keys[\n                vector_map_array_key\n            ]\n            mask = batch.arrays[stayinside_array_key].data\n\n        if mask.shape > binary_map.shape:\n            # assumption: binary map is centered in the mask array\n            padding = (np.asarray(mask.shape) - np.asarray(binary_map.shape)) / 2.0\n            slices = [slice(np.floor(pad), -np.ceil(pad)) for pad in padding]\n            mask = mask[slices]\n\n        binary_map_total = np.zeros_like(binary_map)\n\n        shifted_loc = src_location - np.asarray(offset_bm_phys)\n        shifted_loc = shifted_loc.astype(np.int32) // voxel_size\n        object_id = mask[tuple([loc] for loc in shifted_loc)][\n            0\n        ]  # 0 index, otherwise numpy array with single number\n\n        binary_map[tuple([loc] for loc in shifted_loc)] = 1\n        binary_map = enlarge_binary_map(\n            binary_map, radius=self.radius_phys, voxel_size=voxel_size\n        )\n        binary_map[mask != object_id] = 0\n        binary_map_total += binary_map\n        binary_map.fill(0)\n        binary_map_total[binary_map_total != 0] = 1\n\n        return binary_map_total",
  "def __init__(\n        self,\n        src_and_trg_points,\n        voxel_sizes,\n        radius_phys,\n        partner_criterion,\n        stayinside_array_keys=None,\n        pad_for_partners=(0, 0, 0),\n    ):\n        \"\"\"Creates a vector map of shape [dim_vector, [shape_of_array]] (e.g. [3, 50,50,50] for an array of\n            shape (50,50,50)) where every voxel which is close to a any source point location has a vector which points to\n            one of the source point location's target location.\n            Close to a point location in src_point includes all voxels which\n                1) lie within distance radius_phys of the considered src point location\n                2) (if stayinside_array_keys is not None), lie within the same segment as the src location in the\n                    mask provided in stayinside_array_keys.\n            The partner_criterion decides to which target location of the source point location that the vector of a\n            voxel points (the different criterions are described below).\n\n        Args:\n            src_and_trg_points (dict):      Dictionary from :class:``ArrayKey`` of the vector map to be created\n                                            to a tuple (:class:``GraphKey`` of the source points, :class:``GraphKey``\n                                            of the target points) which define the source and target points.\n            voxel_sizes (dict):             Dictionary from\n                                            :class:``ArrayKey`` of the vector\n                                            map to be created to a\n                                            :class:`Coordinate` for the voxel\n                                            size of the array.\n            stayinside_array_keys (dict):  Dictionary from :class:``ArrayKey`` of the vector map to be created to\n                                            :class:``ArrayKey`` of the stayinside_array.\n                                            The stayinside_array is assumed to contain discrete objects labeled with\n                                            different object ids. The object id at the specific source location is used\n                                            to restrict the region where vectors are created around a source location.\n                                            Voxels that are located outside of this object are set to zero.\n                                            If stayinside_array_keys is None, all the voxels within distance\n                                            radius_phys to the source location receive a vector.\n            pad_for_partners (tuple):       n-dim tuple which defines padding of trg_points request in all dimensions\n                                            (in world units).\n                                            This might be used s.t. also partner locations which lie within the padded\n                                            region, hence slightly outside of the vector map's roi, are considered.\n            radius_phys (int):              Radius (in world units) to restrict region where vectors are created around\n                                            a source location.\n            partner_criterion(str):         'min_distance' or 'all'\n                                            'min_distance': the vectors of all the voxels around a source location\n                                            point to the same target location, namely the location which has the\n                                            minimal distance to the considered source location.\n                                            'all': all partner locations of a given source location are considered.\n                                            The region around a source location is split up into (num_partners)-subregions\n                                            where each voxel points to the target location for which this subregion\n                                            is the closest.\n        \"\"\"\n\n        self.array_to_src_trg_points = src_and_trg_points\n        self.voxel_sizes = voxel_sizes\n        self.array_keys_to_stayinside_array_keys = stayinside_array_keys\n        self.pad_for_partners = pad_for_partners\n        self.radius_phys = radius_phys\n        self.partner_criterion = partner_criterion",
  "def setup(self):\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            for points_key in [src_points_key, trg_points_key]:\n                assert (\n                    points_key in self.spec\n                ), \"Asked for {} in AddVectorMap from {}, where {} is not provided.\".format(\n                    array_key, points_key, points_key\n                )\n            neg_pad_for_partners = Coordinate(\n                (self.pad_for_partners * np.asarray([-1])).tolist()\n            )\n            self.provides(\n                array_key,\n                ArraySpec(\n                    roi=self.spec[src_points_key].roi.grow(\n                        neg_pad_for_partners, neg_pad_for_partners\n                    ),\n                    voxel_size=self.voxel_sizes[array_key],\n                    interpolatable=False,\n                    dtype=np.float32,\n                ),\n            )\n\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            if array_key in request:\n                # increase or set request for points to be array roi + padding for partners outside roi for target points\n                deps[src_points_key] = GraphSpec(request[array_key].roi)\n                padded_roi = request[array_key].roi.grow(\n                    (self.pad_for_partners), (self.pad_for_partners)\n                )\n                deps[trg_points_key] = GraphSpec(padded_roi)\n\n        for (\n            array_key,\n            stayinside_array_key,\n        ) in self.array_keys_to_stayinside_array_keys.items():\n            if array_key in request:\n                deps[stayinside_array_key] = copy.deepcopy(request[array_key])\n\n        return deps",
  "def process(self, batch, request):\n        # create vector map and add it to batch\n        for (\n            array_key,\n            (src_points_key, trg_points_key),\n        ) in self.array_to_src_trg_points.items():\n            if array_key in request:\n                vector_map = self.__get_vector_map(\n                    batch=batch, request=request, vector_map_array_key=array_key\n                )\n                spec = self.spec[array_key].copy()\n                spec.roi = request[array_key].roi\n                batch.arrays[array_key] = Array(data=vector_map, spec=spec)",
  "def __get_vector_map(self, batch, request, vector_map_array_key):\n        src_points_key, trg_points_key = self.array_to_src_trg_points[\n            vector_map_array_key\n        ]\n        dim_vectors = len(request[vector_map_array_key].roi.shape)\n        voxel_size_vm = self.voxel_sizes[vector_map_array_key]\n        offset_vector_map_phys = request[vector_map_array_key].roi.offset\n        vector_map_total = np.zeros(\n            (dim_vectors,) + (request[vector_map_array_key].roi.shape // voxel_size_vm),\n            dtype=np.float32,\n        )\n\n        if batch.graphs[src_points_key].num_vertices() == 0:\n            return vector_map_total\n\n        for node in batch.graphs[src_points_key].nodes:\n            if request[vector_map_array_key].roi.contains(Coordinate(node.location)):\n                # get all partner locations which should be considered\n                relevant_partner_loc = self.__get_relevant_partner_locations(\n                    batch, node, trg_points_key\n                )\n                if len(relevant_partner_loc) > 0:\n                    # get locations where to set vectors around source location\n                    # (look only at region close to src location (to avoid np.nonzero() over entire array))\n                    mask = self.__get_mask(\n                        batch, request, vector_map_array_key, node.location\n                    )\n                    offset_vx_considered_mask = [\n                        (\n                            (\n                                node.location[dim]\n                                - self.radius_phys\n                                - offset_vector_map_phys[dim]\n                            )\n                            // voxel_size_vm[dim]\n                        )\n                        for dim in range(dim_vectors)\n                    ]\n                    clipped_offset_vx_considered_mask = np.clip(\n                        offset_vx_considered_mask, a_min=0, a_max=np.inf\n                    )\n                    slices = tuple(\n                        slice(\n                            int(np.max((0, offset_vx_considered_mask[dim]))),\n                            int(\n                                np.min(\n                                    (\n                                        offset_vx_considered_mask[dim]\n                                        + (2 * self.radius_phys // voxel_size_vm[dim]),\n                                        ((mask.shape[dim])),\n                                    )\n                                )\n                            ),\n                        )\n                        for dim in range(dim_vectors)\n                    )\n                    considered_region_mask = mask[slices]\n                    locations_to_fill_vx = np.reshape(\n                        np.nonzero(considered_region_mask), [dim_vectors, -1]\n                    ).T\n                    locations_to_fill_abs_phys = (\n                        (\n                            (locations_to_fill_vx + clipped_offset_vx_considered_mask)\n                            * voxel_size_vm\n                        )\n                        + offset_vector_map_phys\n                    ).tolist()\n\n                    # check for target node with largest distance first and add vector pointing to it to vector_map_total\n                    num_src_vectors_per_trg_loc = len(\n                        locations_to_fill_abs_phys\n                    ) // len(relevant_partner_loc)\n                    if num_src_vectors_per_trg_loc > 0:\n                        dist_to_locs = {}\n                        for phys_loc in relevant_partner_loc:\n                            dist_to_locs[\n                                np.linalg.norm(node.location - phys_loc)\n                            ] = phys_loc\n                        for nr, dist in enumerate(\n                            reversed(np.sort(list(dist_to_locs.keys())))\n                        ):\n                            trg_loc_abs_phys = dist_to_locs[dist]\n                            kdtree_locs_vector_map = KDTree(locations_to_fill_abs_phys)\n                            if nr == len(relevant_partner_loc) - 1:\n                                num_src_vectors_per_trg_loc = len(\n                                    locations_to_fill_abs_phys\n                                )\n                            distances, ids = kdtree_locs_vector_map.query(\n                                trg_loc_abs_phys, k=num_src_vectors_per_trg_loc\n                            )\n\n                            try:\n                                len(ids)\n                            except TypeError:\n                                ids = [ids]\n\n                            for src_voxel_id in ids:\n                                # remove node from list which are taken as neighbors of THIS target location\n                                neighbor_loc_abs_phys = kdtree_locs_vector_map.data[\n                                    src_voxel_id\n                                ]\n                                locations_to_fill_abs_phys.remove(\n                                    neighbor_loc_abs_phys.tolist()\n                                )\n\n                                # get vector for THIS neighbor to THIS target location, get its location and place it\n                                vector = trg_loc_abs_phys - neighbor_loc_abs_phys\n                                neighbor_loc_shifted_vx = (\n                                    neighbor_loc_abs_phys - offset_vector_map_phys\n                                ) // voxel_size_vm\n                                for dim in range(dim_vectors):\n                                    vector_map_total[dim][\n                                        tuple([int(l)] for l in neighbor_loc_shifted_vx)\n                                    ] = vector[dim]\n        return vector_map_total",
  "def __get_relevant_partner_locations(self, batch, node, trg_points_key):\n        # criterions: 'min_distance' or 'all'\n\n        # get all partner locations\n        all_partners_locations = []\n        for partner_id in node.attrs[\"partner_ids\"]:\n            if batch.graphs[trg_points_key].contains(partner_id):\n                all_partners_locations.append(\n                    batch.graphs[trg_points_key].node(partner_id).location\n                )\n\n        # if only one partner location, return this one for any given criterion\n        if len(all_partners_locations) <= 1:\n            return all_partners_locations\n\n        # return all partner locations\n        elif self.partner_criterion == \"all\":\n            return all_partners_locations\n\n        # return partner with minimal euclidean distance to src_location\n        elif self.partner_criterion == \"min_distance\":\n            min_distance, stored_pos = np.inf, []\n            for partner_loc in all_partners_locations:\n                distance = np.linalg.norm(partner_loc - node.location)\n                if distance < min_distance:\n                    min_distance = distance.copy()\n                    stored_pos = partner_loc.copy()\n            return [stored_pos]",
  "def __get_mask(self, batch, request, vector_map_array_key, src_location):\n        \"\"\"create binary mask encoding where to place vectors for in region around considered src_location\"\"\"\n\n        voxel_size = self.voxel_sizes[vector_map_array_key]\n\n        offset_bm_phys = request[vector_map_array_key].roi.offset\n        shape_bm_array_vx = request[vector_map_array_key].roi.shape // voxel_size\n        binary_map = np.zeros(shape_bm_array_vx, dtype=\"uint8\")\n\n        if self.array_keys_to_stayinside_array_keys is None:\n            mask = np.ones_like(binary_map)\n        else:\n            stayinside_array_key = self.array_keys_to_stayinside_array_keys[\n                vector_map_array_key\n            ]\n            mask = batch.arrays[stayinside_array_key].data\n\n        if mask.shape > binary_map.shape:\n            # assumption: binary map is centered in the mask array\n            padding = (np.asarray(mask.shape) - np.asarray(binary_map.shape)) / 2.0\n            slices = [slice(np.floor(pad), -np.ceil(pad)) for pad in padding]\n            mask = mask[slices]\n\n        binary_map_total = np.zeros_like(binary_map)\n\n        shifted_loc = src_location - np.asarray(offset_bm_phys)\n        shifted_loc = shifted_loc.astype(np.int32) // voxel_size\n        object_id = mask[tuple([loc] for loc in shifted_loc)][\n            0\n        ]  # 0 index, otherwise numpy array with single number\n\n        binary_map[tuple([loc] for loc in shifted_loc)] = 1\n        binary_map = enlarge_binary_map(\n            binary_map, radius=self.radius_phys, voxel_size=voxel_size\n        )\n        binary_map[mask != object_id] = 0\n        binary_map_total += binary_map\n        binary_map.fill(0)\n        binary_map_total[binary_map_total != 0] = 1\n\n        return binary_map_total",
  "class DeformAugment(BatchFilter):\n    \"\"\"Elasticly deform a batch. Requests larger batches upstream to avoid data\n    loss due to rotation and jitter.\n\n    Args:\n\n        control_point_spacing (``tuple`` of ``int``):\n\n            Distance between control points for the elastic deformation, in\n            physical units per dimension.\n\n        jitter_sigma (``tuple`` of ``float``):\n\n            Standard deviation of control point jitter distribution, in physical units\n            per dimension.\n\n        scale_interval (``tuple`` of two ``floats``):\n\n            Interval to randomly sample scale factors from.\n\n        subsample (``int``):\n\n            Instead of creating an elastic transformation on the full\n            resolution, create one subsampled by the given factor, and linearly\n            interpolate to obtain the full resolution transformation. This can\n            significantly speed up this node, at the expense of having visible\n            piecewise linear deformations for large factors. Usually, a factor\n            of 4 can savely by used without noticable changes. However, the\n            default is 1 (i.e., no subsampling).\n\n        spatial_dims (``int``):\n\n            The number of spatial dimensions in arrays. Spatial dimensions are\n            assumed to be the last ones and cannot be more than 3 (default).\n            Set this value here to avoid treating channels as spacial\n            dimension. If, for example, your array is indexed as ``(c,y,x)``\n            (2D plus channels), you would want to set ``spatial_dims=2`` to\n            perform the elastic deformation only on x and y.\n\n        use_fast_points_transform (``bool``):\n\n            By solving for all of your points simultaneously with the following\n            3 step proceedure:\n            1) Rasterize nodes into numpy array\n            2) Apply elastic transform to array\n            3) Read out nodes via center of mass of transformed points\n            You can gain substantial speed up as opposed to calculating the\n            elastic transform for each point individually. However this may\n            lead to nodes being lost during the transform.\n\n        recompute_missing_points (``bool``):\n\n            Whether or not to compute the elastic transform node wise for nodes\n            that were lossed during the fast elastic transform process.\n    \"\"\"\n\n    def __init__(\n        self,\n        control_point_spacing: Coordinate,\n        jitter_sigma: Coordinate,\n        scale_interval=(1.0, 1.0),\n        rotate: bool = True,\n        subsample=1,\n        spatial_dims=3,\n        use_fast_points_transform=False,\n        recompute_missing_points=True,\n        transform_key: ArrayKey = None,\n        graph_raster_voxel_size: Coordinate = None,\n    ):\n        self.control_point_spacing = Coordinate(control_point_spacing)\n        self.jitter_sigma = Coordinate(jitter_sigma)\n        self.scale_min = scale_interval[0]\n        self.scale_max = scale_interval[1]\n        self.rotate = rotate\n        self.subsample = subsample\n        self.spatial_dims = spatial_dims\n        self.use_fast_points_transform = use_fast_points_transform\n        self.recompute_missing_points = recompute_missing_points\n        self.transform_key = transform_key\n        self.graph_raster_voxel_size = Coordinate(graph_raster_voxel_size)\n        assert (\n            self.control_point_spacing.dims\n            == self.jitter_sigma.dims\n            == self.graph_raster_voxel_size.dims\n        )\n\n    def setup(self):\n        if self.transform_key is not None:\n            upstream_roi = self.spec.get_total_roi()\n            upstream_roi = Roi(\n                upstream_roi.offset[-self.spatial_dims :],\n                upstream_roi.shape[-self.spatial_dims :],\n            ).snap_to_grid(self.control_point_spacing, mode=\"shrink\")\n            spec = ArraySpec(\n                roi=upstream_roi,\n                voxel_size=self.control_point_spacing,\n                interpolatable=True,\n            )\n\n            self.provides(self.transform_key, spec)\n\n    def prepare(self, request):\n\n        # get the total ROI of all requests\n        total_roi = request.get_total_roi()\n        logger.debug(\"total ROI is %s\" % total_roi)\n\n        # First, get the total ROI of the request in spatial dimensions only.\n        # Channels and time don't matter. This is our master ROI.\n\n        # get master ROI\n        master_roi = Roi(\n            total_roi.begin[-self.spatial_dims :],\n            total_roi.shape[-self.spatial_dims :],\n        )\n        self.spatial_dims = master_roi.dims\n        logger.debug(\"master ROI is %s\" % master_roi)\n\n        # make sure the master ROI aligns with the control point spacing\n        master_roi_snapped = master_roi.snap_to_grid(\n            self.control_point_spacing, mode=\"grow\"\n        )\n        logger.debug(\n            \"master ROI aligned with control points is %s\" % master_roi_snapped\n        )\n\n        # grow by 1 control point spacing\n        master_roi_snapped = master_roi_snapped.grow(\n            self.control_point_spacing, self.control_point_spacing\n        )\n\n        # get master roi in control point spacing\n        master_roi_sampled = master_roi_snapped / self.control_point_spacing\n        logger.debug(\"master ROI in control point spacing is %s\" % master_roi_sampled)\n\n        # Second, create a master transformation. This is a transformation that\n        # covers all voxels of the all requested ROIs. The master transformation\n        # is zero-based, all transformations are relative to the origin of master_roi_snapped\n        self.master_transformation_spec = ArraySpec(\n            master_roi_snapped, self.control_point_spacing, interpolatable=True\n        )\n        (\n            self.master_transformation,\n            self.local_transformation,\n        ) = self.__create_transformation(self.master_transformation_spec)\n\n        # Third, sample the master transformation for each of the\n        # smaller requested ROIs at their respective voxel resolution.\n        # crop the parts corresponding to the requested ROIs\n        self.transformations = {}\n        deps = BatchRequest()\n        for key, spec in request.items():\n            if key == self.transform_key:\n                continue\n            spec = spec.copy()\n\n            if spec.roi is None:\n                continue\n\n            # get target roi and target spacing (voxel size for arrays or just control point\n            # spacing for graphs)\n            target_roi = Roi(\n                spec.roi.begin[-self.spatial_dims :],\n                spec.roi.shape[-self.spatial_dims :],\n            )\n\n            # get voxel size of arrays or use graph_raster_voxel_size for graphs\n            if isinstance(key, ArrayKey):\n                voxel_size = Coordinate(self.spec[key].voxel_size)\n            else:\n                # must select voxel size for the graph spec because otherwise we would\n                # interpolate the transformation onto a spacing of 1 which may be\n                # way too large\n                voxel_size = self.graph_raster_voxel_size\n\n                # grow target_roi by 1 voxel, this allows us catch nodes that project\n                # outside our bounds\n                target_roi = target_roi.grow(voxel_size, voxel_size)\n                assert (\n                    voxel_size is not None\n                ), \"Please provide a graph_raster_voxel_size when deforming graphs\"\n\n            # use only spatial dims for transformations\n            voxel_size = Coordinate(voxel_size[-self.spatial_dims :])\n            target_spatial_roi = Roi(\n                target_roi.offset[-self.spatial_dims :],\n                target_roi.shape[-self.spatial_dims :],\n            )\n            transform_spec = ArraySpec(\n                target_spatial_roi.snap_to_grid(voxel_size), voxel_size\n            )\n\n            # we save transformations that have been sampled for specific ROI's and voxel sizes,\n            # no need to recompute. This can save time if you are requesting multiple arrays of\n            # the same voxel size and shape\n            if (\n                target_spatial_roi.offset,\n                target_spatial_roi.shape,\n                voxel_size,\n            ) in self.transformations:\n                transformation = self.transformations[\n                    (target_spatial_roi.offset, target_spatial_roi.shape, voxel_size)\n                ]\n            else:\n                # sample the master transformation at the voxel spacing of each array\n                transformation = self.__sample_transform(\n                    self.master_transformation, transform_spec\n                )\n                self.transformations[\n                    (target_spatial_roi.offset, target_spatial_roi.shape, voxel_size)\n                ] = transformation\n\n            # get ROI of all control points necessary to perform transformation\n            #\n            # for that we follow the same transformations to get from the\n            # request ROI to the target ROI in master ROI in control points, just in\n            # reverse\n            source_roi = self.__get_source_roi(transformation)\n\n            # update upstream request\n            spec.roi = Roi(\n                spec.roi.begin[: -self.spatial_dims]\n                + source_roi.begin[-self.spatial_dims :],\n                spec.roi.shape[: -self.spatial_dims]\n                + source_roi.shape[-self.spatial_dims :],\n            )\n\n            deps[key] = spec\n\n            logger.debug(\"upstream request roi for %s = %s\" % (key, spec.roi))\n\n        return deps\n\n    def process(self, batch, request):\n        out_batch = Batch()\n        for array_key, array in batch.arrays.items():\n            request_roi = Roi(\n                request[array_key].roi.offset[-self.spatial_dims :],\n                request[array_key].roi.shape[-self.spatial_dims :],\n            )\n            voxel_size = Coordinate(array.spec.voxel_size[-self.spatial_dims :])\n            assert (\n                request_roi.offset,\n                request_roi.shape,\n                voxel_size,\n            ) in self.transformations, f\"{(request_roi.offset, request_roi.shape, voxel_size)} not in {list(self.transformations.keys())}\"\n\n            # reshape array data into (channels,) + spatial dims\n            transformed_array = self.__apply_transform(\n                array,\n                self.transformations[\n                    (request_roi.offset, request_roi.shape, voxel_size)\n                ],\n            )\n\n            out_batch[array_key] = transformed_array\n\n        for graph_key, graph in batch.graphs.items():\n            target_roi = Roi(\n                request[graph_key].roi.offset[-self.spatial_dims :],\n                request[graph_key].roi.shape[-self.spatial_dims :],\n            )\n            transform_roi = target_roi.grow(\n                self.graph_raster_voxel_size, self.graph_raster_voxel_size\n            )\n            source_roi = Roi(\n                graph.spec.roi.offset[-self.spatial_dims :],\n                graph.spec.roi.shape[-self.spatial_dims :],\n            )\n            nodes = list(graph.nodes)\n\n            if self.use_fast_points_transform:\n                missed_nodes = self.__fast_point_projection(\n                    self.transformations[\n                        transform_roi.offset,\n                        transform_roi.shape,\n                        self.graph_raster_voxel_size,\n                    ],\n                    nodes,\n                    source_roi,\n                    target_roi=transform_roi,\n                )\n                if not self.recompute_missing_points:\n                    for node in set(missed_nodes):\n                        graph.remove_node(node, retain_connectivity=True)\n                    missed_nodes = []\n            else:\n                missed_nodes = nodes\n\n            for node in missed_nodes:\n                # logger.debug(\"projecting %s\", node.location)\n\n                # get location relative to beginning of upstream ROI\n                location = node.location\n\n                # get spatial coordinates of node\n                location_spatial = location[-self.spatial_dims :]\n\n                # get projected location in transformation data space, this\n                # yields voxel coordinates relative to target ROI\n                projected = self.__project(\n                    self.transformations[\n                        transform_roi.offset,\n                        transform_roi.shape,\n                        self.graph_raster_voxel_size,\n                    ],\n                    location_spatial,\n                )\n\n                logger.debug(\"projected: %s\", projected)\n\n                # update spatial coordinates of node location\n                node.location[-self.spatial_dims :] = projected\n\n                logger.debug(\"final location: %s\", node.location)\n\n            out_batch[graph_key] = graph.crop(target_roi)\n\n        if self.transform_key is not None:\n            out_batch[self.transform_key] = self.local_transformation\n\n        return out_batch\n\n    def __apply_transform(self, array: Array, transformation: Array) -> Array:\n        input_shape = array.data.shape\n        output_shape = transformation.data.shape\n        channel_shape = input_shape[: -self.spatial_dims]\n        data = array.data.reshape((-1,) + input_shape[-self.spatial_dims :])\n\n        offset = array.spec.roi.offset[-self.spatial_dims :]\n        voxel_size = array.spec.voxel_size[-self.spatial_dims :]\n\n        # apply transformation on each channel\n        transform = transformation.data.copy()\n        transform -= np.array(offset).reshape((-1,) + (1,) * self.spatial_dims)\n        transform /= np.array(voxel_size).reshape((-1,) + (1,) * self.spatial_dims)\n\n        data = np.array(\n            [\n                apply_transformation(\n                    data[c],\n                    transform,\n                    interpolate=array.spec.interpolatable,\n                )\n                for c in range(data.shape[0])\n            ]\n        )\n        spec = array.spec.copy()\n        spec.roi = Roi(\n            spec.roi.offset[: -self.spatial_dims] + transformation.spec.roi.offset[:],\n            spec.roi.shape[: -self.spatial_dims] + transformation.spec.roi.shape[:],\n        )\n\n        return Array(\n            data.reshape(channel_shape + output_shape[-self.spatial_dims :]), spec\n        )\n\n    def __sample_transform(\n        self,\n        transformation: Array,\n        output_spec: ArraySpec,\n        interpolate_order=1,\n    ) -> Array:\n        if output_spec.voxel_size == transformation.spec.voxel_size:\n            # if voxel_size == control_point_spacing we can simply slice into the master roi\n            relative_output_roi = (\n                output_spec.roi - transformation.spec.roi.offset\n            ).snap_to_grid(output_spec.voxel_size) / output_spec.voxel_size\n            sampled = np.copy(\n                transformation.data[\n                    (slice(None),) + relative_output_roi.get_bounding_box()\n                ]\n            )\n            return Array(\n                sampled,\n                ArraySpec(\n                    output_spec.roi.snap_to_grid(output_spec.voxel_size),\n                    output_spec.voxel_size,\n                    interpolatable=True,\n                ),\n            )\n\n        dims = len(output_spec.voxel_size)\n        output_shape = output_spec.roi.shape / output_spec.voxel_size\n        offset = np.array(\n            [\n                o / s\n                for o, s in zip(\n                    output_spec.roi.offset - transformation.spec.roi.offset,\n                    transformation.spec.voxel_size,\n                )\n            ]\n        )\n        step = np.array(\n            [\n                o / i\n                for o, i in zip(output_spec.voxel_size, transformation.spec.voxel_size)\n            ]\n        )\n        coordinates = np.meshgrid(\n            range(dims),\n            *[\n                np.linspace(o, (shape - 1) * step + o, shape)\n                for o, shape, step in zip(offset, output_shape, step)\n            ],\n            indexing=\"ij\",\n        )\n        coordinates = np.stack(coordinates)\n\n        sampled = ndimage.map_coordinates(\n            transformation.data,\n            coordinates=coordinates,\n            order=3,\n            mode=\"nearest\",\n        )\n        return Array(sampled, ArraySpec(output_spec.roi, output_spec.voxel_size))\n\n    def __create_transformation(self, target_spec: ArraySpec):\n        scale = self.scale_min + random.random() * (self.scale_max - self.scale_min)\n\n        target_shape = target_spec.roi.shape / target_spec.voxel_size\n\n        global_transformation = create_identity_transformation(\n            target_shape,\n            subsample=self.subsample,\n            scale=scale,\n        )\n        local_transformation = np.zeros_like(global_transformation)\n\n        if sum(self.jitter_sigma) > 0:\n            el_transformation = create_elastic_transformation(\n                target_shape,\n                1,\n                np.array(self.jitter_sigma) / self.control_point_spacing,\n                subsample=self.subsample,\n            )\n\n            local_transformation += el_transformation\n\n        if self.rotate:\n            assert min(target_spec.voxel_size) == max(\n                target_spec.voxel_size\n            ), \"Only isotropic control point spacing supported when rotating\"\n            if self.spatial_dims == 2:\n                rot_transformation = create_rotation_transformation(\n                    target_shape,\n                    random.random() * math.pi,\n                )\n            else:\n                angle = Rotation.random()\n                rot_transformation = create_3D_rotation_transformation(\n                    target_shape, angle\n                )\n\n            local_transformation += rot_transformation\n\n        if self.subsample > 1:\n            local_transformation = upscale_transformation(\n                local_transformation, target_shape\n            )\n\n        # transform into world units\n        global_transformation *= np.array(target_spec.voxel_size).reshape(\n            (len(target_spec.voxel_size),) + (1,) * self.spatial_dims\n        )\n        global_transformation += np.array(target_spec.roi.offset).reshape(\n            (len(target_spec.roi.offset),) + (1,) * self.spatial_dims\n        )\n\n        local_transformation *= np.array(target_spec.voxel_size).reshape(\n            (len(target_spec.voxel_size),) + (1,) * self.spatial_dims\n        )\n\n        return (\n            Array(global_transformation + local_transformation, target_spec),\n            Array(local_transformation, target_spec),\n        )\n\n    def __fast_point_projection(self, transformation, nodes, source_roi, target_roi):\n        if len(nodes) < 1:\n            return []\n        # rasterize the points into an array\n        ids, locs = zip(\n            *[\n                (\n                    node.id,\n                    (\n                        np.floor(node.location[-self.spatial_dims :]).astype(int)\n                        - source_roi.begin\n                    )\n                    // self.graph_raster_voxel_size,\n                )\n                for node in nodes\n                if source_roi.contains(node.location)\n            ]\n        )\n        ids, locs = np.array(ids), tuple(zip(*locs))\n        points_array = np.zeros(\n            source_roi.shape / self.graph_raster_voxel_size, dtype=np.int64\n        )\n        points_array[locs] = ids\n\n        # reshape array data into (channels,) + spatial dims\n        shape = points_array.shape\n        data = points_array.reshape((-1,) + shape[-self.spatial_dims :])\n\n        array = Array(\n            data,\n            ArraySpec(\n                Roi(\n                    source_roi.begin[-self.spatial_dims :],\n                    Coordinate(shape) * self.graph_raster_voxel_size,\n                ),\n                self.graph_raster_voxel_size,\n            ),\n        )\n        transformed = self.__apply_transform(array, transformation)\n\n        data = transformed.data\n        missing_points = []\n        projected_locs = ndimage.center_of_mass(data > 0, data, ids)\n        projected_locs = [\n            (np.array(loc[-self.spatial_dims :]) + 0.5) * self.graph_raster_voxel_size\n            + transformation.spec.roi.begin\n            for loc in projected_locs\n        ]\n        node_dict = {node.id: node for node in nodes}\n        for point_id, proj_loc in zip(ids, projected_locs):\n            point = node_dict.pop(point_id)\n            if not any([np.isnan(x) for x in proj_loc]):\n                assert (\n                    len(proj_loc) == self.spatial_dims\n                ), \"projected location has wrong number of dimensions: {}, expected: {}\".format(\n                    len(proj_loc), self.spatial_dims\n                )\n                point.location[-self.spatial_dims :] = proj_loc\n            else:\n                missing_points.append(point)\n        for node in node_dict.values():\n            missing_points.append(node)\n        logger.debug(\n            \"{} of {} points lost in fast points projection\".format(\n                len(missing_points), len(ids)\n            )\n        )\n\n        return missing_points\n\n    def __project(self, transformation: Array, location: np.ndarray) -> np.ndarray:\n        \"\"\"Find the projection of location given by transformation. Returns None\n        if projection lies outside of transformation.\"\"\"\n\n        dims = len(location)\n\n        # subtract location from transformation\n        diff = transformation.data.copy()\n        for d in range(dims):\n            diff[d] -= location[d]\n\n        # square\n        diff2 = diff * diff\n\n        # sum\n        dist = diff2.sum(axis=0)\n\n        # find grid point closes to location\n        center_grid = Coordinate(np.unravel_index(dist.argmin(), dist.shape))\n        center_source = self.__source_at(transformation, center_grid)\n\n        logger.debug(\"projecting %s onto grid\", location)\n        logger.debug(\"grid shape: %s\", transformation.data.shape[1:])\n        logger.debug(\"grid projection: %s\", center_grid)\n        logger.debug(\"dist shape: %s\", dist.shape)\n        logger.debug(\"dist.argmin(): %s\", dist.argmin())\n        logger.debug(\"dist[argmin]: %s\", dist[center_grid])\n        logger.debug(\n            \"transform[argmin]: %s\", transformation.data[(slice(None),) + center_grid]\n        )\n        logger.debug(\"min dist: %s\", dist.min())\n        logger.debug(\"center source: %s\", center_source)\n\n        # add a half voxel step to localize each transformed point to the center of the\n        # closest voxel\n        return (\n            np.array(center_grid, dtype=np.float32) + 0.5\n        ) * transformation.spec.voxel_size + transformation.spec.roi.offset\n\n    def __source_at(self, transformation, index):\n        \"\"\"Read the source point of a transformation at index.\"\"\"\n\n        slices = (slice(None),) + tuple(slice(i, i + 1) for i in index)\n        return transformation.data[slices].flatten()\n\n    def __get_source_roi(self, transformation):\n        # this gets you the source_roi in offset space. We need to add 1 voxel\n        # to the shape to get the closed interval ROI\n\n        # get bounding box of needed data for transformation\n        bb_min = Coordinate(\n            int(math.floor(transformation.data[d].min()))\n            for d in range(transformation.spec.voxel_size.dims)\n        )\n        bb_max = Coordinate(\n            int(math.ceil(transformation.data[d].max())) + s\n            for d, s in zip(\n                range(transformation.spec.voxel_size.dims),\n                transformation.spec.voxel_size,\n            )\n        )\n\n        # create roi sufficiently large to feed transformation\n        source_roi = Roi(bb_min, bb_max - bb_min).snap_to_grid(\n            transformation.spec.voxel_size\n        )\n\n        return source_roi\n\n    def __shift_transformation(self, shift, transformation):\n        for d in range(transformation.shape[0]):\n            transformation[d] += shift[d]",
  "def __init__(\n        self,\n        control_point_spacing: Coordinate,\n        jitter_sigma: Coordinate,\n        scale_interval=(1.0, 1.0),\n        rotate: bool = True,\n        subsample=1,\n        spatial_dims=3,\n        use_fast_points_transform=False,\n        recompute_missing_points=True,\n        transform_key: ArrayKey = None,\n        graph_raster_voxel_size: Coordinate = None,\n    ):\n        self.control_point_spacing = Coordinate(control_point_spacing)\n        self.jitter_sigma = Coordinate(jitter_sigma)\n        self.scale_min = scale_interval[0]\n        self.scale_max = scale_interval[1]\n        self.rotate = rotate\n        self.subsample = subsample\n        self.spatial_dims = spatial_dims\n        self.use_fast_points_transform = use_fast_points_transform\n        self.recompute_missing_points = recompute_missing_points\n        self.transform_key = transform_key\n        self.graph_raster_voxel_size = Coordinate(graph_raster_voxel_size)\n        assert (\n            self.control_point_spacing.dims\n            == self.jitter_sigma.dims\n            == self.graph_raster_voxel_size.dims\n        )",
  "def setup(self):\n        if self.transform_key is not None:\n            upstream_roi = self.spec.get_total_roi()\n            upstream_roi = Roi(\n                upstream_roi.offset[-self.spatial_dims :],\n                upstream_roi.shape[-self.spatial_dims :],\n            ).snap_to_grid(self.control_point_spacing, mode=\"shrink\")\n            spec = ArraySpec(\n                roi=upstream_roi,\n                voxel_size=self.control_point_spacing,\n                interpolatable=True,\n            )\n\n            self.provides(self.transform_key, spec)",
  "def prepare(self, request):\n\n        # get the total ROI of all requests\n        total_roi = request.get_total_roi()\n        logger.debug(\"total ROI is %s\" % total_roi)\n\n        # First, get the total ROI of the request in spatial dimensions only.\n        # Channels and time don't matter. This is our master ROI.\n\n        # get master ROI\n        master_roi = Roi(\n            total_roi.begin[-self.spatial_dims :],\n            total_roi.shape[-self.spatial_dims :],\n        )\n        self.spatial_dims = master_roi.dims\n        logger.debug(\"master ROI is %s\" % master_roi)\n\n        # make sure the master ROI aligns with the control point spacing\n        master_roi_snapped = master_roi.snap_to_grid(\n            self.control_point_spacing, mode=\"grow\"\n        )\n        logger.debug(\n            \"master ROI aligned with control points is %s\" % master_roi_snapped\n        )\n\n        # grow by 1 control point spacing\n        master_roi_snapped = master_roi_snapped.grow(\n            self.control_point_spacing, self.control_point_spacing\n        )\n\n        # get master roi in control point spacing\n        master_roi_sampled = master_roi_snapped / self.control_point_spacing\n        logger.debug(\"master ROI in control point spacing is %s\" % master_roi_sampled)\n\n        # Second, create a master transformation. This is a transformation that\n        # covers all voxels of the all requested ROIs. The master transformation\n        # is zero-based, all transformations are relative to the origin of master_roi_snapped\n        self.master_transformation_spec = ArraySpec(\n            master_roi_snapped, self.control_point_spacing, interpolatable=True\n        )\n        (\n            self.master_transformation,\n            self.local_transformation,\n        ) = self.__create_transformation(self.master_transformation_spec)\n\n        # Third, sample the master transformation for each of the\n        # smaller requested ROIs at their respective voxel resolution.\n        # crop the parts corresponding to the requested ROIs\n        self.transformations = {}\n        deps = BatchRequest()\n        for key, spec in request.items():\n            if key == self.transform_key:\n                continue\n            spec = spec.copy()\n\n            if spec.roi is None:\n                continue\n\n            # get target roi and target spacing (voxel size for arrays or just control point\n            # spacing for graphs)\n            target_roi = Roi(\n                spec.roi.begin[-self.spatial_dims :],\n                spec.roi.shape[-self.spatial_dims :],\n            )\n\n            # get voxel size of arrays or use graph_raster_voxel_size for graphs\n            if isinstance(key, ArrayKey):\n                voxel_size = Coordinate(self.spec[key].voxel_size)\n            else:\n                # must select voxel size for the graph spec because otherwise we would\n                # interpolate the transformation onto a spacing of 1 which may be\n                # way too large\n                voxel_size = self.graph_raster_voxel_size\n\n                # grow target_roi by 1 voxel, this allows us catch nodes that project\n                # outside our bounds\n                target_roi = target_roi.grow(voxel_size, voxel_size)\n                assert (\n                    voxel_size is not None\n                ), \"Please provide a graph_raster_voxel_size when deforming graphs\"\n\n            # use only spatial dims for transformations\n            voxel_size = Coordinate(voxel_size[-self.spatial_dims :])\n            target_spatial_roi = Roi(\n                target_roi.offset[-self.spatial_dims :],\n                target_roi.shape[-self.spatial_dims :],\n            )\n            transform_spec = ArraySpec(\n                target_spatial_roi.snap_to_grid(voxel_size), voxel_size\n            )\n\n            # we save transformations that have been sampled for specific ROI's and voxel sizes,\n            # no need to recompute. This can save time if you are requesting multiple arrays of\n            # the same voxel size and shape\n            if (\n                target_spatial_roi.offset,\n                target_spatial_roi.shape,\n                voxel_size,\n            ) in self.transformations:\n                transformation = self.transformations[\n                    (target_spatial_roi.offset, target_spatial_roi.shape, voxel_size)\n                ]\n            else:\n                # sample the master transformation at the voxel spacing of each array\n                transformation = self.__sample_transform(\n                    self.master_transformation, transform_spec\n                )\n                self.transformations[\n                    (target_spatial_roi.offset, target_spatial_roi.shape, voxel_size)\n                ] = transformation\n\n            # get ROI of all control points necessary to perform transformation\n            #\n            # for that we follow the same transformations to get from the\n            # request ROI to the target ROI in master ROI in control points, just in\n            # reverse\n            source_roi = self.__get_source_roi(transformation)\n\n            # update upstream request\n            spec.roi = Roi(\n                spec.roi.begin[: -self.spatial_dims]\n                + source_roi.begin[-self.spatial_dims :],\n                spec.roi.shape[: -self.spatial_dims]\n                + source_roi.shape[-self.spatial_dims :],\n            )\n\n            deps[key] = spec\n\n            logger.debug(\"upstream request roi for %s = %s\" % (key, spec.roi))\n\n        return deps",
  "def process(self, batch, request):\n        out_batch = Batch()\n        for array_key, array in batch.arrays.items():\n            request_roi = Roi(\n                request[array_key].roi.offset[-self.spatial_dims :],\n                request[array_key].roi.shape[-self.spatial_dims :],\n            )\n            voxel_size = Coordinate(array.spec.voxel_size[-self.spatial_dims :])\n            assert (\n                request_roi.offset,\n                request_roi.shape,\n                voxel_size,\n            ) in self.transformations, f\"{(request_roi.offset, request_roi.shape, voxel_size)} not in {list(self.transformations.keys())}\"\n\n            # reshape array data into (channels,) + spatial dims\n            transformed_array = self.__apply_transform(\n                array,\n                self.transformations[\n                    (request_roi.offset, request_roi.shape, voxel_size)\n                ],\n            )\n\n            out_batch[array_key] = transformed_array\n\n        for graph_key, graph in batch.graphs.items():\n            target_roi = Roi(\n                request[graph_key].roi.offset[-self.spatial_dims :],\n                request[graph_key].roi.shape[-self.spatial_dims :],\n            )\n            transform_roi = target_roi.grow(\n                self.graph_raster_voxel_size, self.graph_raster_voxel_size\n            )\n            source_roi = Roi(\n                graph.spec.roi.offset[-self.spatial_dims :],\n                graph.spec.roi.shape[-self.spatial_dims :],\n            )\n            nodes = list(graph.nodes)\n\n            if self.use_fast_points_transform:\n                missed_nodes = self.__fast_point_projection(\n                    self.transformations[\n                        transform_roi.offset,\n                        transform_roi.shape,\n                        self.graph_raster_voxel_size,\n                    ],\n                    nodes,\n                    source_roi,\n                    target_roi=transform_roi,\n                )\n                if not self.recompute_missing_points:\n                    for node in set(missed_nodes):\n                        graph.remove_node(node, retain_connectivity=True)\n                    missed_nodes = []\n            else:\n                missed_nodes = nodes\n\n            for node in missed_nodes:\n                # logger.debug(\"projecting %s\", node.location)\n\n                # get location relative to beginning of upstream ROI\n                location = node.location\n\n                # get spatial coordinates of node\n                location_spatial = location[-self.spatial_dims :]\n\n                # get projected location in transformation data space, this\n                # yields voxel coordinates relative to target ROI\n                projected = self.__project(\n                    self.transformations[\n                        transform_roi.offset,\n                        transform_roi.shape,\n                        self.graph_raster_voxel_size,\n                    ],\n                    location_spatial,\n                )\n\n                logger.debug(\"projected: %s\", projected)\n\n                # update spatial coordinates of node location\n                node.location[-self.spatial_dims :] = projected\n\n                logger.debug(\"final location: %s\", node.location)\n\n            out_batch[graph_key] = graph.crop(target_roi)\n\n        if self.transform_key is not None:\n            out_batch[self.transform_key] = self.local_transformation\n\n        return out_batch",
  "def __apply_transform(self, array: Array, transformation: Array) -> Array:\n        input_shape = array.data.shape\n        output_shape = transformation.data.shape\n        channel_shape = input_shape[: -self.spatial_dims]\n        data = array.data.reshape((-1,) + input_shape[-self.spatial_dims :])\n\n        offset = array.spec.roi.offset[-self.spatial_dims :]\n        voxel_size = array.spec.voxel_size[-self.spatial_dims :]\n\n        # apply transformation on each channel\n        transform = transformation.data.copy()\n        transform -= np.array(offset).reshape((-1,) + (1,) * self.spatial_dims)\n        transform /= np.array(voxel_size).reshape((-1,) + (1,) * self.spatial_dims)\n\n        data = np.array(\n            [\n                apply_transformation(\n                    data[c],\n                    transform,\n                    interpolate=array.spec.interpolatable,\n                )\n                for c in range(data.shape[0])\n            ]\n        )\n        spec = array.spec.copy()\n        spec.roi = Roi(\n            spec.roi.offset[: -self.spatial_dims] + transformation.spec.roi.offset[:],\n            spec.roi.shape[: -self.spatial_dims] + transformation.spec.roi.shape[:],\n        )\n\n        return Array(\n            data.reshape(channel_shape + output_shape[-self.spatial_dims :]), spec\n        )",
  "def __sample_transform(\n        self,\n        transformation: Array,\n        output_spec: ArraySpec,\n        interpolate_order=1,\n    ) -> Array:\n        if output_spec.voxel_size == transformation.spec.voxel_size:\n            # if voxel_size == control_point_spacing we can simply slice into the master roi\n            relative_output_roi = (\n                output_spec.roi - transformation.spec.roi.offset\n            ).snap_to_grid(output_spec.voxel_size) / output_spec.voxel_size\n            sampled = np.copy(\n                transformation.data[\n                    (slice(None),) + relative_output_roi.get_bounding_box()\n                ]\n            )\n            return Array(\n                sampled,\n                ArraySpec(\n                    output_spec.roi.snap_to_grid(output_spec.voxel_size),\n                    output_spec.voxel_size,\n                    interpolatable=True,\n                ),\n            )\n\n        dims = len(output_spec.voxel_size)\n        output_shape = output_spec.roi.shape / output_spec.voxel_size\n        offset = np.array(\n            [\n                o / s\n                for o, s in zip(\n                    output_spec.roi.offset - transformation.spec.roi.offset,\n                    transformation.spec.voxel_size,\n                )\n            ]\n        )\n        step = np.array(\n            [\n                o / i\n                for o, i in zip(output_spec.voxel_size, transformation.spec.voxel_size)\n            ]\n        )\n        coordinates = np.meshgrid(\n            range(dims),\n            *[\n                np.linspace(o, (shape - 1) * step + o, shape)\n                for o, shape, step in zip(offset, output_shape, step)\n            ],\n            indexing=\"ij\",\n        )\n        coordinates = np.stack(coordinates)\n\n        sampled = ndimage.map_coordinates(\n            transformation.data,\n            coordinates=coordinates,\n            order=3,\n            mode=\"nearest\",\n        )\n        return Array(sampled, ArraySpec(output_spec.roi, output_spec.voxel_size))",
  "def __create_transformation(self, target_spec: ArraySpec):\n        scale = self.scale_min + random.random() * (self.scale_max - self.scale_min)\n\n        target_shape = target_spec.roi.shape / target_spec.voxel_size\n\n        global_transformation = create_identity_transformation(\n            target_shape,\n            subsample=self.subsample,\n            scale=scale,\n        )\n        local_transformation = np.zeros_like(global_transformation)\n\n        if sum(self.jitter_sigma) > 0:\n            el_transformation = create_elastic_transformation(\n                target_shape,\n                1,\n                np.array(self.jitter_sigma) / self.control_point_spacing,\n                subsample=self.subsample,\n            )\n\n            local_transformation += el_transformation\n\n        if self.rotate:\n            assert min(target_spec.voxel_size) == max(\n                target_spec.voxel_size\n            ), \"Only isotropic control point spacing supported when rotating\"\n            if self.spatial_dims == 2:\n                rot_transformation = create_rotation_transformation(\n                    target_shape,\n                    random.random() * math.pi,\n                )\n            else:\n                angle = Rotation.random()\n                rot_transformation = create_3D_rotation_transformation(\n                    target_shape, angle\n                )\n\n            local_transformation += rot_transformation\n\n        if self.subsample > 1:\n            local_transformation = upscale_transformation(\n                local_transformation, target_shape\n            )\n\n        # transform into world units\n        global_transformation *= np.array(target_spec.voxel_size).reshape(\n            (len(target_spec.voxel_size),) + (1,) * self.spatial_dims\n        )\n        global_transformation += np.array(target_spec.roi.offset).reshape(\n            (len(target_spec.roi.offset),) + (1,) * self.spatial_dims\n        )\n\n        local_transformation *= np.array(target_spec.voxel_size).reshape(\n            (len(target_spec.voxel_size),) + (1,) * self.spatial_dims\n        )\n\n        return (\n            Array(global_transformation + local_transformation, target_spec),\n            Array(local_transformation, target_spec),\n        )",
  "def __fast_point_projection(self, transformation, nodes, source_roi, target_roi):\n        if len(nodes) < 1:\n            return []\n        # rasterize the points into an array\n        ids, locs = zip(\n            *[\n                (\n                    node.id,\n                    (\n                        np.floor(node.location[-self.spatial_dims :]).astype(int)\n                        - source_roi.begin\n                    )\n                    // self.graph_raster_voxel_size,\n                )\n                for node in nodes\n                if source_roi.contains(node.location)\n            ]\n        )\n        ids, locs = np.array(ids), tuple(zip(*locs))\n        points_array = np.zeros(\n            source_roi.shape / self.graph_raster_voxel_size, dtype=np.int64\n        )\n        points_array[locs] = ids\n\n        # reshape array data into (channels,) + spatial dims\n        shape = points_array.shape\n        data = points_array.reshape((-1,) + shape[-self.spatial_dims :])\n\n        array = Array(\n            data,\n            ArraySpec(\n                Roi(\n                    source_roi.begin[-self.spatial_dims :],\n                    Coordinate(shape) * self.graph_raster_voxel_size,\n                ),\n                self.graph_raster_voxel_size,\n            ),\n        )\n        transformed = self.__apply_transform(array, transformation)\n\n        data = transformed.data\n        missing_points = []\n        projected_locs = ndimage.center_of_mass(data > 0, data, ids)\n        projected_locs = [\n            (np.array(loc[-self.spatial_dims :]) + 0.5) * self.graph_raster_voxel_size\n            + transformation.spec.roi.begin\n            for loc in projected_locs\n        ]\n        node_dict = {node.id: node for node in nodes}\n        for point_id, proj_loc in zip(ids, projected_locs):\n            point = node_dict.pop(point_id)\n            if not any([np.isnan(x) for x in proj_loc]):\n                assert (\n                    len(proj_loc) == self.spatial_dims\n                ), \"projected location has wrong number of dimensions: {}, expected: {}\".format(\n                    len(proj_loc), self.spatial_dims\n                )\n                point.location[-self.spatial_dims :] = proj_loc\n            else:\n                missing_points.append(point)\n        for node in node_dict.values():\n            missing_points.append(node)\n        logger.debug(\n            \"{} of {} points lost in fast points projection\".format(\n                len(missing_points), len(ids)\n            )\n        )\n\n        return missing_points",
  "def __project(self, transformation: Array, location: np.ndarray) -> np.ndarray:\n        \"\"\"Find the projection of location given by transformation. Returns None\n        if projection lies outside of transformation.\"\"\"\n\n        dims = len(location)\n\n        # subtract location from transformation\n        diff = transformation.data.copy()\n        for d in range(dims):\n            diff[d] -= location[d]\n\n        # square\n        diff2 = diff * diff\n\n        # sum\n        dist = diff2.sum(axis=0)\n\n        # find grid point closes to location\n        center_grid = Coordinate(np.unravel_index(dist.argmin(), dist.shape))\n        center_source = self.__source_at(transformation, center_grid)\n\n        logger.debug(\"projecting %s onto grid\", location)\n        logger.debug(\"grid shape: %s\", transformation.data.shape[1:])\n        logger.debug(\"grid projection: %s\", center_grid)\n        logger.debug(\"dist shape: %s\", dist.shape)\n        logger.debug(\"dist.argmin(): %s\", dist.argmin())\n        logger.debug(\"dist[argmin]: %s\", dist[center_grid])\n        logger.debug(\n            \"transform[argmin]: %s\", transformation.data[(slice(None),) + center_grid]\n        )\n        logger.debug(\"min dist: %s\", dist.min())\n        logger.debug(\"center source: %s\", center_source)\n\n        # add a half voxel step to localize each transformed point to the center of the\n        # closest voxel\n        return (\n            np.array(center_grid, dtype=np.float32) + 0.5\n        ) * transformation.spec.voxel_size + transformation.spec.roi.offset",
  "def __source_at(self, transformation, index):\n        \"\"\"Read the source point of a transformation at index.\"\"\"\n\n        slices = (slice(None),) + tuple(slice(i, i + 1) for i in index)\n        return transformation.data[slices].flatten()",
  "def __get_source_roi(self, transformation):\n        # this gets you the source_roi in offset space. We need to add 1 voxel\n        # to the shape to get the closed interval ROI\n\n        # get bounding box of needed data for transformation\n        bb_min = Coordinate(\n            int(math.floor(transformation.data[d].min()))\n            for d in range(transformation.spec.voxel_size.dims)\n        )\n        bb_max = Coordinate(\n            int(math.ceil(transformation.data[d].max())) + s\n            for d, s in zip(\n                range(transformation.spec.voxel_size.dims),\n                transformation.spec.voxel_size,\n            )\n        )\n\n        # create roi sufficiently large to feed transformation\n        source_roi = Roi(bb_min, bb_max - bb_min).snap_to_grid(\n            transformation.spec.voxel_size\n        )\n\n        return source_roi",
  "def __shift_transformation(self, shift, transformation):\n        for d in range(transformation.shape[0]):\n            transformation[d] += shift[d]",
  "class RandomProvider(BatchProvider):\n    \"\"\"Randomly selects one of the upstream providers::\n\n        (a, b, c) + RandomProvider()\n\n    will create a provider that randomly relays requests to providers ``a``,\n    ``b``, or ``c``. Array and point keys of ``a``, ``b``, and ``c`` should be\n    the same.\n\n    Args:\n        probabilities (1-D array-like, optional):\n\n            An optional list of\n            probabilities for choosing upstream providers, given in the\n            same order. Probabilities do not need to be normalized. Default\n            is ``None``, corresponding to equal probabilities.\n\n        random_provider_key (``ArrayKey``):\n\n            If provided, this node will store the index of the chosen random\n            provider in a nonspatial array.\n    \"\"\"\n\n    def __init__(self, probabilities=None, random_provider_key=None):\n        self.probabilities = probabilities\n        self.random_provider_key = random_provider_key\n\n        # automatically normalize probabilities to sum to 1\n        if self.probabilities is not None:\n            self.probabilities = [\n                float(x) / np.sum(probabilities) for x in self.probabilities\n            ]\n\n    def setup(self):\n        self.enable_placeholders()\n        assert (\n            len(self.get_upstream_providers()) > 0\n        ), \"at least one batch provider must be added to the RandomProvider\"\n        if self.probabilities is not None:\n            assert len(self.get_upstream_providers()) == len(self.probabilities), (\n                \"if probabilities are specified, they \"\n                \"need to be given for each batch \"\n                \"provider added to the RandomProvider\"\n            )\n\n        common_spec = None\n\n        # advertise outputs only if all upstream providers have them\n        for provider in self.get_upstream_providers():\n            if common_spec is None:\n                common_spec = provider.spec.copy()\n            else:\n                for key, spec in list(common_spec.items()):\n                    if key not in provider.spec:\n                        del common_spec[key]\n\n        for key, spec in common_spec.items():\n            self.provides(key, spec)\n\n        if self.random_provider_key is not None:\n            self.provides(self.random_provider_key, ArraySpec(nonspatial=True))\n\n    def provide(self, request):\n\n        if self.random_provider_key is not None:\n            del request[self.random_provider_key]\n\n        i = np.random.choice(\n            range(len(self.get_upstream_providers())), p=self.probabilities\n        )\n        provider = self.get_upstream_providers()[i]\n        batch = provider.request_batch(request)\n        if self.random_provider_key is not None:\n            batch[self.random_provider_key] = Array(\n                np.array(i), ArraySpec(nonspatial=True)\n            )\n        return batch",
  "def __init__(self, probabilities=None, random_provider_key=None):\n        self.probabilities = probabilities\n        self.random_provider_key = random_provider_key\n\n        # automatically normalize probabilities to sum to 1\n        if self.probabilities is not None:\n            self.probabilities = [\n                float(x) / np.sum(probabilities) for x in self.probabilities\n            ]",
  "def setup(self):\n        self.enable_placeholders()\n        assert (\n            len(self.get_upstream_providers()) > 0\n        ), \"at least one batch provider must be added to the RandomProvider\"\n        if self.probabilities is not None:\n            assert len(self.get_upstream_providers()) == len(self.probabilities), (\n                \"if probabilities are specified, they \"\n                \"need to be given for each batch \"\n                \"provider added to the RandomProvider\"\n            )\n\n        common_spec = None\n\n        # advertise outputs only if all upstream providers have them\n        for provider in self.get_upstream_providers():\n            if common_spec is None:\n                common_spec = provider.spec.copy()\n            else:\n                for key, spec in list(common_spec.items()):\n                    if key not in provider.spec:\n                        del common_spec[key]\n\n        for key, spec in common_spec.items():\n            self.provides(key, spec)\n\n        if self.random_provider_key is not None:\n            self.provides(self.random_provider_key, ArraySpec(nonspatial=True))",
  "def provide(self, request):\n\n        if self.random_provider_key is not None:\n            del request[self.random_provider_key]\n\n        i = np.random.choice(\n            range(len(self.get_upstream_providers())), p=self.probabilities\n        )\n        provider = self.get_upstream_providers()[i]\n        batch = provider.request_batch(request)\n        if self.random_provider_key is not None:\n            batch[self.random_provider_key] = Array(\n                np.array(i), ArraySpec(nonspatial=True)\n            )\n        return batch",
  "class Pad(BatchFilter):\n    \"\"\"Add a constant intensity padding around arrays of another batch\n    provider. This is useful if your requested batches can be larger than what\n    your source provides.\n\n    Args:\n\n        key (:class:`ArrayKey` or :class:`GraphKey`):\n\n            The array or points set to pad.\n\n        size (:class:`Coordinate` or ``None``):\n\n            The padding to be added. If None, an infinite padding is added. If\n            a coordinate, this amount will be added to the ROI in the positive\n            and negative direction.\n\n        value (scalar or ``None``):\n\n            The value to report inside the padding. If not given, 0 is used.\n            Only used for :class:`Array<Arrays>`.\n    \"\"\"\n\n    def __init__(self, key, size, value=None):\n        self.key = key\n        self.size = size\n        self.value = value\n\n    def setup(self):\n        self.enable_autoskip()\n\n        assert self.key in self.spec, (\n            \"Asked to pad %s, but is not provided upstream.\" % self.key\n        )\n        assert self.spec[self.key].roi is not None, (\n            \"Asked to pad %s, but upstream provider doesn't have a ROI for \"\n            \"it.\" % self.key\n        )\n\n        spec = self.spec[self.key].copy()\n        if self.size is not None:\n            spec.roi = spec.roi.grow(self.size, self.size)\n        else:\n            spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n        self.updates(self.key, spec)\n\n    def prepare(self, request):\n        upstream_spec = self.get_upstream_provider().spec\n\n        logger.debug(\"request: %s\" % request)\n        logger.debug(\"upstream spec: %s\" % upstream_spec)\n\n        # TODO: remove this?\n        if self.key not in request:\n            return\n\n        roi = request[self.key].roi.copy()\n\n        # change request to fit into upstream spec\n        request[self.key].roi = roi.intersect(upstream_spec[self.key].roi)\n\n        if request[self.key].roi.empty:\n            logger.warning(\n                \"Requested %s ROI %s lies entirely outside of upstream \" \"ROI %s.\",\n                self.key,\n                roi,\n                upstream_spec[self.key].roi,\n            )\n\n            # ensure a valid request by asking for empty ROI\n            request[self.key].roi = Roi(\n                upstream_spec[self.key].roi.offset,\n                (0,) * upstream_spec[self.key].roi.dims,\n            )\n\n        logger.debug(\"new request: %s\" % request)\n\n        deps = BatchRequest()\n        deps[self.key] = request[self.key]\n        return deps\n\n    def process(self, batch, request):\n        if self.key not in request:\n            return\n\n        # restore requested batch size and ROI\n        if isinstance(self.key, ArrayKey):\n            array = batch.arrays[self.key]\n            array.data = self.__expand(\n                array.data,\n                array.spec.roi / array.spec.voxel_size,\n                request[self.key].roi / array.spec.voxel_size,\n                self.value if self.value else 0,\n            )\n            array.spec.roi = request[self.key].roi\n\n        else:\n            points = batch.graphs[self.key]\n            points.spec.roi = request[self.key].roi\n\n    def __expand(self, a, from_roi, to_roi, value):\n        \"\"\"from_roi and to_roi should be in voxels.\"\"\"\n\n        logger.debug(\n            \"expanding array of shape %s from %s to %s\", str(a.shape), from_roi, to_roi\n        )\n\n        num_channels = len(a.shape) - from_roi.dims\n        channel_shapes = a.shape[:num_channels]\n\n        b = np.zeros(channel_shapes + to_roi.shape, dtype=a.dtype)\n        if value != 0:\n            b[:] = value\n\n        shift = -to_roi.offset\n        logger.debug(\"shifting 'from' by \" + str(shift))\n        a_in_b = from_roi.shift(shift).to_slices()\n\n        logger.debug(\"target shape is \" + str(b.shape))\n        logger.debug(\"target slice is \" + str(a_in_b))\n\n        b[(slice(None),) * num_channels + a_in_b] = a\n\n        return b",
  "def __init__(self, key, size, value=None):\n        self.key = key\n        self.size = size\n        self.value = value",
  "def setup(self):\n        self.enable_autoskip()\n\n        assert self.key in self.spec, (\n            \"Asked to pad %s, but is not provided upstream.\" % self.key\n        )\n        assert self.spec[self.key].roi is not None, (\n            \"Asked to pad %s, but upstream provider doesn't have a ROI for \"\n            \"it.\" % self.key\n        )\n\n        spec = self.spec[self.key].copy()\n        if self.size is not None:\n            spec.roi = spec.roi.grow(self.size, self.size)\n        else:\n            spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n        self.updates(self.key, spec)",
  "def prepare(self, request):\n        upstream_spec = self.get_upstream_provider().spec\n\n        logger.debug(\"request: %s\" % request)\n        logger.debug(\"upstream spec: %s\" % upstream_spec)\n\n        # TODO: remove this?\n        if self.key not in request:\n            return\n\n        roi = request[self.key].roi.copy()\n\n        # change request to fit into upstream spec\n        request[self.key].roi = roi.intersect(upstream_spec[self.key].roi)\n\n        if request[self.key].roi.empty:\n            logger.warning(\n                \"Requested %s ROI %s lies entirely outside of upstream \" \"ROI %s.\",\n                self.key,\n                roi,\n                upstream_spec[self.key].roi,\n            )\n\n            # ensure a valid request by asking for empty ROI\n            request[self.key].roi = Roi(\n                upstream_spec[self.key].roi.offset,\n                (0,) * upstream_spec[self.key].roi.dims,\n            )\n\n        logger.debug(\"new request: %s\" % request)\n\n        deps = BatchRequest()\n        deps[self.key] = request[self.key]\n        return deps",
  "def process(self, batch, request):\n        if self.key not in request:\n            return\n\n        # restore requested batch size and ROI\n        if isinstance(self.key, ArrayKey):\n            array = batch.arrays[self.key]\n            array.data = self.__expand(\n                array.data,\n                array.spec.roi / array.spec.voxel_size,\n                request[self.key].roi / array.spec.voxel_size,\n                self.value if self.value else 0,\n            )\n            array.spec.roi = request[self.key].roi\n\n        else:\n            points = batch.graphs[self.key]\n            points.spec.roi = request[self.key].roi",
  "def __expand(self, a, from_roi, to_roi, value):\n        \"\"\"from_roi and to_roi should be in voxels.\"\"\"\n\n        logger.debug(\n            \"expanding array of shape %s from %s to %s\", str(a.shape), from_roi, to_roi\n        )\n\n        num_channels = len(a.shape) - from_roi.dims\n        channel_shapes = a.shape[:num_channels]\n\n        b = np.zeros(channel_shapes + to_roi.shape, dtype=a.dtype)\n        if value != 0:\n            b[:] = value\n\n        shift = -to_roi.offset\n        logger.debug(\"shifting 'from' by \" + str(shift))\n        a_in_b = from_roi.shift(shift).to_slices()\n\n        logger.debug(\"target shape is \" + str(b.shape))\n        logger.debug(\"target slice is \" + str(a_in_b))\n\n        b[(slice(None),) * num_channels + a_in_b] = a\n\n        return b",
  "class Snapshot(BatchFilter):\n    \"\"\"Save a passing batch in an HDF file.\n\n    The default behaviour is to periodically save a snapshot after\n    ``every`` iterations.\n\n    Data-dependent criteria for saving can be implemented by subclassing and\n    overwriting :func:`write_if`. This method is applied as an additional\n    filter to the batches picked for periodic saving. It should return ``True``\n    if a batch meets the criteria for saving.\n\n    Args:\n\n        dataset_names (``dict``, :class:`ArrayKey` -> ``string``):\n\n            A dictionary from array keys to names of the datasets to store them\n            in.\n\n        output_dir (``string``):\n\n            The directory to save the snapshots. Will be created, if it does\n            not exist.\n\n        output_filename (``string``):\n\n            Template for output filenames. ``{id}`` in the string will be\n            replaced with the ID of the batch. ``{iteration}`` with the training\n            iteration (if training was performed on this batch).\n\n        every (``int``):\n\n            How often to save a batch. ``every=1`` indicates that every batch\n            will be stored, ``every=2`` every second and so on. By default,\n            every batch will be stored.\n\n        additional_request (:class:`BatchRequest`):\n\n            An additional batch request to merge with the passing request, if a\n            snapshot is to be made. If not given, only the arrays that are in\n            the batch anyway are recorded. This is useful to request additional\n            arrays like loss gradients for visualization that are otherwise not\n            needed.\n\n        compression_type (``string`` or ``int``):\n\n            Compression strategy.  Legal values are ``gzip``, ``szip``,\n            ``lzf``. If an integer between 1 and 10, this indicates ``gzip``\n            compression level.\n\n        dataset_dtypes (``dict``, :class:`ArrayKey` -> data type):\n\n            A dictionary from array keys to datatype (eg. ``np.int8``). If\n            given, arrays are stored using this type. The original arrays\n            within the pipeline remain unchanged.\n\n        store_value_range (``bool``):\n\n            If set to ``True``, store range of values in data set attributes.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_names,\n        output_dir=\"snapshots\",\n        output_filename=\"{id}.zarr\",\n        every=1,\n        additional_request=None,\n        compression_type=None,\n        dataset_dtypes=None,\n        store_value_range=False,\n    ):\n        self.dataset_names = dataset_names\n        self.output_dir = output_dir\n        self.output_filename = output_filename\n        self.every = max(1, every)\n        self.additional_request = (\n            BatchRequest() if additional_request is None else additional_request\n        )\n        self.n = 0\n        self.compression_type = compression_type\n        self.store_value_range = store_value_range\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.mode = \"w\"\n\n    def write_if(self, batch):\n        \"\"\"To be implemented in subclasses.\n\n        This function is run in :func:`process` and acts as a data-dependent\n        filter for saving snapshots.\n\n        Args:\n\n            batch (:class:`Batch`):\n\n                The batch received from upstream.\n\n        Returns:\n\n            ``True`` if ``batch`` should be written to snapshot, ``False``\n            otherwise.\n        \"\"\"\n\n        return True\n\n    def setup(self):\n        for key, _ in self.additional_request.items():\n            assert key in self.dataset_names, (\n                \"%s requested but not in dataset_names\" % key\n            )\n\n        for array_key in self.additional_request.array_specs.keys():\n            spec = self.spec[array_key]\n            self.updates(array_key, spec)\n        for graph_key in self.additional_request.graph_specs.keys():\n            spec = self.spec[graph_key]\n            self.updates(graph_key, spec)\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for key, spec in request.items():\n            if key in self.dataset_names:\n                deps[key] = spec\n\n        self.record_snapshot = self.n % self.every == 0\n\n        if self.record_snapshot:\n            # append additional array requests, don't overwrite existing ones\n            for array_key, spec in self.additional_request.array_specs.items():\n                if array_key not in deps:\n                    deps[array_key] = spec\n            for graph_key, spec in self.additional_request.graph_specs.items():\n                if graph_key not in deps:\n                    deps[graph_key] = spec\n\n            for key in self.dataset_names.keys():\n                assert key in deps, \"%s wanted for %s, but not in request.\" % (\n                    key,\n                    self.name(),\n                )\n\n        return deps\n\n    def process(self, batch, request):\n        if self.record_snapshot and self.write_if(batch):\n            try:\n                os.makedirs(self.output_dir)\n            except:\n                pass\n\n            snapshot_name = os.path.join(\n                self.output_dir,\n                self.output_filename.format(\n                    id=str(batch.id).zfill(8), iteration=int(batch.iteration or 0)\n                ),\n            )\n            logger.info(\"saving to %s\" % snapshot_name)\n            if snapshot_name.endswith(\".hdf\"):\n                open_func = h5py.File\n            elif snapshot_name.endswith(\".zarr\"):\n                open_func = ZarrFile\n            else:\n                logger.warning(\"ambiguous file type\")\n                open_func = h5py.File\n\n            with open_func(snapshot_name, self.mode) as f:\n                for array_key, array in batch.arrays.items():\n                    if array_key not in self.dataset_names:\n                        continue\n\n                    ds_name = self.dataset_names[array_key]\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                        dataset = f.create_dataset(\n                            name=ds_name,\n                            data=array.data.astype(dtype),\n                            compression=self.compression_type,\n                        )\n\n                    else:\n                        dataset = f.create_dataset(\n                            name=ds_name,\n                            data=array.data,\n                            compression=self.compression_type,\n                        )\n\n                    if not array.spec.nonspatial:\n                        if array.spec.roi is not None:\n                            dataset.attrs[\"offset\"] = array.spec.roi.offset\n                        dataset.attrs[\"resolution\"] = self.spec[array_key].voxel_size\n\n                    if self.store_value_range:\n                        dataset.attrs[\"value_range\"] = (\n                            np.asscalar(array.data.min()),\n                            np.asscalar(array.data.max()),\n                        )\n\n                    # if array has attributes, add them to the dataset\n                    for attribute_name, attribute in array.attrs.items():\n                        dataset.attrs[attribute_name] = attribute\n\n                for graph_key, graph in batch.graphs.items():\n                    if graph_key not in self.dataset_names:\n                        continue\n\n                    ds_name = self.dataset_names[graph_key]\n\n                    node_ids = []\n                    locations = []\n                    edges = []\n                    for node in graph.nodes:\n                        node_ids.append(node.id)\n                        locations.append(node.location)\n                    for edge in graph.edges:\n                        edges.append((edge.u, edge.v))\n\n                    f.create_dataset(\n                        name=f\"{ds_name}-ids\",\n                        data=np.array(node_ids, dtype=int),\n                        compression=self.compression_type,\n                    )\n                    f.create_dataset(\n                        name=f\"{ds_name}-locations\",\n                        data=np.array(locations),\n                        compression=self.compression_type,\n                    )\n                    f.create_dataset(\n                        name=f\"{ds_name}-edges\",\n                        data=np.array(edges),\n                        compression=self.compression_type,\n                    )\n\n                if batch.loss is not None:\n                    f[\"/\"].attrs[\"loss\"] = float(batch.loss)\n\n        self.n += 1",
  "def __init__(\n        self,\n        dataset_names,\n        output_dir=\"snapshots\",\n        output_filename=\"{id}.zarr\",\n        every=1,\n        additional_request=None,\n        compression_type=None,\n        dataset_dtypes=None,\n        store_value_range=False,\n    ):\n        self.dataset_names = dataset_names\n        self.output_dir = output_dir\n        self.output_filename = output_filename\n        self.every = max(1, every)\n        self.additional_request = (\n            BatchRequest() if additional_request is None else additional_request\n        )\n        self.n = 0\n        self.compression_type = compression_type\n        self.store_value_range = store_value_range\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.mode = \"w\"",
  "def write_if(self, batch):\n        \"\"\"To be implemented in subclasses.\n\n        This function is run in :func:`process` and acts as a data-dependent\n        filter for saving snapshots.\n\n        Args:\n\n            batch (:class:`Batch`):\n\n                The batch received from upstream.\n\n        Returns:\n\n            ``True`` if ``batch`` should be written to snapshot, ``False``\n            otherwise.\n        \"\"\"\n\n        return True",
  "def setup(self):\n        for key, _ in self.additional_request.items():\n            assert key in self.dataset_names, (\n                \"%s requested but not in dataset_names\" % key\n            )\n\n        for array_key in self.additional_request.array_specs.keys():\n            spec = self.spec[array_key]\n            self.updates(array_key, spec)\n        for graph_key in self.additional_request.graph_specs.keys():\n            spec = self.spec[graph_key]\n            self.updates(graph_key, spec)",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for key, spec in request.items():\n            if key in self.dataset_names:\n                deps[key] = spec\n\n        self.record_snapshot = self.n % self.every == 0\n\n        if self.record_snapshot:\n            # append additional array requests, don't overwrite existing ones\n            for array_key, spec in self.additional_request.array_specs.items():\n                if array_key not in deps:\n                    deps[array_key] = spec\n            for graph_key, spec in self.additional_request.graph_specs.items():\n                if graph_key not in deps:\n                    deps[graph_key] = spec\n\n            for key in self.dataset_names.keys():\n                assert key in deps, \"%s wanted for %s, but not in request.\" % (\n                    key,\n                    self.name(),\n                )\n\n        return deps",
  "def process(self, batch, request):\n        if self.record_snapshot and self.write_if(batch):\n            try:\n                os.makedirs(self.output_dir)\n            except:\n                pass\n\n            snapshot_name = os.path.join(\n                self.output_dir,\n                self.output_filename.format(\n                    id=str(batch.id).zfill(8), iteration=int(batch.iteration or 0)\n                ),\n            )\n            logger.info(\"saving to %s\" % snapshot_name)\n            if snapshot_name.endswith(\".hdf\"):\n                open_func = h5py.File\n            elif snapshot_name.endswith(\".zarr\"):\n                open_func = ZarrFile\n            else:\n                logger.warning(\"ambiguous file type\")\n                open_func = h5py.File\n\n            with open_func(snapshot_name, self.mode) as f:\n                for array_key, array in batch.arrays.items():\n                    if array_key not in self.dataset_names:\n                        continue\n\n                    ds_name = self.dataset_names[array_key]\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                        dataset = f.create_dataset(\n                            name=ds_name,\n                            data=array.data.astype(dtype),\n                            compression=self.compression_type,\n                        )\n\n                    else:\n                        dataset = f.create_dataset(\n                            name=ds_name,\n                            data=array.data,\n                            compression=self.compression_type,\n                        )\n\n                    if not array.spec.nonspatial:\n                        if array.spec.roi is not None:\n                            dataset.attrs[\"offset\"] = array.spec.roi.offset\n                        dataset.attrs[\"resolution\"] = self.spec[array_key].voxel_size\n\n                    if self.store_value_range:\n                        dataset.attrs[\"value_range\"] = (\n                            np.asscalar(array.data.min()),\n                            np.asscalar(array.data.max()),\n                        )\n\n                    # if array has attributes, add them to the dataset\n                    for attribute_name, attribute in array.attrs.items():\n                        dataset.attrs[attribute_name] = attribute\n\n                for graph_key, graph in batch.graphs.items():\n                    if graph_key not in self.dataset_names:\n                        continue\n\n                    ds_name = self.dataset_names[graph_key]\n\n                    node_ids = []\n                    locations = []\n                    edges = []\n                    for node in graph.nodes:\n                        node_ids.append(node.id)\n                        locations.append(node.location)\n                    for edge in graph.edges:\n                        edges.append((edge.u, edge.v))\n\n                    f.create_dataset(\n                        name=f\"{ds_name}-ids\",\n                        data=np.array(node_ids, dtype=int),\n                        compression=self.compression_type,\n                    )\n                    f.create_dataset(\n                        name=f\"{ds_name}-locations\",\n                        data=np.array(locations),\n                        compression=self.compression_type,\n                    )\n                    f.create_dataset(\n                        name=f\"{ds_name}-edges\",\n                        data=np.array(edges),\n                        compression=self.compression_type,\n                    )\n\n                if batch.loss is not None:\n                    f[\"/\"].attrs[\"loss\"] = float(batch.loss)\n\n        self.n += 1",
  "class Resample(BatchFilter):\n    \"\"\"Up- or downsample arrays in a batch to match a given voxel size. Note: Behavior is not a pixel-perfect copy of down/upsample nodes, because this node relies on skimage.transform.rescale to perform non-integer scaling factors.\n\n    Args:\n\n        source (:class:`ArrayKey`):\n\n            The key of the array to resample.\n\n        target_voxel_size (:class:`Coordinate`):\n\n            The voxel size of the target.\n\n        target (:class:`ArrayKey`):\n\n            The key of the array to store the resampled ``source``.\n\n        ndim (``int``, optional):\n\n            Dimensionality of upsampling. This allows users to, for instance, specify against\n            resampling in unused z-dimension when processing slices of anisotropic data.\n            Default is to use the dimensionality of ``target_voxel_size``.\n\n        interp_order (``int``, optional):\n\n            The order of interpolation. The order has to be in the range 0-5:\n                0: Nearest-neighbor\n                1: Bi-linear (default)\n                2: Bi-quadratic\n                3: Bi-cubic\n                4: Bi-quartic\n                5: Bi-quintic\n\n                Default is 0 if image.dtype is bool or interpolatable is False, and 1 otherwise.\n\n    \"\"\"\n\n    def __init__(self, source, target_voxel_size, target, ndim=None, interp_order=None):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n\n        self.source = source\n        self.target_voxel_size = Coordinate(target_voxel_size)\n        self.target = target\n        if ndim is None:\n            self.ndim = len(target_voxel_size)\n        else:\n            self.ndim = ndim\n        self.interp_order = interp_order\n\n    def setup(self):\n        spec = self.spec[self.source].copy()\n        source_voxel_size = self.spec[self.source].voxel_size\n        spec.voxel_size = self.target_voxel_size\n        self.pad = Coordinate(\n            (0,) * (len(source_voxel_size) - self.ndim)\n            + source_voxel_size[-self.ndim :]\n        )\n\n        spec.roi = spec.roi.grow(\n            -self.pad, -self.pad\n        )  # Pad w/ 1 voxel per side for interpolation to avoid edge effects\n        spec.roi = spec.roi.snap_to_grid(\n            np.lcm(source_voxel_size, self.target_voxel_size), mode=\"shrink\"\n        )\n\n        self.provides(self.target, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        source_voxel_size = self.spec[self.source].voxel_size\n        source_request = request[self.target].copy()\n        source_request.voxel_size = source_voxel_size\n        source_request.roi = source_request.roi.grow(\n            self.pad, self.pad\n        )  # Pad w/ 1 voxel per side for interpolation to avoid edge effects\n        source_request.roi = source_request.roi.snap_to_grid(\n            np.lcm(source_voxel_size, self.target_voxel_size), mode=\"grow\"\n        )\n        source_request.roi = source_request.roi.intersect(\n            self.spec[self.source].roi\n        ).snap_to_grid(np.lcm(source_voxel_size, self.target_voxel_size), mode=\"shrink\")\n\n        deps = BatchRequest()\n        deps[self.source] = source_request\n\n        return deps\n\n    def process(self, batch, request):\n        source = batch.arrays[self.source]\n        source_data = source.data\n        source_voxel_size = self.spec[self.source].voxel_size\n\n        scales = np.array(source_voxel_size) / np.array(self.target_voxel_size)\n        scales = (1,) * (source_data.ndim - source_voxel_size.dims) + tuple(scales)\n\n        if self.interp_order != 0 and (\n            self.spec[self.source].interpolatable\n            or self.spec[self.source].interpolatable is None\n        ):\n            resampled_data = rescale(\n                source_data.astype(np.float32), scales, order=self.interp_order\n            ).astype(source_data.dtype)\n        else:  # Force nearest-neighbor interpolation for non-interpolatable arrays\n            if self.interp_order is not None and self.interp_order != 0:\n                logger.warning(\n                    \"Interpolation other than nearest-neighbor requested for non-interpolatable array. Using nearest-neighbor instead.\"\n                )\n            resampled_data = rescale(\n                source_data.astype(np.float32), scales, order=0, anti_aliasing=False\n            ).astype(source_data.dtype)\n\n        target_spec = source.spec.copy()\n        target_spec.roi = Roi(\n            source.spec.roi.get_begin(),\n            self.target_voxel_size\n            * Coordinate(resampled_data.shape[-self.target_voxel_size.dims :]),\n        )\n        target_spec.voxel_size = self.target_voxel_size\n        target_spec.dtype = resampled_data.dtype\n        target_array = Array(resampled_data, target_spec)\n        target_array.crop(request[self.target].roi)\n\n        # create output array\n        outputs = Batch()\n        outputs.arrays[self.target] = target_array\n\n        return outputs",
  "def __init__(self, source, target_voxel_size, target, ndim=None, interp_order=None):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n\n        self.source = source\n        self.target_voxel_size = Coordinate(target_voxel_size)\n        self.target = target\n        if ndim is None:\n            self.ndim = len(target_voxel_size)\n        else:\n            self.ndim = ndim\n        self.interp_order = interp_order",
  "def setup(self):\n        spec = self.spec[self.source].copy()\n        source_voxel_size = self.spec[self.source].voxel_size\n        spec.voxel_size = self.target_voxel_size\n        self.pad = Coordinate(\n            (0,) * (len(source_voxel_size) - self.ndim)\n            + source_voxel_size[-self.ndim :]\n        )\n\n        spec.roi = spec.roi.grow(\n            -self.pad, -self.pad\n        )  # Pad w/ 1 voxel per side for interpolation to avoid edge effects\n        spec.roi = spec.roi.snap_to_grid(\n            np.lcm(source_voxel_size, self.target_voxel_size), mode=\"shrink\"\n        )\n\n        self.provides(self.target, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        source_voxel_size = self.spec[self.source].voxel_size\n        source_request = request[self.target].copy()\n        source_request.voxel_size = source_voxel_size\n        source_request.roi = source_request.roi.grow(\n            self.pad, self.pad\n        )  # Pad w/ 1 voxel per side for interpolation to avoid edge effects\n        source_request.roi = source_request.roi.snap_to_grid(\n            np.lcm(source_voxel_size, self.target_voxel_size), mode=\"grow\"\n        )\n        source_request.roi = source_request.roi.intersect(\n            self.spec[self.source].roi\n        ).snap_to_grid(np.lcm(source_voxel_size, self.target_voxel_size), mode=\"shrink\")\n\n        deps = BatchRequest()\n        deps[self.source] = source_request\n\n        return deps",
  "def process(self, batch, request):\n        source = batch.arrays[self.source]\n        source_data = source.data\n        source_voxel_size = self.spec[self.source].voxel_size\n\n        scales = np.array(source_voxel_size) / np.array(self.target_voxel_size)\n        scales = (1,) * (source_data.ndim - source_voxel_size.dims) + tuple(scales)\n\n        if self.interp_order != 0 and (\n            self.spec[self.source].interpolatable\n            or self.spec[self.source].interpolatable is None\n        ):\n            resampled_data = rescale(\n                source_data.astype(np.float32), scales, order=self.interp_order\n            ).astype(source_data.dtype)\n        else:  # Force nearest-neighbor interpolation for non-interpolatable arrays\n            if self.interp_order is not None and self.interp_order != 0:\n                logger.warning(\n                    \"Interpolation other than nearest-neighbor requested for non-interpolatable array. Using nearest-neighbor instead.\"\n                )\n            resampled_data = rescale(\n                source_data.astype(np.float32), scales, order=0, anti_aliasing=False\n            ).astype(source_data.dtype)\n\n        target_spec = source.spec.copy()\n        target_spec.roi = Roi(\n            source.spec.roi.get_begin(),\n            self.target_voxel_size\n            * Coordinate(resampled_data.shape[-self.target_voxel_size.dims :]),\n        )\n        target_spec.voxel_size = self.target_voxel_size\n        target_spec.dtype = resampled_data.dtype\n        target_array = Array(resampled_data, target_spec)\n        target_array.crop(request[self.target].roi)\n\n        # create output array\n        outputs = Batch()\n        outputs.arrays[self.target] = target_array\n\n        return outputs",
  "class MergeProvider(BatchProvider):\n    \"\"\"Merges different providers::\n\n        (a, b, c) + MergeProvider()\n\n    will create a provider that combines the arrays and points offered by\n    ``a``, ``b``, and ``c``. Array and point keys of ``a``, ``b``, and ``c`` should be\n    the disjoint.\n    \"\"\"\n\n    def __init__(self):\n        self.key_to_provider = {}\n\n    def setup(self):\n        self.enable_placeholders()\n        assert (\n            len(self.get_upstream_providers()) > 0\n        ), \"at least one batch provider needs to be added to the MergeProvider\"\n        # Only allow merging if no two upstream_providers have the same\n        # array/points keys\n        error_message = (\n            \"Key {} provided by more than one upstream provider. Node MergeProvider only allows to merge \"\n            \"providers with different keys.\"\n        )\n        for provider in self.get_upstream_providers():\n            for key, spec in provider.spec.items():\n                assert self.spec is None or key not in self.spec, error_message.format(\n                    key\n                )\n                self.provides(key, spec)\n                self.key_to_provider[key] = provider\n\n    def provide(self, request):\n        # create upstream requests\n        upstream_requests = {}\n        for key, spec in request.items():\n            provider = self.key_to_provider[key]\n            if provider not in upstream_requests:\n                if request.is_deterministic():\n                    random_seed = random.randint(0, 2**32)\n                else:\n                    random_seed = None\n                upstream_requests[provider] = BatchRequest(random_seed=random_seed)\n\n            upstream_requests[provider][key] = spec\n\n        # execute requests, merge batches\n        merged_batch = Batch()\n        for provider, upstream_request in upstream_requests.items():\n            batch = provider.request_batch(upstream_request)\n            for key, array in batch.arrays.items():\n                merged_batch.arrays[key] = array\n            for key, graph in batch.graphs.items():\n                merged_batch.graphs[key] = graph\n            merged_batch.profiling_stats.merge_with(batch.profiling_stats)\n\n        return merged_batch",
  "def __init__(self):\n        self.key_to_provider = {}",
  "def setup(self):\n        self.enable_placeholders()\n        assert (\n            len(self.get_upstream_providers()) > 0\n        ), \"at least one batch provider needs to be added to the MergeProvider\"\n        # Only allow merging if no two upstream_providers have the same\n        # array/points keys\n        error_message = (\n            \"Key {} provided by more than one upstream provider. Node MergeProvider only allows to merge \"\n            \"providers with different keys.\"\n        )\n        for provider in self.get_upstream_providers():\n            for key, spec in provider.spec.items():\n                assert self.spec is None or key not in self.spec, error_message.format(\n                    key\n                )\n                self.provides(key, spec)\n                self.key_to_provider[key] = provider",
  "def provide(self, request):\n        # create upstream requests\n        upstream_requests = {}\n        for key, spec in request.items():\n            provider = self.key_to_provider[key]\n            if provider not in upstream_requests:\n                if request.is_deterministic():\n                    random_seed = random.randint(0, 2**32)\n                else:\n                    random_seed = None\n                upstream_requests[provider] = BatchRequest(random_seed=random_seed)\n\n            upstream_requests[provider][key] = spec\n\n        # execute requests, merge batches\n        merged_batch = Batch()\n        for provider, upstream_request in upstream_requests.items():\n            batch = provider.request_batch(upstream_request)\n            for key, array in batch.arrays.items():\n                merged_batch.arrays[key] = array\n            for key, graph in batch.graphs.items():\n                merged_batch.graphs[key] = graph\n            merged_batch.profiling_stats.merge_with(batch.profiling_stats)\n\n        return merged_batch",
  "class SpecifiedLocation(BatchFilter):\n    \"\"\"Choses a batch at a location from the list provided at init, making sure\n    it is in the bounding box of the upstream provider.\n\n    Locations should be given in world units.\n\n    Locations will be chosen in order or at random from the list depending on the\n    ``choose_randomly`` parameter.\n\n    If a location requires a shift outside the bounding box of any upstream provider\n    the module will skip that location with a warning.\n\n    Args:\n\n        locations (``list`` of locations):\n\n            Locations to center batches around.\n\n        choose_randomly (``bool``):\n\n            Defines whether locations should be picked in order or at random\n            from the list.\n\n        extra_data (``list`` of array-like):\n\n            A list of data that will be passed along with the arrays provided\n            by this node. This data will be appended as an attribute to the\n            dataset so it must be a data format compatible with hdf5.\n\n        jitter (``tuple`` of int):\n\n            How far to allow the point to shift in each direction.\n            Default is None, which places the point in the center.\n            Chooses uniformly from [loc - jitter, loc + jitter] in each\n            direction.\n    \"\"\"\n\n    def __init__(self, locations, choose_randomly=False, extra_data=None, jitter=None):\n        self.coordinates = locations\n        self.choose_randomly = choose_randomly\n        self.jitter = jitter\n        self.loc_i = -1\n        self.upstream_spec = None\n        self.specified_shift = None\n\n        if extra_data is not None:\n            assert len(extra_data) == len(locations), (\n                \"extra_data (%d) should match the length of specified locations (%d)\"\n                % (len(extra_data), len(locations))\n            )\n\n        self.extra_data = extra_data\n\n    def setup(self):\n        self.upstream_spec = self.get_upstream_provider().spec\n\n        # clear bounding boxes of all provided arrays and points --\n        # SpecifiedLocation does know its locations at setup (checks on the fly)\n        for key, spec in self.spec.items():\n            spec.roi.shape = (None,) * spec.roi.dims\n            self.updates(key, spec)\n\n    def prepare(self, request):\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n\n        # shift to center\n        total_roi = request.get_total_roi()\n        request_center = total_roi.shape / 2 + total_roi.offset\n\n        self.specified_shift = self._get_next_shift(request_center, lcm_voxel_size)\n        while not self.__check_shift(request):\n            logger.warning(\n                \"Location %s (shift %s) skipped\"\n                % (self.coordinates[self.loc_i], self.specified_shift)\n            )\n            self.specified_shift = self._get_next_shift(request_center, lcm_voxel_size)\n\n        # Set shift for all requests\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                roi = spec.roi.shift(self.specified_shift)\n                specs_type[key].roi = roi\n\n        logger.debug(\n            \"{}'th ({}) shift selected: {}\".format(\n                self.loc_i, self.coordinates[self.loc_i], self.specified_shift\n            )\n        )\n\n        deps = request\n        return deps\n\n    def process(self, batch, request):\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n            if self.extra_data is not None:\n                batch.arrays[array_key].attrs[\n                    \"specified_location_extra_data\"\n                ] = self.extra_data[self.loc_i]\n\n        for graph_key, spec in request.graph_specs.items():\n            batch.points[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.points[graph_key].shift(-self.specified_shift)\n\n    def _get_next_shift(self, center_shift, voxel_size):\n        # gets next coordinate from list\n\n        if self.choose_randomly:\n            self.loc_i = randrange(len(self.coordinates))\n        else:\n            self.loc_i += 1\n            if self.loc_i >= len(self.coordinates):\n                self.loc_i = 0\n                logger.warning(\"Ran out of specified locations, looping list\")\n        next_shift = Coordinate(self.coordinates[self.loc_i]) - center_shift\n\n        if self.jitter is not None:\n            rnd = []\n            for i in range(len(self.jitter)):\n                rnd.append(np.random.randint(-self.jitter[i], self.jitter[i] + 1))\n            next_shift += Coordinate(rnd)\n        logger.debug(\"Shift before rounding: %s\" % str(next_shift))\n        # make sure shift is a multiple of voxel size (round to nearest)\n        next_shift = Coordinate(\n            [\n                int(vs * round(float(shift) / vs))\n                for vs, shift in zip(voxel_size, next_shift)\n            ]\n        )\n        logger.debug(\"Shift after rounding: %s\" % str(next_shift))\n        return next_shift\n\n    def __check_shift(self, request):\n        for key, spec in request.items():\n            request_roi = spec.roi\n            if key in self.upstream_spec:\n                provided_roi = self.upstream_spec[key].roi\n            else:\n                raise Exception(\"Requested %s, but upstream does not provide it.\" % key)\n            shifted_roi = request_roi.shift(self.specified_shift)\n            if not provided_roi.contains(shifted_roi):\n                logger.warning(\n                    \"Provided roi %s for key %s does not contain shifted roi %s\"\n                    % (provided_roi, key, shifted_roi)\n                )\n                return False\n        return True",
  "def __init__(self, locations, choose_randomly=False, extra_data=None, jitter=None):\n        self.coordinates = locations\n        self.choose_randomly = choose_randomly\n        self.jitter = jitter\n        self.loc_i = -1\n        self.upstream_spec = None\n        self.specified_shift = None\n\n        if extra_data is not None:\n            assert len(extra_data) == len(locations), (\n                \"extra_data (%d) should match the length of specified locations (%d)\"\n                % (len(extra_data), len(locations))\n            )\n\n        self.extra_data = extra_data",
  "def setup(self):\n        self.upstream_spec = self.get_upstream_provider().spec\n\n        # clear bounding boxes of all provided arrays and points --\n        # SpecifiedLocation does know its locations at setup (checks on the fly)\n        for key, spec in self.spec.items():\n            spec.roi.shape = (None,) * spec.roi.dims\n            self.updates(key, spec)",
  "def prepare(self, request):\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n\n        # shift to center\n        total_roi = request.get_total_roi()\n        request_center = total_roi.shape / 2 + total_roi.offset\n\n        self.specified_shift = self._get_next_shift(request_center, lcm_voxel_size)\n        while not self.__check_shift(request):\n            logger.warning(\n                \"Location %s (shift %s) skipped\"\n                % (self.coordinates[self.loc_i], self.specified_shift)\n            )\n            self.specified_shift = self._get_next_shift(request_center, lcm_voxel_size)\n\n        # Set shift for all requests\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                roi = spec.roi.shift(self.specified_shift)\n                specs_type[key].roi = roi\n\n        logger.debug(\n            \"{}'th ({}) shift selected: {}\".format(\n                self.loc_i, self.coordinates[self.loc_i], self.specified_shift\n            )\n        )\n\n        deps = request\n        return deps",
  "def process(self, batch, request):\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n            if self.extra_data is not None:\n                batch.arrays[array_key].attrs[\n                    \"specified_location_extra_data\"\n                ] = self.extra_data[self.loc_i]\n\n        for graph_key, spec in request.graph_specs.items():\n            batch.points[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.points[graph_key].shift(-self.specified_shift)",
  "def _get_next_shift(self, center_shift, voxel_size):\n        # gets next coordinate from list\n\n        if self.choose_randomly:\n            self.loc_i = randrange(len(self.coordinates))\n        else:\n            self.loc_i += 1\n            if self.loc_i >= len(self.coordinates):\n                self.loc_i = 0\n                logger.warning(\"Ran out of specified locations, looping list\")\n        next_shift = Coordinate(self.coordinates[self.loc_i]) - center_shift\n\n        if self.jitter is not None:\n            rnd = []\n            for i in range(len(self.jitter)):\n                rnd.append(np.random.randint(-self.jitter[i], self.jitter[i] + 1))\n            next_shift += Coordinate(rnd)\n        logger.debug(\"Shift before rounding: %s\" % str(next_shift))\n        # make sure shift is a multiple of voxel size (round to nearest)\n        next_shift = Coordinate(\n            [\n                int(vs * round(float(shift) / vs))\n                for vs, shift in zip(voxel_size, next_shift)\n            ]\n        )\n        logger.debug(\"Shift after rounding: %s\" % str(next_shift))\n        return next_shift",
  "def __check_shift(self, request):\n        for key, spec in request.items():\n            request_roi = spec.roi\n            if key in self.upstream_spec:\n                provided_roi = self.upstream_spec[key].roi\n            else:\n                raise Exception(\"Requested %s, but upstream does not provide it.\" % key)\n            shifted_roi = request_roi.shift(self.specified_shift)\n            if not provided_roi.contains(shifted_roi):\n                logger.warning(\n                    \"Provided roi %s for key %s does not contain shifted roi %s\"\n                    % (provided_roi, key, shifted_roi)\n                )\n                return False\n        return True",
  "class Stack(BatchFilter):\n    \"\"\"Request several batches and stack them together, introducing a new\n    dimension for each array. This is useful to create batches with several\n    samples and only makes sense if there is a source of randomness upstream.\n\n    This node stacks only arrays, not points. The resulting batch will have the\n    same point sets as found in the first batch requested upstream.\n\n    Args:\n\n        num_repetitions (``int``):\n\n            How many upstream batches to stack.\n    \"\"\"\n\n    def __init__(self, num_repetitions):\n        self.num_repetitions = num_repetitions\n\n    def provide(self, request):\n\n        batches = []\n        for _ in range(self.num_repetitions):\n            upstream_request = request.copy()\n            if upstream_request.is_deterministic():\n                # if the request is deterministic, create new seeds for each\n                # upstream request (otherwise we would get the same batch over\n                # and over). Using randint here is still deterministic, since\n                # the RNG was already seeded with the requests original seed.\n                seed = random.randint(0, 2**32)\n                upstream_request._random_seed = seed\n            batch = self.get_upstream_provider().request_batch(upstream_request)\n            batches.append(batch)\n\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n        for b in batches:\n            batch.profiling_stats.merge_with(b.profiling_stats)\n\n        for key, spec in request.array_specs.items():\n            data = np.stack([b[key].data for b in batches])\n            batch[key] = Array(data, batches[0][key].spec.copy())\n\n        # copy points of first batch requested\n        for key, spec in request.graph_specs.items():\n            batch[key] = batches[0][key]\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __init__(self, num_repetitions):\n        self.num_repetitions = num_repetitions",
  "def provide(self, request):\n\n        batches = []\n        for _ in range(self.num_repetitions):\n            upstream_request = request.copy()\n            if upstream_request.is_deterministic():\n                # if the request is deterministic, create new seeds for each\n                # upstream request (otherwise we would get the same batch over\n                # and over). Using randint here is still deterministic, since\n                # the RNG was already seeded with the requests original seed.\n                seed = random.randint(0, 2**32)\n                upstream_request._random_seed = seed\n            batch = self.get_upstream_provider().request_batch(upstream_request)\n            batches.append(batch)\n\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n        for b in batches:\n            batch.profiling_stats.merge_with(b.profiling_stats)\n\n        for key, spec in request.array_specs.items():\n            data = np.stack([b[key].data for b in batches])\n            batch[key] = Array(data, batches[0][key].spec.copy())\n\n        # copy points of first batch requested\n        for key, spec in request.graph_specs.items():\n            batch[key] = batches[0][key]\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "class ShiftAugment(BatchFilter):\n    def __init__(self, prob_slip=0, prob_shift=0, sigma=0, shift_axis=0):\n        self.prob_slip = prob_slip\n        self.prob_shift = prob_shift\n        self.sigma = sigma\n        self.shift_axis = shift_axis\n\n        self.ndim = None\n        self.shift_sigmas = None\n        self.shift_array = None\n        self.lcm_voxel_size = None\n\n    def prepare(self, request):\n\n        self.ndim = request.get_total_roi().dims\n        assert self.shift_axis in range(self.ndim)\n\n        try:\n            self.shift_sigmas = tuple(self.sigma)\n        except TypeError:\n            self.shift_sigmas = [float(self.sigma)] * self.ndim\n            self.shift_sigmas[self.shift_axis] = 0.0\n            self.shift_sigmas = tuple(self.shift_sigmas)\n\n        assert len(self.shift_sigmas) == self.ndim\n        assert self.shift_sigmas[self.shift_axis] == 0.0\n\n        has_nonzero = False\n        for sigma in self.shift_sigmas:\n            if sigma != 0.0:\n                has_nonzero = True\n                break\n        assert has_nonzero\n\n        if not request.array_specs:\n            raise ValueError(\n                \"Request passed to Jitter node must contain at least one array key. \"\n                + \"Check to make sure that Jitter node is not upstream of a RandomLocation node \"\n                + \"with an ensure_nonempty argument.\"\n            )\n\n        self.lcm_voxel_size = self.spec.get_lcm_voxel_size(\n            array_keys=request.array_specs.keys()\n        )\n        assert self.lcm_voxel_size\n\n        roi_shape = request.get_total_roi().shape\n        assert (\n            roi_shape // self.lcm_voxel_size * self.lcm_voxel_size == roi_shape\n        ), \"total roi shape {} must be divisible by least common voxel size {}\".format(\n            roi_shape, self.lcm_voxel_size\n        )\n        roi_shape_adjusted = roi_shape // self.lcm_voxel_size\n        shift_axis_len = roi_shape_adjusted[self.shift_axis]\n\n        self.shift_array = self.construct_global_shift_array(\n            shift_axis_len,\n            self.shift_sigmas,\n            self.prob_slip,\n            self.prob_shift,\n            self.lcm_voxel_size,\n        )\n\n        for key, spec in request.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            updated_roi = self.compute_upstream_roi(spec.roi, sub_shift_array)\n            spec.roi.offset = updated_roi.offset\n            spec.roi.shape = updated_roi.shape\n            request[key] = spec\n\n        deps = request\n        return deps\n\n    def process(self, batch, request):\n        for array_key, array in batch.arrays.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                array.spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            array.data = self.shift_and_crop(\n                array.data,\n                request[array_key].roi.shape,\n                sub_shift_array,\n                array.spec.voxel_size,\n            )\n            array.spec.roi = request[array_key].roi\n            assert (\n                request[array_key].roi.shape\n                == Coordinate(array.data.shape) * self.lcm_voxel_size\n            ), \"request roi shape {} is not the same as generated array shape {}\".format(\n                request[array_key].roi.shape, array.data.shape\n            )\n            batch[array_key] = array\n\n        for points_key, points in batch.graphs.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                points.spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            points = self.shift_points(\n                points,\n                request[points_key].roi,\n                sub_shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            batch[points_key] = points\n\n    def shift_and_crop(self, arr, roi_shape, sub_shift_array, voxel_size):\n        \"\"\"Shift an array received from upstream and crop it to the target downstream region\n\n        :param arr: an array of upstream data to be shifted and cropped\n        :param roi_shape: the shape of the downstream ROI\n        :param sub_shift_array: the cropped section of the global shift array that applies to this specific request\n        :param voxel_size: the voxel sizes of the data in the array\n        :return: an array of shape roi_shape that contains the array to be passed downstream\n        \"\"\"\n\n        array_shift_axis_len = arr.shape[self.shift_axis]\n        sub_shift_array_len = len(sub_shift_array)\n        assert (\n            array_shift_axis_len % sub_shift_array_len == 0\n        ), \"array shift axis length {} is not divisible by the sub_shift_array length {}\".format(\n            arr.shape[self.shift_axis], sub_shift_array.shape[0]\n        )\n\n        voxel_ratio = array_shift_axis_len // sub_shift_array_len\n\n        # assumption: each sub shift array element divides evenly by the voxel size\n        rescaled_sub_shift_array = sub_shift_array // np.array(voxel_size, dtype=int)\n\n        max_shift = rescaled_sub_shift_array.max(axis=0)\n        batch = arr.copy()\n        batch_view = np.moveaxis(batch, self.shift_axis, 0)\n        for index, plane in enumerate(batch_view):\n            adjusted_index = index // voxel_ratio\n            shift = rescaled_sub_shift_array[adjusted_index, :] - max_shift\n            shift = np.delete(shift, self.shift_axis, axis=0)\n            assert len(shift) == plane.ndim\n            plane = np.roll(plane, shift, axis=tuple(range(len(shift))))\n            batch_view[index] = plane\n\n        adjusted_roi_shape = Coordinate(roi_shape) // Coordinate(voxel_size)\n\n        sl = tuple(slice(0, adjusted_roi_shape[index]) for index in range(self.ndim))\n        return batch[sl]\n\n    @staticmethod\n    def shift_points(points, request_roi, sub_shift_array, shift_axis, lcm_voxel_size):\n        \"\"\"Shift a set of points received from upstream and crop out those not the the target downstream region\n\n        :param points: the points from upstream\n        :param request_roi: the downstream ROI\n        :param sub_shift_array: the cropped section of the global shift array that applies to this specific request\n        :param shift_axis: the axis to perform the shift along\n        :param lcm_voxel_size: the least common voxel size for the arrays in the request\n        :return a Graph object with the updated point locations and ROI\n        \"\"\"\n\n        nodes = list(points.nodes)\n        spec = points.spec\n        shift_axis_start_pos = spec.roi.offset[shift_axis]\n\n        for node in nodes:\n            loc = node.location\n            shift_axis_position = loc[shift_axis]\n            shift_array_index = int(\n                (shift_axis_position - shift_axis_start_pos)\n                // lcm_voxel_size[shift_axis]\n            )\n            assert shift_array_index >= 0\n            shift = Coordinate(sub_shift_array[shift_array_index])\n            loc += shift\n            if not request_roi.contains(loc):\n                points.remove_node(node)\n\n        points.spec.roi = request_roi\n        return points\n\n    @staticmethod\n    def get_sub_shift_array(\n        total_roi, item_roi, shift_array, shift_axis, lcm_voxel_size\n    ):\n        \"\"\"Slices the global shift array to return the sub-shift array to shift an item in the request\n\n        :param total_roi: the total roi of the request\n        :param item_roi: the roi of the item (array or points) being shifted\n        :param shift_array: the shift array for the total_roi\n        :param shift_axis: the axis along which we are shifting\n        :param lcm_voxel_size: the least common voxel size for the arrays in the request\n        :return: the portion of the global shift array that should be used to shift the item\n        \"\"\"\n        item_offset_from_total = item_roi.offset - total_roi.offset\n        offset_in_shift_axis = (\n            item_offset_from_total[shift_axis] // lcm_voxel_size[shift_axis]\n        )\n        len_in_shift_axis = item_roi.shape[shift_axis] // lcm_voxel_size[shift_axis]\n        return shift_array[\n            offset_in_shift_axis : offset_in_shift_axis + len_in_shift_axis\n        ]\n\n    @staticmethod\n    def construct_global_shift_array(\n        shift_axis_len, shift_sigmas, prob_slip, prob_shift, lcm_voxel_size\n    ):\n        \"\"\"Sets the attribute variable self.shift_array\n\n        :param shift_axis_len: the length of the shift axis\n        :param shift_sigmas: the sigma to generate the normal distribution of shift amounts in each direction\n        :param prob_slip: the probability of the slice shifting independently of all other slices\n        :param prob_shift: the probability of the slice and all following slices shifting\n        :param lcm_voxel_size: the least common voxel size of all the arrays in the request\n        :return: the shift_array for the total_roi\n        \"\"\"\n        # each row is one slice along shift axis\n        shift_array = np.zeros(shape=(shift_axis_len, len(shift_sigmas)), dtype=int)\n        base_shift = np.zeros(shape=len(shift_sigmas), dtype=int)\n        assert prob_slip + prob_shift <= 1\n\n        for shift_axis_position in range(shift_axis_len):\n            r = random.random()\n            slip = np.array(\n                [\n                    np.random.normal(scale=sigma / lcm_voxel_size[dimension])\n                    for dimension, sigma in enumerate(shift_sigmas)\n                ]\n            )\n            slip = np.rint(slip).astype(int)\n            slip = slip * np.array(lcm_voxel_size, dtype=int)\n\n            if r <= prob_slip:\n                shift_array[shift_axis_position] = base_shift + slip\n            elif r <= prob_slip + prob_shift:\n                base_shift += slip\n                shift_array[shift_axis_position] = base_shift\n            else:\n                shift_array[shift_axis_position] = base_shift\n\n        return shift_array\n\n    @staticmethod\n    def compute_upstream_roi(request_roi, sub_shift_array):\n        \"\"\"Compute the ROI to pass upstream for a specific item (array or points) in a request\n\n        :param request_roi: the downstream ROI passed to the Jitter node\n        :param sub_shift_array: the portion of the global shift array that should be used to shift the item\n        :return: the expanded ROI to pass upstream\n        \"\"\"\n\n        max_shift = Coordinate(sub_shift_array.max(axis=0))\n        min_shift = Coordinate(sub_shift_array.min(axis=0))\n\n        downstream_offset = request_roi.offset\n        upstream_offset = downstream_offset - max_shift\n        upstream_shape = request_roi.shape + max_shift - min_shift\n        return Roi(offset=upstream_offset, shape=upstream_shape)",
  "def __init__(self, prob_slip=0, prob_shift=0, sigma=0, shift_axis=0):\n        self.prob_slip = prob_slip\n        self.prob_shift = prob_shift\n        self.sigma = sigma\n        self.shift_axis = shift_axis\n\n        self.ndim = None\n        self.shift_sigmas = None\n        self.shift_array = None\n        self.lcm_voxel_size = None",
  "def prepare(self, request):\n\n        self.ndim = request.get_total_roi().dims\n        assert self.shift_axis in range(self.ndim)\n\n        try:\n            self.shift_sigmas = tuple(self.sigma)\n        except TypeError:\n            self.shift_sigmas = [float(self.sigma)] * self.ndim\n            self.shift_sigmas[self.shift_axis] = 0.0\n            self.shift_sigmas = tuple(self.shift_sigmas)\n\n        assert len(self.shift_sigmas) == self.ndim\n        assert self.shift_sigmas[self.shift_axis] == 0.0\n\n        has_nonzero = False\n        for sigma in self.shift_sigmas:\n            if sigma != 0.0:\n                has_nonzero = True\n                break\n        assert has_nonzero\n\n        if not request.array_specs:\n            raise ValueError(\n                \"Request passed to Jitter node must contain at least one array key. \"\n                + \"Check to make sure that Jitter node is not upstream of a RandomLocation node \"\n                + \"with an ensure_nonempty argument.\"\n            )\n\n        self.lcm_voxel_size = self.spec.get_lcm_voxel_size(\n            array_keys=request.array_specs.keys()\n        )\n        assert self.lcm_voxel_size\n\n        roi_shape = request.get_total_roi().shape\n        assert (\n            roi_shape // self.lcm_voxel_size * self.lcm_voxel_size == roi_shape\n        ), \"total roi shape {} must be divisible by least common voxel size {}\".format(\n            roi_shape, self.lcm_voxel_size\n        )\n        roi_shape_adjusted = roi_shape // self.lcm_voxel_size\n        shift_axis_len = roi_shape_adjusted[self.shift_axis]\n\n        self.shift_array = self.construct_global_shift_array(\n            shift_axis_len,\n            self.shift_sigmas,\n            self.prob_slip,\n            self.prob_shift,\n            self.lcm_voxel_size,\n        )\n\n        for key, spec in request.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            updated_roi = self.compute_upstream_roi(spec.roi, sub_shift_array)\n            spec.roi.offset = updated_roi.offset\n            spec.roi.shape = updated_roi.shape\n            request[key] = spec\n\n        deps = request\n        return deps",
  "def process(self, batch, request):\n        for array_key, array in batch.arrays.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                array.spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            array.data = self.shift_and_crop(\n                array.data,\n                request[array_key].roi.shape,\n                sub_shift_array,\n                array.spec.voxel_size,\n            )\n            array.spec.roi = request[array_key].roi\n            assert (\n                request[array_key].roi.shape\n                == Coordinate(array.data.shape) * self.lcm_voxel_size\n            ), \"request roi shape {} is not the same as generated array shape {}\".format(\n                request[array_key].roi.shape, array.data.shape\n            )\n            batch[array_key] = array\n\n        for points_key, points in batch.graphs.items():\n            sub_shift_array = self.get_sub_shift_array(\n                request.get_total_roi(),\n                points.spec.roi,\n                self.shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            points = self.shift_points(\n                points,\n                request[points_key].roi,\n                sub_shift_array,\n                self.shift_axis,\n                self.lcm_voxel_size,\n            )\n            batch[points_key] = points",
  "def shift_and_crop(self, arr, roi_shape, sub_shift_array, voxel_size):\n        \"\"\"Shift an array received from upstream and crop it to the target downstream region\n\n        :param arr: an array of upstream data to be shifted and cropped\n        :param roi_shape: the shape of the downstream ROI\n        :param sub_shift_array: the cropped section of the global shift array that applies to this specific request\n        :param voxel_size: the voxel sizes of the data in the array\n        :return: an array of shape roi_shape that contains the array to be passed downstream\n        \"\"\"\n\n        array_shift_axis_len = arr.shape[self.shift_axis]\n        sub_shift_array_len = len(sub_shift_array)\n        assert (\n            array_shift_axis_len % sub_shift_array_len == 0\n        ), \"array shift axis length {} is not divisible by the sub_shift_array length {}\".format(\n            arr.shape[self.shift_axis], sub_shift_array.shape[0]\n        )\n\n        voxel_ratio = array_shift_axis_len // sub_shift_array_len\n\n        # assumption: each sub shift array element divides evenly by the voxel size\n        rescaled_sub_shift_array = sub_shift_array // np.array(voxel_size, dtype=int)\n\n        max_shift = rescaled_sub_shift_array.max(axis=0)\n        batch = arr.copy()\n        batch_view = np.moveaxis(batch, self.shift_axis, 0)\n        for index, plane in enumerate(batch_view):\n            adjusted_index = index // voxel_ratio\n            shift = rescaled_sub_shift_array[adjusted_index, :] - max_shift\n            shift = np.delete(shift, self.shift_axis, axis=0)\n            assert len(shift) == plane.ndim\n            plane = np.roll(plane, shift, axis=tuple(range(len(shift))))\n            batch_view[index] = plane\n\n        adjusted_roi_shape = Coordinate(roi_shape) // Coordinate(voxel_size)\n\n        sl = tuple(slice(0, adjusted_roi_shape[index]) for index in range(self.ndim))\n        return batch[sl]",
  "def shift_points(points, request_roi, sub_shift_array, shift_axis, lcm_voxel_size):\n        \"\"\"Shift a set of points received from upstream and crop out those not the the target downstream region\n\n        :param points: the points from upstream\n        :param request_roi: the downstream ROI\n        :param sub_shift_array: the cropped section of the global shift array that applies to this specific request\n        :param shift_axis: the axis to perform the shift along\n        :param lcm_voxel_size: the least common voxel size for the arrays in the request\n        :return a Graph object with the updated point locations and ROI\n        \"\"\"\n\n        nodes = list(points.nodes)\n        spec = points.spec\n        shift_axis_start_pos = spec.roi.offset[shift_axis]\n\n        for node in nodes:\n            loc = node.location\n            shift_axis_position = loc[shift_axis]\n            shift_array_index = int(\n                (shift_axis_position - shift_axis_start_pos)\n                // lcm_voxel_size[shift_axis]\n            )\n            assert shift_array_index >= 0\n            shift = Coordinate(sub_shift_array[shift_array_index])\n            loc += shift\n            if not request_roi.contains(loc):\n                points.remove_node(node)\n\n        points.spec.roi = request_roi\n        return points",
  "def get_sub_shift_array(\n        total_roi, item_roi, shift_array, shift_axis, lcm_voxel_size\n    ):\n        \"\"\"Slices the global shift array to return the sub-shift array to shift an item in the request\n\n        :param total_roi: the total roi of the request\n        :param item_roi: the roi of the item (array or points) being shifted\n        :param shift_array: the shift array for the total_roi\n        :param shift_axis: the axis along which we are shifting\n        :param lcm_voxel_size: the least common voxel size for the arrays in the request\n        :return: the portion of the global shift array that should be used to shift the item\n        \"\"\"\n        item_offset_from_total = item_roi.offset - total_roi.offset\n        offset_in_shift_axis = (\n            item_offset_from_total[shift_axis] // lcm_voxel_size[shift_axis]\n        )\n        len_in_shift_axis = item_roi.shape[shift_axis] // lcm_voxel_size[shift_axis]\n        return shift_array[\n            offset_in_shift_axis : offset_in_shift_axis + len_in_shift_axis\n        ]",
  "def construct_global_shift_array(\n        shift_axis_len, shift_sigmas, prob_slip, prob_shift, lcm_voxel_size\n    ):\n        \"\"\"Sets the attribute variable self.shift_array\n\n        :param shift_axis_len: the length of the shift axis\n        :param shift_sigmas: the sigma to generate the normal distribution of shift amounts in each direction\n        :param prob_slip: the probability of the slice shifting independently of all other slices\n        :param prob_shift: the probability of the slice and all following slices shifting\n        :param lcm_voxel_size: the least common voxel size of all the arrays in the request\n        :return: the shift_array for the total_roi\n        \"\"\"\n        # each row is one slice along shift axis\n        shift_array = np.zeros(shape=(shift_axis_len, len(shift_sigmas)), dtype=int)\n        base_shift = np.zeros(shape=len(shift_sigmas), dtype=int)\n        assert prob_slip + prob_shift <= 1\n\n        for shift_axis_position in range(shift_axis_len):\n            r = random.random()\n            slip = np.array(\n                [\n                    np.random.normal(scale=sigma / lcm_voxel_size[dimension])\n                    for dimension, sigma in enumerate(shift_sigmas)\n                ]\n            )\n            slip = np.rint(slip).astype(int)\n            slip = slip * np.array(lcm_voxel_size, dtype=int)\n\n            if r <= prob_slip:\n                shift_array[shift_axis_position] = base_shift + slip\n            elif r <= prob_slip + prob_shift:\n                base_shift += slip\n                shift_array[shift_axis_position] = base_shift\n            else:\n                shift_array[shift_axis_position] = base_shift\n\n        return shift_array",
  "def compute_upstream_roi(request_roi, sub_shift_array):\n        \"\"\"Compute the ROI to pass upstream for a specific item (array or points) in a request\n\n        :param request_roi: the downstream ROI passed to the Jitter node\n        :param sub_shift_array: the portion of the global shift array that should be used to shift the item\n        :return: the expanded ROI to pass upstream\n        \"\"\"\n\n        max_shift = Coordinate(sub_shift_array.max(axis=0))\n        min_shift = Coordinate(sub_shift_array.min(axis=0))\n\n        downstream_offset = request_roi.offset\n        upstream_offset = downstream_offset - max_shift\n        upstream_shape = request_roi.shape + max_shift - min_shift\n        return Roi(offset=upstream_offset, shape=upstream_shape)",
  "class ZarrSource(BatchProvider):\n    \"\"\"A `zarr <https://github.com/zarr-developers/zarr>`_ data source.\n\n    Provides arrays from zarr datasets. If the attribute ``resolution`` is set\n    in a zarr dataset, it will be used as the array's ``voxel_size``. If the\n    attribute ``offset`` is set in a dataset, it will be used as the offset of\n    the :class:`Roi` for this array. It is assumed that the offset is given in\n    world units.\n\n    Args:\n\n        store (``string``, ``zarr.BaseStore``):\n\n            A zarr store or path to a zarr directory or zip file.\n\n        datasets (``dict``, :class:`ArrayKey` -> ``string``):\n\n            Dictionary of array keys to dataset names that this source offers.\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            An optional dictionary of array keys to array specs to overwrite\n            the array specs automatically determined from the data file. This\n            is useful to set a missing ``voxel_size``, for example. Only fields\n            that are not ``None`` in the given :class:`ArraySpec` will be used.\n\n        channels_first (``bool``, optional):\n\n            Specifies the ordering of the dimensions of the HDF5-like data source.\n            If channels_first is set (default), then the input shape is expected\n            to be (channels, spatial dimensions). This is recommended because of\n            better performance. If channels_first is set to false, then the input\n            data is read in channels_last manner and converted to channels_first.\n    \"\"\"\n\n    def __init__(\n        self,\n        store: Union[BaseStore, MutableMapping, str] = None,\n        datasets=None,\n        array_specs=None,\n        channels_first=True,\n        filename=None,\n    ):\n        # datasets is not really optional, this is for backwards compatibility\n        # only\n        assert datasets is not None, \"Argument 'datasets' has to be provided\"\n\n        if filename is not None:\n            warnings.warn(\n                \"Argument 'filename' will be replaced in v2.0, \" \"use 'store' instead\",\n                DeprecationWarning,\n            )\n\n            assert store is None, \"If 'store' is given, 'filename' has to be None\"\n\n            store = filename\n\n        self.store = store\n\n        if array_specs is None:\n            self.array_specs = {}\n        else:\n            self.array_specs = array_specs\n\n        self.channels_first = channels_first\n        self.datasets = datasets\n\n    def _get_voxel_size(self, dataset):\n        if \"resolution\" not in dataset.attrs:\n            return None\n\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"resolution\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"resolution\"])\n\n    def _get_offset(self, dataset):\n        if \"offset\" not in dataset.attrs:\n            return None\n\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"offset\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"offset\"])\n\n    def _rev_metadata(self):\n        with ZarrFile(self.store, mode=\"a\") as store:\n            return isinstance(store, N5Store) or isinstance(store, N5FSStore)\n\n    def _open_file(self, store):\n        return ZarrFile(store, mode=\"r\")\n\n    def setup(self):\n        with self._open_file(self.store) as data_file:\n            for array_key, ds_name in self.datasets.items():\n                if ds_name not in data_file:\n                    raise RuntimeError(\"%s not in %s\" % (ds_name, self.store))\n\n                spec = self.__read_spec(array_key, data_file, ds_name)\n\n                self.provides(array_key, spec)\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with self._open_file(self.store) as data_file:\n            for array_key, request_spec in request.array_specs.items():\n                logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n                voxel_size = self.spec[array_key].voxel_size\n\n                # scale request roi to voxel units\n                dataset_roi = request_spec.roi / voxel_size\n\n                # shift request roi into dataset\n                dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n                # create array spec\n                array_spec = self.spec[array_key].copy()\n                array_spec.roi = request_spec.roi\n\n                # add array to batch\n                batch.arrays[array_key] = Array(\n                    self.__read(data_file, self.datasets[array_key], dataset_roi),\n                    array_spec,\n                )\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def __read_spec(self, array_key, data_file, ds_name):\n        dataset = data_file[ds_name]\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            voxel_size = self._get_voxel_size(dataset)\n            if voxel_size is None:\n                voxel_size = Coordinate((1,) * len(dataset.shape))\n                logger.warning(\n                    \"WARNING: File %s does not contain resolution information \"\n                    \"for %s (dataset %s), voxel size has been set to %s. This \"\n                    \"might not be what you want.\",\n                    self.store,\n                    array_key,\n                    ds_name,\n                    spec.voxel_size,\n                )\n            spec.voxel_size = voxel_size\n\n        self.ndims = len(spec.voxel_size)\n\n        if spec.roi is None:\n            offset = self._get_offset(dataset)\n            if offset is None:\n                offset = Coordinate((0,) * self.ndims)\n\n            if self.channels_first:\n                shape = Coordinate(dataset.shape[-self.ndims :])\n            else:\n                shape = Coordinate(dataset.shape[: self.ndims])\n\n            spec.roi = Roi(offset, shape * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dataset.dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from dataset %s dtype %s\"\n                % (self.array_specs[array_key].dtype, array_key, ds_name, dataset.dtype)\n            )\n        else:\n            spec.dtype = dataset.dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s \"\n                \"(dataset %s). Based on the dtype %s, it has been \"\n                \"set to %s. This might not be what you want.\",\n                array_key,\n                ds_name,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec\n\n    def __read(self, data_file, ds_name, roi):\n        c = len(data_file[ds_name].shape) - self.ndims\n\n        if self.channels_first:\n            array = np.asarray(data_file[ds_name][(slice(None),) * c + roi.to_slices()])\n        else:\n            array = np.asarray(data_file[ds_name][roi.to_slices() + (slice(None),) * c])\n            array = np.transpose(\n                array, axes=[i + self.ndims for i in range(c)] + list(range(self.ndims))\n            )\n\n        return array\n\n    def name(self):\n        return super().name() + f\"[{self.store}]\"",
  "def __init__(\n        self,\n        store: Union[BaseStore, MutableMapping, str] = None,\n        datasets=None,\n        array_specs=None,\n        channels_first=True,\n        filename=None,\n    ):\n        # datasets is not really optional, this is for backwards compatibility\n        # only\n        assert datasets is not None, \"Argument 'datasets' has to be provided\"\n\n        if filename is not None:\n            warnings.warn(\n                \"Argument 'filename' will be replaced in v2.0, \" \"use 'store' instead\",\n                DeprecationWarning,\n            )\n\n            assert store is None, \"If 'store' is given, 'filename' has to be None\"\n\n            store = filename\n\n        self.store = store\n\n        if array_specs is None:\n            self.array_specs = {}\n        else:\n            self.array_specs = array_specs\n\n        self.channels_first = channels_first\n        self.datasets = datasets",
  "def _get_voxel_size(self, dataset):\n        if \"resolution\" not in dataset.attrs:\n            return None\n\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"resolution\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"resolution\"])",
  "def _get_offset(self, dataset):\n        if \"offset\" not in dataset.attrs:\n            return None\n\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"offset\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"offset\"])",
  "def _rev_metadata(self):\n        with ZarrFile(self.store, mode=\"a\") as store:\n            return isinstance(store, N5Store) or isinstance(store, N5FSStore)",
  "def _open_file(self, store):\n        return ZarrFile(store, mode=\"r\")",
  "def setup(self):\n        with self._open_file(self.store) as data_file:\n            for array_key, ds_name in self.datasets.items():\n                if ds_name not in data_file:\n                    raise RuntimeError(\"%s not in %s\" % (ds_name, self.store))\n\n                spec = self.__read_spec(array_key, data_file, ds_name)\n\n                self.provides(array_key, spec)",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with self._open_file(self.store) as data_file:\n            for array_key, request_spec in request.array_specs.items():\n                logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n                voxel_size = self.spec[array_key].voxel_size\n\n                # scale request roi to voxel units\n                dataset_roi = request_spec.roi / voxel_size\n\n                # shift request roi into dataset\n                dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n                # create array spec\n                array_spec = self.spec[array_key].copy()\n                array_spec.roi = request_spec.roi\n\n                # add array to batch\n                batch.arrays[array_key] = Array(\n                    self.__read(data_file, self.datasets[array_key], dataset_roi),\n                    array_spec,\n                )\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __read_spec(self, array_key, data_file, ds_name):\n        dataset = data_file[ds_name]\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            voxel_size = self._get_voxel_size(dataset)\n            if voxel_size is None:\n                voxel_size = Coordinate((1,) * len(dataset.shape))\n                logger.warning(\n                    \"WARNING: File %s does not contain resolution information \"\n                    \"for %s (dataset %s), voxel size has been set to %s. This \"\n                    \"might not be what you want.\",\n                    self.store,\n                    array_key,\n                    ds_name,\n                    spec.voxel_size,\n                )\n            spec.voxel_size = voxel_size\n\n        self.ndims = len(spec.voxel_size)\n\n        if spec.roi is None:\n            offset = self._get_offset(dataset)\n            if offset is None:\n                offset = Coordinate((0,) * self.ndims)\n\n            if self.channels_first:\n                shape = Coordinate(dataset.shape[-self.ndims :])\n            else:\n                shape = Coordinate(dataset.shape[: self.ndims])\n\n            spec.roi = Roi(offset, shape * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dataset.dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from dataset %s dtype %s\"\n                % (self.array_specs[array_key].dtype, array_key, ds_name, dataset.dtype)\n            )\n        else:\n            spec.dtype = dataset.dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s \"\n                \"(dataset %s). Based on the dtype %s, it has been \"\n                \"set to %s. This might not be what you want.\",\n                array_key,\n                ds_name,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec",
  "def __read(self, data_file, ds_name, roi):\n        c = len(data_file[ds_name].shape) - self.ndims\n\n        if self.channels_first:\n            array = np.asarray(data_file[ds_name][(slice(None),) * c + roi.to_slices()])\n        else:\n            array = np.asarray(data_file[ds_name][roi.to_slices() + (slice(None),) * c])\n            array = np.transpose(\n                array, axes=[i + self.ndims for i in range(c)] + list(range(self.ndims))\n            )\n\n        return array",
  "def name(self):\n        return super().name() + f\"[{self.store}]\"",
  "class IntensityScaleShift(BatchFilter):\n    \"\"\"Scales the intensities of a batch by ``scale``, then adds ``shift``.\n\n    Args:\n\n        array (:class:`ArrayKey`):\n\n            The key of the array to modify.\n\n        scale (``float``):\n        shift (``float``):\n\n            The shift and scale to apply to ``array``.\n    \"\"\"\n\n    def __init__(self, array, scale, shift):\n        self.array = array\n        self.scale = scale\n        self.shift = shift\n\n    def process(self, batch, request):\n        if self.array not in batch.arrays:\n            return\n\n        raw = batch.arrays[self.array]\n        raw.data = raw.data * self.scale + self.shift",
  "def __init__(self, array, scale, shift):\n        self.array = array\n        self.scale = scale\n        self.shift = shift",
  "def process(self, batch, request):\n        if self.array not in batch.arrays:\n            return\n\n        raw = batch.arrays[self.array]\n        raw.data = raw.data * self.scale + self.shift",
  "class RasterizationSettings(Freezable):\n    \"\"\"Data structure to store parameters for rasterization of graph.\n\n    Args:\n\n        radius (``float`` or ``tuple`` of ``float``):\n\n            The radius (for balls or tubes) or sigma (for peaks) in world units.\n\n        mode (``string``):\n\n            One of ``ball`` or ``peak``. If ``ball`` (the default), a ball with the\n            given ``radius`` will be drawn. If ``peak``, the point will be\n            rasterized as a peak with values :math:`\\exp(-|x-p|^2/\\sigma)` with\n            sigma set by ``radius``.\n\n        mask (:class:`ArrayKey`, optional):\n\n            Used to mask the rasterization of points. The array is assumed to\n            contain discrete labels. The object id at the specific point being\n            rasterized is used to intersect the rasterization to keep it inside\n            the specific object.\n\n        inner_radius_fraction (``float``, optional):\n\n            Only for mode ``ball``.\n\n            If set, instead of a ball, a hollow sphere is rastered. The radius\n            of the whole sphere corresponds to the radius specified with\n            ``radius``. This parameter sets the radius of the hollow area, as a\n            fraction of ``radius``.\n\n        fg_value (``int``, optional):\n\n            Only for mode ``ball``.\n\n            The value to use to rasterize points, defaults to 1.\n\n        bg_value (``int``, optional):\n\n            Only for mode ``ball``.\n\n            The value to use to for the background in the output array,\n            defaults to 0.\n\n        edges (``bool``, optional):\n\n            Whether to rasterize edges by linearly interpolating between Nodes.\n            Default is True.\n\n        color_attr (``str``, optional)\n\n            Which graph attribute to use for coloring nodes and edges. One\n            useful example might be `component` which would color your graph\n            based on the component labels.\n            Notes:\n            - Only available in \"ball\" mode\n            - Nodes and Edges missing the attribute will be skipped.\n            - color_attr must be populated for nodes and edges upstream of this node\n    \"\"\"\n\n    def __init__(\n        self,\n        radius,\n        mode=\"ball\",\n        mask=None,\n        inner_radius_fraction=None,\n        fg_value=1,\n        bg_value=0,\n        edges=True,\n        color_attr=None,\n    ):\n        radius = np.array([radius]).flatten().astype(np.float64)\n\n        if inner_radius_fraction is not None:\n            assert (\n                inner_radius_fraction > 0.0 and inner_radius_fraction < 1.0\n            ), \"Inner radius fraction has to be between (excluding) 0 and 1\"\n            inner_radius_fraction = 1.0 - inner_radius_fraction\n\n        self.radius = radius\n        self.mode = mode\n        self.mask = mask\n        self.inner_radius_fraction = inner_radius_fraction\n        self.fg_value = fg_value\n        self.bg_value = bg_value\n        self.edges = edges\n        self.color_attr = color_attr\n        self.freeze()",
  "class RasterizeGraph(BatchFilter):\n    \"\"\"Draw graphs into a binary array as balls/tubes of a given radius.\n\n    Args:\n\n        graph (:class:`GraphKey`):\n            The key of the graph to rasterize.\n\n        array (:class:`ArrayKey`):\n            The key of the binary array to create.\n\n        array_spec (:class:`ArraySpec`, optional):\n\n            The spec of the array to create. Use this to set the datatype and\n            voxel size.\n\n        settings (:class:`RasterizationSettings`, optional):\n            Which settings to use to rasterize the graph.\n    \"\"\"\n\n    def __init__(self, graph, array, array_spec=None, settings=None):\n        self.graph = graph\n        self.array = array\n        if array_spec is None:\n            self.array_spec = ArraySpec()\n        else:\n            self.array_spec = array_spec\n        if settings is None:\n            self.settings = RasterizationSettings(1)\n        else:\n            self.settings = settings\n\n    def setup(self):\n        graph_roi = self.spec[self.graph].roi\n\n        if self.array_spec.voxel_size is None:\n            self.array_spec.voxel_size = Coordinate((1,) * graph_roi.dims)\n\n        if self.array_spec.dtype is None:\n            if self.settings.mode == \"ball\":\n                self.array_spec.dtype = np.uint8\n            else:\n                self.array_spec.dtype = np.float32\n\n        self.array_spec.roi = graph_roi.copy()\n        self.provides(self.array, self.array_spec)\n\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        if self.settings.mode == \"ball\":\n            context = np.ceil(self.settings.radius).astype(int)\n        elif self.settings.mode == \"peak\":\n            context = np.ceil(2 * self.settings.radius).astype(int)\n        else:\n            raise RuntimeError(\"unknown raster mode %s\" % self.settings.mode)\n\n        dims = self.array_spec.roi.dims\n        if len(context) == 1:\n            context = context.repeat(dims)\n\n        # request graph in a larger area to get rasterization from outside\n        # graph\n        graph_roi = request[self.array].roi.grow(\n            Coordinate(context), Coordinate(context)\n        )\n\n        # however, restrict the request to the graph actually provided\n        graph_roi = graph_roi.intersect(self.spec[self.graph].roi)\n\n        deps = BatchRequest()\n        deps[self.graph] = GraphSpec(roi=graph_roi)\n\n        if self.settings.mask is not None:\n            mask_voxel_size = self.spec[self.settings.mask].voxel_size\n            assert (\n                self.spec[self.array].voxel_size == mask_voxel_size\n            ), \"Voxel size of mask and rasterized volume need to be equal\"\n\n            new_mask_roi = graph_roi.snap_to_grid(mask_voxel_size)\n            deps[self.settings.mask] = ArraySpec(roi=new_mask_roi)\n\n        return deps\n\n    def process(self, batch, request):\n        graph = batch.graphs[self.graph]\n        mask = self.settings.mask\n        voxel_size = self.spec[self.array].voxel_size\n\n        # get roi used for creating the new array (graph_roi does not\n        # necessarily align with voxel size)\n        enlarged_vol_roi = graph.spec.roi.snap_to_grid(voxel_size)\n        offset = enlarged_vol_roi.begin / voxel_size\n        shape = enlarged_vol_roi.shape / voxel_size\n        data_roi = Roi(offset, shape)\n\n        logger.debug(\"Graph in %s\", graph.spec.roi)\n        for node in graph.nodes:\n            logger.debug(\"%d, %s\", node.id, node.location)\n        logger.debug(\"Data roi in voxels: %s\", data_roi)\n        logger.debug(\"Data roi in world units: %s\", data_roi * voxel_size)\n\n        if graph.num_vertices == 0:\n            # If there are no nodes at all, just create an empty matrix.\n            rasterized_graph_data = np.zeros(\n                data_roi.shape, dtype=self.spec[self.array].dtype\n            )\n        elif mask is not None:\n            mask_array = batch.arrays[mask].crop(enlarged_vol_roi)\n            # get those component labels in the mask, that contain graph\n            labels = []\n            for i, point in graph.data.items():\n                v = Coordinate(point.location / voxel_size)\n                v -= data_roi.begin\n                labels.append(mask_array.data[v])\n            # Make list unique\n            labels = list(set(labels))\n\n            # zero label should be ignored\n            if 0 in labels:\n                labels.remove(0)\n\n            if len(labels) == 0:\n                logger.debug(\n                    \"Graph and provided object mask do not overlap. No graph to rasterize.\"\n                )\n                rasterized_graph_data = np.zeros(\n                    data_roi.shape, dtype=self.spec[self.array].dtype\n                )\n            else:\n                # create data for the whole graph ROI, \"or\"ed together over\n                # individual object masks\n                rasterized_graph_data = np.sum(\n                    [\n                        self.__rasterize(\n                            graph,\n                            data_roi,\n                            voxel_size,\n                            self.spec[self.array].dtype,\n                            self.settings,\n                            Array(data=mask_array.data == label, spec=mask_array.spec),\n                        )\n                        for label in labels\n                    ],\n                    axis=0,\n                )\n\n        else:\n            # create data for the whole graph ROI without mask\n            rasterized_graph_data = self.__rasterize(\n                graph, data_roi, voxel_size, self.spec[self.array].dtype, self.settings\n            )\n\n        # fix bg/fg labelling if requested\n        if self.settings.bg_value != 0 or self.settings.fg_value != 1:\n            replaced = replace(\n                rasterized_graph_data,\n                [0, 1],\n                [self.settings.bg_value, self.settings.fg_value],\n            )\n            rasterized_graph_data = replaced.astype(self.spec[self.array].dtype)\n\n        # create array and crop it to requested roi\n        spec = self.spec[self.array].copy()\n        spec.roi = data_roi * voxel_size\n        rasterized_points = Array(data=rasterized_graph_data, spec=spec)\n        batch[self.array] = rasterized_points.crop(request[self.array].roi)\n\n    def __rasterize(\n        self, graph, data_roi, voxel_size, dtype, settings, mask_array=None\n    ):\n        \"\"\"Rasterize 'graph' into an array with the given 'voxel_size\"\"\"\n\n        mask = mask_array.data if mask_array is not None else None\n\n        logger.debug(\"Rasterizing graph in %s\", graph.spec.roi)\n\n        # prepare output array\n        rasterized_graph = np.zeros(data_roi.shape, dtype=dtype)\n\n        # Fast rasterization currently only implemented for mode ball without\n        # inner radius set\n        use_fast_rasterization = (\n            settings.mode == \"ball\"\n            and settings.inner_radius_fraction is None\n            and len(list(graph.edges)) == 0\n        )\n\n        if use_fast_rasterization:\n            dims = len(rasterized_graph.shape)\n\n            # get structuring element for mode ball\n            ball_kernel = create_ball_kernel(settings.radius, voxel_size)\n            radius_voxel = Coordinate(np.array(ball_kernel.shape) / 2)\n            data_roi_base = Roi(\n                offset=Coordinate((0,) * dims), shape=Coordinate(rasterized_graph.shape)\n            )\n            kernel_roi_base = Roi(\n                offset=Coordinate((0,) * dims), shape=Coordinate(ball_kernel.shape)\n            )\n\n        # Rasterize volume either with single voxel or with defined struct elememt\n        for node in graph.nodes:\n            # get the voxel coordinate, 'Coordinate' ensures integer\n            v = Coordinate(node.location / voxel_size)\n\n            # get the voxel coordinate relative to output array start\n            v -= data_roi.begin\n\n            # skip graph outside of mask\n            if mask is not None and not mask[v]:\n                continue\n\n            logger.debug(\n                \"Rasterizing node %s at %s\",\n                node.location,\n                node.location / voxel_size - data_roi.begin,\n            )\n\n            if use_fast_rasterization:\n                # Calculate where to crop the kernel mask and the rasterized array\n                shifted_kernel = kernel_roi_base.shift(v - radius_voxel)\n                shifted_data = data_roi_base.shift(-(v - radius_voxel))\n                arr_crop = data_roi_base.intersect(shifted_kernel)\n                kernel_crop = kernel_roi_base.intersect(shifted_data)\n                arr_crop_ind = arr_crop.get_bounding_box()\n                kernel_crop_ind = kernel_crop.get_bounding_box()\n\n                rasterized_graph[arr_crop_ind] = np.logical_or(\n                    ball_kernel[kernel_crop_ind], rasterized_graph[arr_crop_ind]\n                )\n\n            else:\n                if settings.color_attr is not None:\n                    c = graph.nodes[node].get(settings.color_attr)\n                    if c is None:\n                        logger.debug(f\"Skipping node: {node}\")\n                        continue\n                    elif np.isclose(c, 1) and not np.isclose(settings.fg_value, 1):\n                        logger.warning(\n                            f\"Node {node} is being colored with color {c} according to \"\n                            f\"attribute {settings.color_attr} \"\n                            f\"but color 1 will be replaced with fg_value: {settings.fg_value}\"\n                        )\n                else:\n                    c = 1\n                rasterized_graph[v] = c\n        if settings.edges:\n            for e in graph.edges:\n                if settings.color_attr is not None:\n                    c = graph.edges[e].get(settings.color_attr)\n                    if c is None:\n                        continue\n                    elif np.isclose(c, 1) and not np.isclose(settings.fg_value, 1):\n                        logger.warning(\n                            f\"Edge {e} is being colored with color {c} according to \"\n                            f\"attribute {settings.color_attr} \"\n                            f\"but color 1 will be replaced with fg_value: {settings.fg_value}\"\n                        )\n\n                u = graph.node(e.u)\n                v = graph.node(e.v)\n                u_coord = Coordinate(u.location / voxel_size)\n                v_coord = Coordinate(v.location / voxel_size)\n                line = draw.line_nd(u_coord, v_coord, endpoint=True)\n                rasterized_graph[line] = 1\n\n        # grow graph\n        if not use_fast_rasterization:\n            if settings.mode == \"ball\":\n                enlarge_binary_map(\n                    rasterized_graph,\n                    settings.radius,\n                    voxel_size,\n                    settings.inner_radius_fraction,\n                    in_place=True,\n                )\n\n            else:\n                sigmas = settings.radius / voxel_size\n\n                gaussian_filter(\n                    rasterized_graph, sigmas, output=rasterized_graph, mode=\"constant\"\n                )\n\n                # renormalize to have 1 be the highest value\n                max_value = np.max(rasterized_graph)\n                if max_value > 0:\n                    rasterized_graph /= max_value\n\n        if mask_array is not None:\n            # use more efficient bitwise operation when possible\n            if settings.mode == \"ball\":\n                rasterized_graph &= mask\n            else:\n                rasterized_graph *= mask\n\n        return rasterized_graph",
  "def __init__(\n        self,\n        radius,\n        mode=\"ball\",\n        mask=None,\n        inner_radius_fraction=None,\n        fg_value=1,\n        bg_value=0,\n        edges=True,\n        color_attr=None,\n    ):\n        radius = np.array([radius]).flatten().astype(np.float64)\n\n        if inner_radius_fraction is not None:\n            assert (\n                inner_radius_fraction > 0.0 and inner_radius_fraction < 1.0\n            ), \"Inner radius fraction has to be between (excluding) 0 and 1\"\n            inner_radius_fraction = 1.0 - inner_radius_fraction\n\n        self.radius = radius\n        self.mode = mode\n        self.mask = mask\n        self.inner_radius_fraction = inner_radius_fraction\n        self.fg_value = fg_value\n        self.bg_value = bg_value\n        self.edges = edges\n        self.color_attr = color_attr\n        self.freeze()",
  "def __init__(self, graph, array, array_spec=None, settings=None):\n        self.graph = graph\n        self.array = array\n        if array_spec is None:\n            self.array_spec = ArraySpec()\n        else:\n            self.array_spec = array_spec\n        if settings is None:\n            self.settings = RasterizationSettings(1)\n        else:\n            self.settings = settings",
  "def setup(self):\n        graph_roi = self.spec[self.graph].roi\n\n        if self.array_spec.voxel_size is None:\n            self.array_spec.voxel_size = Coordinate((1,) * graph_roi.dims)\n\n        if self.array_spec.dtype is None:\n            if self.settings.mode == \"ball\":\n                self.array_spec.dtype = np.uint8\n            else:\n                self.array_spec.dtype = np.float32\n\n        self.array_spec.roi = graph_roi.copy()\n        self.provides(self.array, self.array_spec)\n\n        self.enable_autoskip()",
  "def prepare(self, request):\n        if self.settings.mode == \"ball\":\n            context = np.ceil(self.settings.radius).astype(int)\n        elif self.settings.mode == \"peak\":\n            context = np.ceil(2 * self.settings.radius).astype(int)\n        else:\n            raise RuntimeError(\"unknown raster mode %s\" % self.settings.mode)\n\n        dims = self.array_spec.roi.dims\n        if len(context) == 1:\n            context = context.repeat(dims)\n\n        # request graph in a larger area to get rasterization from outside\n        # graph\n        graph_roi = request[self.array].roi.grow(\n            Coordinate(context), Coordinate(context)\n        )\n\n        # however, restrict the request to the graph actually provided\n        graph_roi = graph_roi.intersect(self.spec[self.graph].roi)\n\n        deps = BatchRequest()\n        deps[self.graph] = GraphSpec(roi=graph_roi)\n\n        if self.settings.mask is not None:\n            mask_voxel_size = self.spec[self.settings.mask].voxel_size\n            assert (\n                self.spec[self.array].voxel_size == mask_voxel_size\n            ), \"Voxel size of mask and rasterized volume need to be equal\"\n\n            new_mask_roi = graph_roi.snap_to_grid(mask_voxel_size)\n            deps[self.settings.mask] = ArraySpec(roi=new_mask_roi)\n\n        return deps",
  "def process(self, batch, request):\n        graph = batch.graphs[self.graph]\n        mask = self.settings.mask\n        voxel_size = self.spec[self.array].voxel_size\n\n        # get roi used for creating the new array (graph_roi does not\n        # necessarily align with voxel size)\n        enlarged_vol_roi = graph.spec.roi.snap_to_grid(voxel_size)\n        offset = enlarged_vol_roi.begin / voxel_size\n        shape = enlarged_vol_roi.shape / voxel_size\n        data_roi = Roi(offset, shape)\n\n        logger.debug(\"Graph in %s\", graph.spec.roi)\n        for node in graph.nodes:\n            logger.debug(\"%d, %s\", node.id, node.location)\n        logger.debug(\"Data roi in voxels: %s\", data_roi)\n        logger.debug(\"Data roi in world units: %s\", data_roi * voxel_size)\n\n        if graph.num_vertices == 0:\n            # If there are no nodes at all, just create an empty matrix.\n            rasterized_graph_data = np.zeros(\n                data_roi.shape, dtype=self.spec[self.array].dtype\n            )\n        elif mask is not None:\n            mask_array = batch.arrays[mask].crop(enlarged_vol_roi)\n            # get those component labels in the mask, that contain graph\n            labels = []\n            for i, point in graph.data.items():\n                v = Coordinate(point.location / voxel_size)\n                v -= data_roi.begin\n                labels.append(mask_array.data[v])\n            # Make list unique\n            labels = list(set(labels))\n\n            # zero label should be ignored\n            if 0 in labels:\n                labels.remove(0)\n\n            if len(labels) == 0:\n                logger.debug(\n                    \"Graph and provided object mask do not overlap. No graph to rasterize.\"\n                )\n                rasterized_graph_data = np.zeros(\n                    data_roi.shape, dtype=self.spec[self.array].dtype\n                )\n            else:\n                # create data for the whole graph ROI, \"or\"ed together over\n                # individual object masks\n                rasterized_graph_data = np.sum(\n                    [\n                        self.__rasterize(\n                            graph,\n                            data_roi,\n                            voxel_size,\n                            self.spec[self.array].dtype,\n                            self.settings,\n                            Array(data=mask_array.data == label, spec=mask_array.spec),\n                        )\n                        for label in labels\n                    ],\n                    axis=0,\n                )\n\n        else:\n            # create data for the whole graph ROI without mask\n            rasterized_graph_data = self.__rasterize(\n                graph, data_roi, voxel_size, self.spec[self.array].dtype, self.settings\n            )\n\n        # fix bg/fg labelling if requested\n        if self.settings.bg_value != 0 or self.settings.fg_value != 1:\n            replaced = replace(\n                rasterized_graph_data,\n                [0, 1],\n                [self.settings.bg_value, self.settings.fg_value],\n            )\n            rasterized_graph_data = replaced.astype(self.spec[self.array].dtype)\n\n        # create array and crop it to requested roi\n        spec = self.spec[self.array].copy()\n        spec.roi = data_roi * voxel_size\n        rasterized_points = Array(data=rasterized_graph_data, spec=spec)\n        batch[self.array] = rasterized_points.crop(request[self.array].roi)",
  "def __rasterize(\n        self, graph, data_roi, voxel_size, dtype, settings, mask_array=None\n    ):\n        \"\"\"Rasterize 'graph' into an array with the given 'voxel_size\"\"\"\n\n        mask = mask_array.data if mask_array is not None else None\n\n        logger.debug(\"Rasterizing graph in %s\", graph.spec.roi)\n\n        # prepare output array\n        rasterized_graph = np.zeros(data_roi.shape, dtype=dtype)\n\n        # Fast rasterization currently only implemented for mode ball without\n        # inner radius set\n        use_fast_rasterization = (\n            settings.mode == \"ball\"\n            and settings.inner_radius_fraction is None\n            and len(list(graph.edges)) == 0\n        )\n\n        if use_fast_rasterization:\n            dims = len(rasterized_graph.shape)\n\n            # get structuring element for mode ball\n            ball_kernel = create_ball_kernel(settings.radius, voxel_size)\n            radius_voxel = Coordinate(np.array(ball_kernel.shape) / 2)\n            data_roi_base = Roi(\n                offset=Coordinate((0,) * dims), shape=Coordinate(rasterized_graph.shape)\n            )\n            kernel_roi_base = Roi(\n                offset=Coordinate((0,) * dims), shape=Coordinate(ball_kernel.shape)\n            )\n\n        # Rasterize volume either with single voxel or with defined struct elememt\n        for node in graph.nodes:\n            # get the voxel coordinate, 'Coordinate' ensures integer\n            v = Coordinate(node.location / voxel_size)\n\n            # get the voxel coordinate relative to output array start\n            v -= data_roi.begin\n\n            # skip graph outside of mask\n            if mask is not None and not mask[v]:\n                continue\n\n            logger.debug(\n                \"Rasterizing node %s at %s\",\n                node.location,\n                node.location / voxel_size - data_roi.begin,\n            )\n\n            if use_fast_rasterization:\n                # Calculate where to crop the kernel mask and the rasterized array\n                shifted_kernel = kernel_roi_base.shift(v - radius_voxel)\n                shifted_data = data_roi_base.shift(-(v - radius_voxel))\n                arr_crop = data_roi_base.intersect(shifted_kernel)\n                kernel_crop = kernel_roi_base.intersect(shifted_data)\n                arr_crop_ind = arr_crop.get_bounding_box()\n                kernel_crop_ind = kernel_crop.get_bounding_box()\n\n                rasterized_graph[arr_crop_ind] = np.logical_or(\n                    ball_kernel[kernel_crop_ind], rasterized_graph[arr_crop_ind]\n                )\n\n            else:\n                if settings.color_attr is not None:\n                    c = graph.nodes[node].get(settings.color_attr)\n                    if c is None:\n                        logger.debug(f\"Skipping node: {node}\")\n                        continue\n                    elif np.isclose(c, 1) and not np.isclose(settings.fg_value, 1):\n                        logger.warning(\n                            f\"Node {node} is being colored with color {c} according to \"\n                            f\"attribute {settings.color_attr} \"\n                            f\"but color 1 will be replaced with fg_value: {settings.fg_value}\"\n                        )\n                else:\n                    c = 1\n                rasterized_graph[v] = c\n        if settings.edges:\n            for e in graph.edges:\n                if settings.color_attr is not None:\n                    c = graph.edges[e].get(settings.color_attr)\n                    if c is None:\n                        continue\n                    elif np.isclose(c, 1) and not np.isclose(settings.fg_value, 1):\n                        logger.warning(\n                            f\"Edge {e} is being colored with color {c} according to \"\n                            f\"attribute {settings.color_attr} \"\n                            f\"but color 1 will be replaced with fg_value: {settings.fg_value}\"\n                        )\n\n                u = graph.node(e.u)\n                v = graph.node(e.v)\n                u_coord = Coordinate(u.location / voxel_size)\n                v_coord = Coordinate(v.location / voxel_size)\n                line = draw.line_nd(u_coord, v_coord, endpoint=True)\n                rasterized_graph[line] = 1\n\n        # grow graph\n        if not use_fast_rasterization:\n            if settings.mode == \"ball\":\n                enlarge_binary_map(\n                    rasterized_graph,\n                    settings.radius,\n                    voxel_size,\n                    settings.inner_radius_fraction,\n                    in_place=True,\n                )\n\n            else:\n                sigmas = settings.radius / voxel_size\n\n                gaussian_filter(\n                    rasterized_graph, sigmas, output=rasterized_graph, mode=\"constant\"\n                )\n\n                # renormalize to have 1 be the highest value\n                max_value = np.max(rasterized_graph)\n                if max_value > 0:\n                    rasterized_graph /= max_value\n\n        if mask_array is not None:\n            # use more efficient bitwise operation when possible\n            if settings.mode == \"ball\":\n                rasterized_graph &= mask\n            else:\n                rasterized_graph *= mask\n\n        return rasterized_graph",
  "class PrintProfilingStats(BatchFilter):\n    \"\"\"Print profiling information about nodes upstream of this node in the DAG.\n\n    The output also includes a ``TOTAL`` section, which shows the wall-time\n    spent in the upstream and downstream passes. For the downstream pass, this\n    information is not available in the first iteration, since the request-batch\n    cycle is not completed, yet.\n\n    Args:\n\n        every (``int``):\n\n            Collect statistics about that many batch requests and show min,\n            max, mean, and median runtimes.\n    \"\"\"\n\n    def __init__(self, every=1):\n        self.every = every\n        self.n = 0\n        self.accumulated_stats = ProfilingStats()\n        self.__upstream_timing = Timing(self)\n        self.__upstream_timing_summary = TimingSummary()\n        self.__downstream_timing = Timing(self)\n        self.__downstream_timing_summary = TimingSummary()\n\n    def prepare(self, request):\n        self.__downstream_timing.stop()\n        # skip the first one, where we don't know how much time we spent\n        # downstream\n        if self.__downstream_timing.elapsed() > 0:\n            self.__downstream_timing_summary.add(self.__downstream_timing)\n            self.__downstream_timing = Timing(self)\n\n        self.__upstream_timing.start()\n\n        deps = request\n        return deps\n\n    def process(self, batch, request):\n        self.__upstream_timing.stop()\n        self.__upstream_timing_summary.add(self.__upstream_timing)\n        self.__upstream_timing = Timing(self)\n\n        self.__downstream_timing.start()\n\n        self.n += 1\n        print_stats = self.n % self.every == 0\n\n        self.accumulated_stats.merge_with(batch.profiling_stats)\n\n        if not print_stats:\n            return\n\n        stats = \"\\n\"\n        stats += \"Profiling Stats\\n\"\n        stats += \"===============\\n\"\n        stats += \"\\n\"\n        stats += \"NODE\".ljust(20)\n        stats += \"METHOD\".ljust(10)\n        stats += \"COUNTS\".ljust(10)\n        stats += \"MIN\".ljust(10)\n        stats += \"MAX\".ljust(10)\n        stats += \"MEAN\".ljust(10)\n        stats += \"MEDIAN\".ljust(10)\n        stats += \"\\n\"\n\n        summaries = list(self.accumulated_stats.get_timing_summaries().items())\n        summaries.sort()\n\n        for (node_name, method_name), summary in summaries:\n            if summary.counts() > 0:\n                stats += node_name[:19].ljust(20)\n                stats += (\n                    method_name[:19].ljust(10) if method_name is not None else \" \" * 10\n                )\n                stats += (\"%d\" % summary.counts())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.min())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.max())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.mean())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.median())[:9].ljust(10)\n                stats += \"\\n\"\n\n        stats += \"\\n\"\n        stats += \"TOTAL\"\n        stats += \"\\n\"\n\n        for phase, summary in zip(\n            [\"upstream\", \"downstream\"],\n            [self.__upstream_timing_summary, self.__downstream_timing_summary],\n        ):\n            if summary.counts() > 0:\n                stats += phase[:19].ljust(30)\n                stats += (\"%d\" % summary.counts())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.min())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.max())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.mean())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.median())[:9].ljust(10)\n                stats += \"\\n\"\n\n        stats += \"\\n\"\n\n        logger.info(stats)\n\n        # reset summaries\n        self.accumulated_stats = ProfilingStats()\n        self.__upstream_timing_summary = TimingSummary()\n        self.__downstream_timing_summary = TimingSummary()",
  "def __init__(self, every=1):\n        self.every = every\n        self.n = 0\n        self.accumulated_stats = ProfilingStats()\n        self.__upstream_timing = Timing(self)\n        self.__upstream_timing_summary = TimingSummary()\n        self.__downstream_timing = Timing(self)\n        self.__downstream_timing_summary = TimingSummary()",
  "def prepare(self, request):\n        self.__downstream_timing.stop()\n        # skip the first one, where we don't know how much time we spent\n        # downstream\n        if self.__downstream_timing.elapsed() > 0:\n            self.__downstream_timing_summary.add(self.__downstream_timing)\n            self.__downstream_timing = Timing(self)\n\n        self.__upstream_timing.start()\n\n        deps = request\n        return deps",
  "def process(self, batch, request):\n        self.__upstream_timing.stop()\n        self.__upstream_timing_summary.add(self.__upstream_timing)\n        self.__upstream_timing = Timing(self)\n\n        self.__downstream_timing.start()\n\n        self.n += 1\n        print_stats = self.n % self.every == 0\n\n        self.accumulated_stats.merge_with(batch.profiling_stats)\n\n        if not print_stats:\n            return\n\n        stats = \"\\n\"\n        stats += \"Profiling Stats\\n\"\n        stats += \"===============\\n\"\n        stats += \"\\n\"\n        stats += \"NODE\".ljust(20)\n        stats += \"METHOD\".ljust(10)\n        stats += \"COUNTS\".ljust(10)\n        stats += \"MIN\".ljust(10)\n        stats += \"MAX\".ljust(10)\n        stats += \"MEAN\".ljust(10)\n        stats += \"MEDIAN\".ljust(10)\n        stats += \"\\n\"\n\n        summaries = list(self.accumulated_stats.get_timing_summaries().items())\n        summaries.sort()\n\n        for (node_name, method_name), summary in summaries:\n            if summary.counts() > 0:\n                stats += node_name[:19].ljust(20)\n                stats += (\n                    method_name[:19].ljust(10) if method_name is not None else \" \" * 10\n                )\n                stats += (\"%d\" % summary.counts())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.min())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.max())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.mean())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.median())[:9].ljust(10)\n                stats += \"\\n\"\n\n        stats += \"\\n\"\n        stats += \"TOTAL\"\n        stats += \"\\n\"\n\n        for phase, summary in zip(\n            [\"upstream\", \"downstream\"],\n            [self.__upstream_timing_summary, self.__downstream_timing_summary],\n        ):\n            if summary.counts() > 0:\n                stats += phase[:19].ljust(30)\n                stats += (\"%d\" % summary.counts())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.min())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.max())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.mean())[:9].ljust(10)\n                stats += (\"%.2f\" % summary.median())[:9].ljust(10)\n                stats += \"\\n\"\n\n        stats += \"\\n\"\n\n        logger.info(stats)\n\n        # reset summaries\n        self.accumulated_stats = ProfilingStats()\n        self.__upstream_timing_summary = TimingSummary()\n        self.__downstream_timing_summary = TimingSummary()",
  "class Hdf5LikeSource(BatchProvider):\n    \"\"\"An HDF5-like data source.\n\n    Provides arrays from datasets accessed with an h5py-like API for each array\n    key given. If the attribute ``resolution`` is set in a dataset, it will be\n    used as the array's ``voxel_size``. If the attribute ``offset`` is set in a\n    dataset, it will be used as the offset of the :class:`Roi` for this array.\n    It is assumed that the offset is given in world units.\n\n    Args:\n\n        filename (``string``):\n\n            The input file.\n\n        datasets (``dict``, :class:`ArrayKey` -> ``string``):\n\n            Dictionary of array keys to dataset names that this source offers.\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            An optional dictionary of array keys to array specs to overwrite\n            the array specs automatically determined from the data file. This\n            is useful to set a missing ``voxel_size``, for example. Only fields\n            that are not ``None`` in the given :class:`ArraySpec` will be used.\n\n        channels_first (``bool``, optional):\n\n            Specifies the ordering of the dimensions of the HDF5-like data source.\n            If channels_first is set (default), then the input shape is expected\n            to be (channels, spatial dimensions). This is recommended due to\n            better performance. If channels_first is set to false, then the input\n            data is read in channels_last manner and converted to channels_first.\n    \"\"\"\n\n    def __init__(self, filename, datasets, array_specs=None, channels_first=True):\n        warnings.warn(\n            \"HDF5LikeSource is depricated and will soon be removed in v2.0\",\n            DeprecationWarning,\n        )\n\n        self.filename = filename\n        self.datasets = datasets\n\n        if array_specs is None:\n            self.array_specs = {}\n        else:\n            self.array_specs = array_specs\n\n        self.channels_first = channels_first\n\n        # number of spatial dimensions\n        self.ndims = None\n\n    def _open_file(self, filename):\n        raise NotImplementedError(\"Only implemented in subclasses\")\n\n    def setup(self):\n        with self._open_file(self.filename) as data_file:\n            for array_key, ds_name in self.datasets.items():\n                if ds_name not in data_file:\n                    raise RuntimeError(\"%s not in %s\" % (ds_name, self.filename))\n\n                spec = self.__read_spec(array_key, data_file, ds_name)\n\n                self.provides(array_key, spec)\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with self._open_file(self.filename) as data_file:\n            for array_key, request_spec in request.array_specs.items():\n                logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n                voxel_size = self.spec[array_key].voxel_size\n\n                # scale request roi to voxel units\n                dataset_roi = request_spec.roi / voxel_size\n\n                # shift request roi into dataset\n                dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n                # create array spec\n                array_spec = self.spec[array_key].copy()\n                array_spec.roi = request_spec.roi\n\n                # add array to batch\n                batch.arrays[array_key] = Array(\n                    self.__read(data_file, self.datasets[array_key], dataset_roi),\n                    array_spec,\n                )\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def _get_voxel_size(self, dataset):\n        try:\n            return Coordinate(dataset.attrs[\"resolution\"])\n        except Exception:  # todo: make specific when z5py supports it\n            return None\n\n    def _get_offset(self, dataset):\n        try:\n            return Coordinate(dataset.attrs[\"offset\"])\n        except Exception:  # todo: make specific when z5py supports it\n            return None\n\n    def __read_spec(self, array_key, data_file, ds_name):\n        dataset = data_file[ds_name]\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            voxel_size = self._get_voxel_size(dataset)\n            if voxel_size is None:\n                voxel_size = Coordinate((1,) * len(dataset.shape))\n                logger.warning(\n                    \"WARNING: File %s does not contain resolution information \"\n                    \"for %s (dataset %s), voxel size has been set to %s. This \"\n                    \"might not be what you want.\",\n                    self.filename,\n                    array_key,\n                    ds_name,\n                    spec.voxel_size,\n                )\n            spec.voxel_size = voxel_size\n\n        self.ndims = len(spec.voxel_size)\n\n        if spec.roi is None:\n            offset = self._get_offset(dataset)\n            if offset is None:\n                offset = Coordinate((0,) * self.ndims)\n\n            if self.channels_first:\n                shape = Coordinate(dataset.shape[-self.ndims :])\n            else:\n                shape = Coordinate(dataset.shape[: self.ndims])\n\n            spec.roi = Roi(offset, shape * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dataset.dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from dataset %s dtype %s\"\n                % (self.array_specs[array_key].dtype, array_key, ds_name, dataset.dtype)\n            )\n        else:\n            spec.dtype = dataset.dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s \"\n                \"(dataset %s). Based on the dtype %s, it has been \"\n                \"set to %s. This might not be what you want.\",\n                array_key,\n                ds_name,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec\n\n    def __read(self, data_file, ds_name, roi):\n        c = len(data_file[ds_name].shape) - self.ndims\n\n        if self.channels_first:\n            array = np.asarray(data_file[ds_name][(slice(None),) * c + roi.to_slices()])\n        else:\n            array = np.asarray(data_file[ds_name][roi.to_slices() + (slice(None),) * c])\n            array = np.transpose(\n                array, axes=[i + self.ndims for i in range(c)] + list(range(self.ndims))\n            )\n\n        return array\n\n    def name(self):\n        return super().name() + f\"[{self.filename}]\"",
  "def __init__(self, filename, datasets, array_specs=None, channels_first=True):\n        warnings.warn(\n            \"HDF5LikeSource is depricated and will soon be removed in v2.0\",\n            DeprecationWarning,\n        )\n\n        self.filename = filename\n        self.datasets = datasets\n\n        if array_specs is None:\n            self.array_specs = {}\n        else:\n            self.array_specs = array_specs\n\n        self.channels_first = channels_first\n\n        # number of spatial dimensions\n        self.ndims = None",
  "def _open_file(self, filename):\n        raise NotImplementedError(\"Only implemented in subclasses\")",
  "def setup(self):\n        with self._open_file(self.filename) as data_file:\n            for array_key, ds_name in self.datasets.items():\n                if ds_name not in data_file:\n                    raise RuntimeError(\"%s not in %s\" % (ds_name, self.filename))\n\n                spec = self.__read_spec(array_key, data_file, ds_name)\n\n                self.provides(array_key, spec)",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        with self._open_file(self.filename) as data_file:\n            for array_key, request_spec in request.array_specs.items():\n                logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n                voxel_size = self.spec[array_key].voxel_size\n\n                # scale request roi to voxel units\n                dataset_roi = request_spec.roi / voxel_size\n\n                # shift request roi into dataset\n                dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n                # create array spec\n                array_spec = self.spec[array_key].copy()\n                array_spec.roi = request_spec.roi\n\n                # add array to batch\n                batch.arrays[array_key] = Array(\n                    self.__read(data_file, self.datasets[array_key], dataset_roi),\n                    array_spec,\n                )\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def _get_voxel_size(self, dataset):\n        try:\n            return Coordinate(dataset.attrs[\"resolution\"])\n        except Exception:  # todo: make specific when z5py supports it\n            return None",
  "def _get_offset(self, dataset):\n        try:\n            return Coordinate(dataset.attrs[\"offset\"])\n        except Exception:  # todo: make specific when z5py supports it\n            return None",
  "def __read_spec(self, array_key, data_file, ds_name):\n        dataset = data_file[ds_name]\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            voxel_size = self._get_voxel_size(dataset)\n            if voxel_size is None:\n                voxel_size = Coordinate((1,) * len(dataset.shape))\n                logger.warning(\n                    \"WARNING: File %s does not contain resolution information \"\n                    \"for %s (dataset %s), voxel size has been set to %s. This \"\n                    \"might not be what you want.\",\n                    self.filename,\n                    array_key,\n                    ds_name,\n                    spec.voxel_size,\n                )\n            spec.voxel_size = voxel_size\n\n        self.ndims = len(spec.voxel_size)\n\n        if spec.roi is None:\n            offset = self._get_offset(dataset)\n            if offset is None:\n                offset = Coordinate((0,) * self.ndims)\n\n            if self.channels_first:\n                shape = Coordinate(dataset.shape[-self.ndims :])\n            else:\n                shape = Coordinate(dataset.shape[: self.ndims])\n\n            spec.roi = Roi(offset, shape * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dataset.dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from dataset %s dtype %s\"\n                % (self.array_specs[array_key].dtype, array_key, ds_name, dataset.dtype)\n            )\n        else:\n            spec.dtype = dataset.dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s \"\n                \"(dataset %s). Based on the dtype %s, it has been \"\n                \"set to %s. This might not be what you want.\",\n                array_key,\n                ds_name,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec",
  "def __read(self, data_file, ds_name, roi):\n        c = len(data_file[ds_name].shape) - self.ndims\n\n        if self.channels_first:\n            array = np.asarray(data_file[ds_name][(slice(None),) * c + roi.to_slices()])\n        else:\n            array = np.asarray(data_file[ds_name][roi.to_slices() + (slice(None),) * c])\n            array = np.transpose(\n                array, axes=[i + self.ndims for i in range(c)] + list(range(self.ndims))\n            )\n\n        return array",
  "def name(self):\n        return super().name() + f\"[{self.filename}]\"",
  "class CsvPointsSource(BatchProvider):\n    \"\"\"Read a set of points from a comma-separated-values text file. Each line\n    in the file represents one point, e.g. z y x (id)\n\n    Args:\n\n        filename (``string``):\n\n            The file to read from.\n\n        points (:class:`GraphKey`):\n\n            The key of the points set to create.\n\n        points_spec (:class:`GraphSpec`, optional):\n\n            An optional :class:`GraphSpec` to overwrite the points specs\n            automatically determined from the CSV file. This is useful to set\n            the :class:`Roi` manually.\n\n        scale (scalar or array-like):\n\n            An optional scaling to apply to the coordinates of the points read\n            from the CSV file. This is useful if the points refer to voxel\n            positions to convert them to world units.\n\n        ndims (``int``):\n\n            If ``ndims`` is None, all values in one line are considered as the\n            location of the point. If positive, only the first ``ndims`` are used.\n            If negative, all but the last ``-ndims`` are used.\n\n         id_dim (``int``):\n\n            Each line may optionally contain an id for each point. This parameter\n            specifies its location, has to come after the position values.\n    \"\"\"\n\n    def __init__(\n        self, filename, points, points_spec=None, scale=None, ndims=None, id_dim=None\n    ):\n        self.filename = filename\n        self.points = points\n        self.points_spec = points_spec\n        self.scale = scale\n        self.ndims = ndims\n        self.id_dim = id_dim\n        self.data = None\n\n    def setup(self):\n        self._parse_csv()\n\n        if self.points_spec is not None:\n            self.provides(self.points, self.points_spec)\n            return\n\n        min_bb = Coordinate(np.floor(np.amin(self.data[:, : self.ndims], 0)))\n        max_bb = Coordinate(np.ceil(np.amax(self.data[:, : self.ndims], 0)) + 1)\n\n        roi = Roi(min_bb, max_bb - min_bb)\n\n        self.provides(self.points, GraphSpec(roi=roi))\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        min_bb = request[self.points].roi.begin\n        max_bb = request[self.points].roi.end\n\n        logger.debug(\"CSV points source got request for %s\", request[self.points].roi)\n\n        point_filter = np.ones((self.data.shape[0],), dtype=bool)\n        for d in range(self.ndims):\n            point_filter = np.logical_and(point_filter, self.data[:, d] >= min_bb[d])\n            point_filter = np.logical_and(point_filter, self.data[:, d] < max_bb[d])\n\n        points_data = self._get_points(point_filter)\n        points_spec = GraphSpec(roi=request[self.points].roi.copy())\n\n        batch = Batch()\n        batch.graphs[self.points] = Graph(points_data, [], points_spec)\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def _get_points(self, point_filter):\n        filtered = self.data[point_filter][:, : self.ndims]\n\n        if self.id_dim is not None:\n            ids = self.data[point_filter][:, self.id_dim]\n        else:\n            ids = np.arange(len(self.data))[point_filter]\n\n        return [Node(id=i, location=p) for i, p in zip(ids, filtered)]\n\n    def _parse_csv(self):\n        \"\"\"Read one point per line. If ``ndims`` is None, all values in one line\n        are considered as the location of the point. If positive, only the\n        first ``ndims`` are used. If negative, all but the last ``-ndims`` are\n        used.\n        \"\"\"\n\n        with open(self.filename, \"r\") as f:\n            self.data = np.array(\n                [[float(t.strip(\",\")) for t in line.split()] for line in f],\n                dtype=np.float32,\n            )\n\n        if self.ndims is None:\n            self.ndims = self.data.shape[1]\n\n        if self.scale is not None:\n            self.data[:, : self.ndims] *= self.scale",
  "def __init__(\n        self, filename, points, points_spec=None, scale=None, ndims=None, id_dim=None\n    ):\n        self.filename = filename\n        self.points = points\n        self.points_spec = points_spec\n        self.scale = scale\n        self.ndims = ndims\n        self.id_dim = id_dim\n        self.data = None",
  "def setup(self):\n        self._parse_csv()\n\n        if self.points_spec is not None:\n            self.provides(self.points, self.points_spec)\n            return\n\n        min_bb = Coordinate(np.floor(np.amin(self.data[:, : self.ndims], 0)))\n        max_bb = Coordinate(np.ceil(np.amax(self.data[:, : self.ndims], 0)) + 1)\n\n        roi = Roi(min_bb, max_bb - min_bb)\n\n        self.provides(self.points, GraphSpec(roi=roi))",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        min_bb = request[self.points].roi.begin\n        max_bb = request[self.points].roi.end\n\n        logger.debug(\"CSV points source got request for %s\", request[self.points].roi)\n\n        point_filter = np.ones((self.data.shape[0],), dtype=bool)\n        for d in range(self.ndims):\n            point_filter = np.logical_and(point_filter, self.data[:, d] >= min_bb[d])\n            point_filter = np.logical_and(point_filter, self.data[:, d] < max_bb[d])\n\n        points_data = self._get_points(point_filter)\n        points_spec = GraphSpec(roi=request[self.points].roi.copy())\n\n        batch = Batch()\n        batch.graphs[self.points] = Graph(points_data, [], points_spec)\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def _get_points(self, point_filter):\n        filtered = self.data[point_filter][:, : self.ndims]\n\n        if self.id_dim is not None:\n            ids = self.data[point_filter][:, self.id_dim]\n        else:\n            ids = np.arange(len(self.data))[point_filter]\n\n        return [Node(id=i, location=p) for i, p in zip(ids, filtered)]",
  "def _parse_csv(self):\n        \"\"\"Read one point per line. If ``ndims`` is None, all values in one line\n        are considered as the location of the point. If positive, only the\n        first ``ndims`` are used. If negative, all but the last ``-ndims`` are\n        used.\n        \"\"\"\n\n        with open(self.filename, \"r\") as f:\n            self.data = np.array(\n                [[float(t.strip(\",\")) for t in line.split()] for line in f],\n                dtype=np.float32,\n            )\n\n        if self.ndims is None:\n            self.ndims = self.data.shape[1]\n\n        if self.scale is not None:\n            self.data[:, : self.ndims] *= self.scale",
  "class BalanceLabels(BatchFilter):\n    \"\"\"Creates a scale array to balance the loss between class labels.\n\n    Note that this only balances loss weights per-batch and does not accumulate\n    statistics about class balance across batches.\n\n    Args:\n\n        labels (:class:`ArrayKey`):\n\n            An array containing binary or integer labels.\n\n        scales (:class:`ArrayKey`):\n\n            A array with scales to be created. This new array will have the\n            same ROI and resolution as ``labels``.\n\n        mask (:class:`ArrayKey`, optional):\n\n            An optional mask (or list of masks) to consider for balancing.\n            Every voxel marked with a 0 will not contribute to the scaling and\n            will have a scale of 0 in ``scales``.\n\n        slab (``tuple`` of ``int``, optional):\n\n            A shape specification to perform the balancing in slabs of this\n            size. -1 can be used to refer to the actual size of the label\n            array. For example, a slab of::\n\n                (2, -1, -1, -1)\n\n            will perform the balancing for every each slice ``[0:2,:]``,\n            ``[2:4,:]``, ... individually.\n\n        num_classes(``int``, optional):\n\n            The number of classes. Labels will be expected to be in the\n            interval [0, ``num_classes``). Defaults to 2 for binary\n            classification.\n\n        clipmin (``float``, optional):\n\n            Clip class fraction to clipmin when calculating class weights.\n            Defaults to 0.05. Set to None if you do not want to clip min values.\n\n        clipmax (``float``, optional):\n\n            Clip class fraction to clipmax when calculating class weights.\n            Defaults to 0.95. Set to None, if you do not want to clip max\n            values.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        labels,\n        scales,\n        mask=None,\n        slab=None,\n        num_classes=2,\n        clipmin=0.05,\n        clipmax=0.95,\n    ):\n        self.labels = labels\n        self.scales = scales\n        if mask is None:\n            self.masks = []\n        elif not isinstance(mask, Iterable):\n            self.masks = [mask]\n        else:\n            self.masks = mask\n\n        self.slab = slab\n        self.num_classes = num_classes\n        self.clipmin = clipmin\n        self.clipmax = clipmax\n\n    def setup(self):\n        assert self.labels in self.spec, (\n            \"Asked to balance labels %s, which are not provided.\" % self.labels\n        )\n\n        for mask in self.masks:\n            assert mask in self.spec, (\n                \"Asked to apply mask %s to balance labels, but mask is not \"\n                \"provided.\" % mask\n            )\n\n        spec = self.spec[self.labels].copy()\n        spec.dtype = np.float32\n        self.provides(self.scales, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.labels] = request[self.scales]\n        for mask in self.masks:\n            deps[mask] = request[self.scales]\n        return deps\n\n    def process(self, batch, request):\n        labels = batch.arrays[self.labels]\n\n        assert len(np.unique(labels.data)) <= self.num_classes, (\n            \"Found more unique labels than classes in %s.\" % self.labels\n        )\n        assert 0 <= np.min(labels.data) < self.num_classes, (\n            \"Labels %s are not in [0, num_classes).\" % self.labels\n        )\n        assert 0 <= np.max(labels.data) < self.num_classes, (\n            \"Labels %s are not in [0, num_classes).\" % self.labels\n        )\n\n        # initialize error scale with 1s\n        error_scale = np.ones(labels.data.shape, dtype=np.float32)\n\n        # set error_scale to 0 in masked-out areas\n        for key in self.masks:\n            mask = batch.arrays[key]\n            assert (\n                labels.data.shape == mask.data.shape\n            ), \"Shape of mask %s %s does not match %s %s\" % (\n                mask,\n                mask.data.shape,\n                self.labels,\n                labels.data.shape,\n            )\n            error_scale *= mask.data\n\n        if not self.slab:\n            slab = error_scale.shape\n        else:\n            # slab with -1 replaced by shape\n            slab = tuple(\n                m if s == -1 else s for m, s in zip(error_scale.shape, self.slab)\n            )\n\n        slab_ranges = (range(0, m, s) for m, s in zip(error_scale.shape, slab))\n\n        for start in itertools.product(*slab_ranges):\n            slices = tuple(\n                slice(start[d], start[d] + slab[d]) for d in range(len(slab))\n            )\n            self.__balance(labels.data[slices], error_scale[slices])\n\n        spec = self.spec[self.scales].copy()\n        spec.roi = labels.spec.roi\n\n        outputs = Batch()\n        outputs[self.scales] = Array(error_scale, spec)\n        return outputs\n\n    def __balance(self, labels, scale):\n        labels = labels.astype(np.int64)\n\n        # in the masked-in area, compute the fraction of per-class samples\n        masked_in = scale.sum()\n        classes, counts = np.unique(labels[np.nonzero(scale)], return_counts=True)\n        fracs = (\n            counts.astype(float) / masked_in if masked_in > 0 else np.zeros(counts.size)\n        )\n        if self.clipmin is not None or self.clipmax is not None:\n            np.clip(fracs, self.clipmin, self.clipmax, fracs)\n\n        # compute the class weights\n        w_sparse = 1.0 / float(self.num_classes) / fracs\n        w = np.zeros(self.num_classes)\n        w[classes] = w_sparse\n\n        # scale the masked-in scale with the class weights\n        scale *= np.take(w, labels)",
  "def __init__(\n        self,\n        labels,\n        scales,\n        mask=None,\n        slab=None,\n        num_classes=2,\n        clipmin=0.05,\n        clipmax=0.95,\n    ):\n        self.labels = labels\n        self.scales = scales\n        if mask is None:\n            self.masks = []\n        elif not isinstance(mask, Iterable):\n            self.masks = [mask]\n        else:\n            self.masks = mask\n\n        self.slab = slab\n        self.num_classes = num_classes\n        self.clipmin = clipmin\n        self.clipmax = clipmax",
  "def setup(self):\n        assert self.labels in self.spec, (\n            \"Asked to balance labels %s, which are not provided.\" % self.labels\n        )\n\n        for mask in self.masks:\n            assert mask in self.spec, (\n                \"Asked to apply mask %s to balance labels, but mask is not \"\n                \"provided.\" % mask\n            )\n\n        spec = self.spec[self.labels].copy()\n        spec.dtype = np.float32\n        self.provides(self.scales, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.labels] = request[self.scales]\n        for mask in self.masks:\n            deps[mask] = request[self.scales]\n        return deps",
  "def process(self, batch, request):\n        labels = batch.arrays[self.labels]\n\n        assert len(np.unique(labels.data)) <= self.num_classes, (\n            \"Found more unique labels than classes in %s.\" % self.labels\n        )\n        assert 0 <= np.min(labels.data) < self.num_classes, (\n            \"Labels %s are not in [0, num_classes).\" % self.labels\n        )\n        assert 0 <= np.max(labels.data) < self.num_classes, (\n            \"Labels %s are not in [0, num_classes).\" % self.labels\n        )\n\n        # initialize error scale with 1s\n        error_scale = np.ones(labels.data.shape, dtype=np.float32)\n\n        # set error_scale to 0 in masked-out areas\n        for key in self.masks:\n            mask = batch.arrays[key]\n            assert (\n                labels.data.shape == mask.data.shape\n            ), \"Shape of mask %s %s does not match %s %s\" % (\n                mask,\n                mask.data.shape,\n                self.labels,\n                labels.data.shape,\n            )\n            error_scale *= mask.data\n\n        if not self.slab:\n            slab = error_scale.shape\n        else:\n            # slab with -1 replaced by shape\n            slab = tuple(\n                m if s == -1 else s for m, s in zip(error_scale.shape, self.slab)\n            )\n\n        slab_ranges = (range(0, m, s) for m, s in zip(error_scale.shape, slab))\n\n        for start in itertools.product(*slab_ranges):\n            slices = tuple(\n                slice(start[d], start[d] + slab[d]) for d in range(len(slab))\n            )\n            self.__balance(labels.data[slices], error_scale[slices])\n\n        spec = self.spec[self.scales].copy()\n        spec.roi = labels.spec.roi\n\n        outputs = Batch()\n        outputs[self.scales] = Array(error_scale, spec)\n        return outputs",
  "def __balance(self, labels, scale):\n        labels = labels.astype(np.int64)\n\n        # in the masked-in area, compute the fraction of per-class samples\n        masked_in = scale.sum()\n        classes, counts = np.unique(labels[np.nonzero(scale)], return_counts=True)\n        fracs = (\n            counts.astype(float) / masked_in if masked_in > 0 else np.zeros(counts.size)\n        )\n        if self.clipmin is not None or self.clipmax is not None:\n            np.clip(fracs, self.clipmin, self.clipmax, fracs)\n\n        # compute the class weights\n        w_sparse = 1.0 / float(self.num_classes) / fracs\n        w = np.zeros(self.num_classes)\n        w[classes] = w_sparse\n\n        # scale the masked-in scale with the class weights\n        scale *= np.take(w, labels)",
  "class IterateLocations(BatchFilter):\n    \"\"\"Iterates over the nodes in a graph and centers\n    batches at their locations. The iteration is thread safe.\n\n    Args:\n        graph (:class:`GraphKey`): Key of graph to read nodes from\n\n        roi (:class:`Roi`): Roi within which to read and iterate over nodes.\n            Defaults to None, which queries the whole Roi of the upstream graph\n            source\n\n        node_id (:class:`ArrayKey`, optional): Nonspatial array key in which to\n            store the id of the \"current\" node in graph.  Default is None, in\n            which case no attribute is stored and there is no way to tell which\n            node is being considered.\n\n        choose_randomly (bool): If true, choose nodes randomly with\n            replacement. Default is false, which loops over the list.\n    \"\"\"\n\n    __global_index = mp.Value(\"i\", -1)\n    visited_all = mp.Value(\"b\", False)\n\n    def __init__(self, graph, roi=None, node_id=None, choose_randomly=False):\n        self.graph = graph\n        self.roi = roi\n        self.node_id = node_id\n        self.choose_randomly = choose_randomly\n        self.nodes = None\n        self.coordinates = None\n        self.local_index = None\n        self.shift = None\n\n    def setup(self):\n        upstream = self.get_upstream_provider()\n        self.upstream_spec = upstream.spec\n        assert self.graph in self.upstream_spec, (\n            \"Upstream provider does not have graph %s\" % self.graph\n        )\n        query_spec = self.upstream_spec.graph_specs[self.graph].copy()\n        if self.roi:\n            query_spec.roi = query_spec.roi.intersect(self.roi)\n        # TODO: For scalability, scan upstream roi in blocks instead of\n        #       storing all nodes in memory\n        logger.info(\"Requesting all %s points in roi %s\", self.graph, query_spec.roi)\n        upstream_request = BatchRequest({self.graph: query_spec})\n        upstream_batch = upstream.request_batch(upstream_request)\n        self.nodes = list(upstream_batch[self.graph].nodes)\n        self.coordinates = [node.location for node in self.nodes]\n        assert (\n            len(self.coordinates) > 0\n        ), \"Graph  %s doesn't have nodes to iterate over in roi %s\" % (\n            self.graph,\n            self.roi,\n        )\n\n        # clear bounding boxes of all provided arrays and points\n        for key, spec in self.spec.items():\n            if spec.roi is not None:\n                spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n                self.updates(key, spec)\n        if self.node_id is not None:\n            self.provides(self.node_id, ArraySpec(nonspatial=True))\n\n    def prepare(self, request):\n        logger.debug(\"request: %s\", request.array_specs)\n        logger.debug(\"my spec: %s\", self.spec)\n\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n        if lcm_voxel_size is None:\n            ndims = len(self.coordinates[0])\n            lcm_voxel_size = Coordinate((1,) * ndims)\n\n        # shift to center\n        total_roi = request.get_total_roi()\n        request_center = total_roi.shape / 2 + total_roi.offset\n\n        self.shift = self._get_next_shift(request_center, lcm_voxel_size)\n        max_tries = 15\n        tries = 0\n        while not self.__check_shift(request):\n            logger.warning(\n                \"Location %s (shift %s) skipped\"\n                % (self.coordinates[self.local_index], self.shift)\n            )\n            assert tries < max_tries, (\n                \"Unable to find valid shift after %d tries\",\n                tries,\n            )\n            self.shift = self._get_next_shift(request_center, lcm_voxel_size)\n            tries += 1\n\n        # Set shift for all requests\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                if isinstance(spec, ArraySpec) and spec.nonspatial:\n                    continue\n                roi = spec.roi.shift(self.shift)\n                specs_type[key].roi = roi\n\n        logger.debug(\n            \"{}'th ({}) shift selected: {}\".format(\n                self.local_index, self.coordinates[self.local_index], self.shift\n            )\n        )\n\n    def process(self, batch, request):\n        if self.node_id:\n            node_id = self.nodes[self.local_index].id\n            spec = self.spec[self.node_id].copy()\n            batch[self.node_id] = Array([node_id], spec)\n\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n\n        for graph_key, spec in request.graph_specs.items():\n            batch.graphs[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.graphs[graph_key].shift(-self.shift)\n\n    def _get_next_shift(self, center_shift, voxel_size):\n        # gets next coordinate from list\n        if self.choose_randomly:\n            self.local_index = randrange(len(self.coordinates))\n        else:\n            with IterateLocations.__global_index.get_lock():\n                IterateLocations.__global_index.value += 1\n                if IterateLocations.__global_index.value == len(self.coordinates) - 1:\n                    logger.info(\"After this request, all points have been visited\")\n                    with IterateLocations.visited_all.get_lock():\n                        IterateLocations.visited_all.value = True\n                if IterateLocations.__global_index.value == len(self.coordinates):\n                    logger.warning(\"Ran out of locations, looping list\")\n                self.local_index = IterateLocations.__global_index.value % len(\n                    self.coordinates\n                )\n        next_shift = Coordinate(self.coordinates[self.local_index]) - center_shift\n\n        logger.debug(\"Shift before rounding: %s\" % str(next_shift))\n        # make sure shift is a multiple of voxel size (round to nearest)\n        next_shift = Coordinate(\n            [\n                int(vs * round(float(shift) / vs))\n                for vs, shift in zip(voxel_size, next_shift)\n            ]\n        )\n        logger.debug(\"Shift after rounding: %s\" % str(next_shift))\n        return next_shift\n\n    def __check_shift(self, request):\n        for key, spec in request.items():\n            if isinstance(spec, ArraySpec) and spec.nonspatial:\n                continue\n            request_roi = spec.roi\n            if key in self.upstream_spec:\n                provided_roi = self.upstream_spec[key].roi\n            else:\n                raise Exception(\"Requested %s, but upstream does not provide it.\" % key)\n            shifted_roi = request_roi.shift(self.shift)\n            if not provided_roi.contains(shifted_roi):\n                logger.warning(\n                    (\"Provided roi %s for key %s does notcontain\" \" shifted roi %s\"),\n                    provided_roi,\n                    key,\n                    shifted_roi,\n                )\n                return False\n        return True",
  "def __init__(self, graph, roi=None, node_id=None, choose_randomly=False):\n        self.graph = graph\n        self.roi = roi\n        self.node_id = node_id\n        self.choose_randomly = choose_randomly\n        self.nodes = None\n        self.coordinates = None\n        self.local_index = None\n        self.shift = None",
  "def setup(self):\n        upstream = self.get_upstream_provider()\n        self.upstream_spec = upstream.spec\n        assert self.graph in self.upstream_spec, (\n            \"Upstream provider does not have graph %s\" % self.graph\n        )\n        query_spec = self.upstream_spec.graph_specs[self.graph].copy()\n        if self.roi:\n            query_spec.roi = query_spec.roi.intersect(self.roi)\n        # TODO: For scalability, scan upstream roi in blocks instead of\n        #       storing all nodes in memory\n        logger.info(\"Requesting all %s points in roi %s\", self.graph, query_spec.roi)\n        upstream_request = BatchRequest({self.graph: query_spec})\n        upstream_batch = upstream.request_batch(upstream_request)\n        self.nodes = list(upstream_batch[self.graph].nodes)\n        self.coordinates = [node.location for node in self.nodes]\n        assert (\n            len(self.coordinates) > 0\n        ), \"Graph  %s doesn't have nodes to iterate over in roi %s\" % (\n            self.graph,\n            self.roi,\n        )\n\n        # clear bounding boxes of all provided arrays and points\n        for key, spec in self.spec.items():\n            if spec.roi is not None:\n                spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n                self.updates(key, spec)\n        if self.node_id is not None:\n            self.provides(self.node_id, ArraySpec(nonspatial=True))",
  "def prepare(self, request):\n        logger.debug(\"request: %s\", request.array_specs)\n        logger.debug(\"my spec: %s\", self.spec)\n\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n        if lcm_voxel_size is None:\n            ndims = len(self.coordinates[0])\n            lcm_voxel_size = Coordinate((1,) * ndims)\n\n        # shift to center\n        total_roi = request.get_total_roi()\n        request_center = total_roi.shape / 2 + total_roi.offset\n\n        self.shift = self._get_next_shift(request_center, lcm_voxel_size)\n        max_tries = 15\n        tries = 0\n        while not self.__check_shift(request):\n            logger.warning(\n                \"Location %s (shift %s) skipped\"\n                % (self.coordinates[self.local_index], self.shift)\n            )\n            assert tries < max_tries, (\n                \"Unable to find valid shift after %d tries\",\n                tries,\n            )\n            self.shift = self._get_next_shift(request_center, lcm_voxel_size)\n            tries += 1\n\n        # Set shift for all requests\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                if isinstance(spec, ArraySpec) and spec.nonspatial:\n                    continue\n                roi = spec.roi.shift(self.shift)\n                specs_type[key].roi = roi\n\n        logger.debug(\n            \"{}'th ({}) shift selected: {}\".format(\n                self.local_index, self.coordinates[self.local_index], self.shift\n            )\n        )",
  "def process(self, batch, request):\n        if self.node_id:\n            node_id = self.nodes[self.local_index].id\n            spec = self.spec[self.node_id].copy()\n            batch[self.node_id] = Array([node_id], spec)\n\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n\n        for graph_key, spec in request.graph_specs.items():\n            batch.graphs[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.graphs[graph_key].shift(-self.shift)",
  "def _get_next_shift(self, center_shift, voxel_size):\n        # gets next coordinate from list\n        if self.choose_randomly:\n            self.local_index = randrange(len(self.coordinates))\n        else:\n            with IterateLocations.__global_index.get_lock():\n                IterateLocations.__global_index.value += 1\n                if IterateLocations.__global_index.value == len(self.coordinates) - 1:\n                    logger.info(\"After this request, all points have been visited\")\n                    with IterateLocations.visited_all.get_lock():\n                        IterateLocations.visited_all.value = True\n                if IterateLocations.__global_index.value == len(self.coordinates):\n                    logger.warning(\"Ran out of locations, looping list\")\n                self.local_index = IterateLocations.__global_index.value % len(\n                    self.coordinates\n                )\n        next_shift = Coordinate(self.coordinates[self.local_index]) - center_shift\n\n        logger.debug(\"Shift before rounding: %s\" % str(next_shift))\n        # make sure shift is a multiple of voxel size (round to nearest)\n        next_shift = Coordinate(\n            [\n                int(vs * round(float(shift) / vs))\n                for vs, shift in zip(voxel_size, next_shift)\n            ]\n        )\n        logger.debug(\"Shift after rounding: %s\" % str(next_shift))\n        return next_shift",
  "def __check_shift(self, request):\n        for key, spec in request.items():\n            if isinstance(spec, ArraySpec) and spec.nonspatial:\n                continue\n            request_roi = spec.roi\n            if key in self.upstream_spec:\n                provided_roi = self.upstream_spec[key].roi\n            else:\n                raise Exception(\"Requested %s, but upstream does not provide it.\" % key)\n            shifted_roi = request_roi.shift(self.shift)\n            if not provided_roi.contains(shifted_roi):\n                logger.warning(\n                    (\"Provided roi %s for key %s does notcontain\" \" shifted roi %s\"),\n                    provided_roi,\n                    key,\n                    shifted_roi,\n                )\n                return False\n        return True",
  "class RandomLocation(BatchFilter):\n    \"\"\"Choses a batch at a random location in the bounding box of the upstream\n    provider.\n\n    The random location is chosen such that the batch request ROI lies entirely\n    inside the provider's ROI.\n\n    If ``min_masked`` and ``mask`` are set, only batches are returned that have\n    at least the given ratio of masked-in voxels. This is in general faster\n    than using the :class:`Reject` node, at the expense of storing an integral\n    array of the complete mask.\n\n    If ``ensure_nonempty`` is set to a :class:`GraphKey`, only batches are\n    returned that have at least one point of this point collection within the\n    requested ROI.\n\n    Additional tests for randomly picked locations can be implemented by\n    subclassing and overwriting of :func:`accepts`. This method takes the\n    randomly shifted request that meets all previous criteria (like\n    ``min_masked`` and ``ensure_nonempty``) and should return ``True`` if the\n    request is acceptable.\n\n    Args:\n\n        min_masked (``float``, optional):\n\n            If non-zero, require that the random sample contains at least that\n            ratio of masked-in voxels.\n\n        mask (:class:`ArrayKey`, optional):\n\n            The array to use for mask checks.\n\n        ensure_nonempty (:class:`GraphKey`, optional):\n\n            Ensures that when finding a random location, a request for\n            ``ensure_nonempty`` will contain at least one point.\n\n        p_nonempty (``float``, optional):\n\n            If ``ensure_nonempty`` is set, it defines the probability that a\n            request for ``ensure_nonempty`` will contain at least one point.\n            Default value is 1.0.\n\n        ensure_centered (``bool``, optional):\n\n            if ``ensure_nonempty`` is set, ``ensure_centered`` guarantees\n            that the center voxel of the roi contains a point.\n\n        point_balance_radius (``int``):\n\n            if ``ensure_nonempty`` is set, ``point_balance_radius`` defines\n            a radius s.t. for every point `p` in ``ensure_nonempty``, the\n            probability of picking p is inversely related to the number of\n            other points within a distance of ``point_balance_radius`` to p.\n            This helps avoid oversampling of dense regions of the graph, and\n            undersampling of sparse regions.\n\n        random_shift_key (``ArrayKey`` optional):\n\n            if ``random_shift_key`` is not None, this node will populate\n            that key with a nonspatial array containing the random shift\n            used for each request. This can be useful for snapshot iterations\n            if you want to figure out where that snapshot came from.\n    \"\"\"\n\n    def __init__(\n        self,\n        min_masked=0,\n        mask=None,\n        ensure_nonempty=None,\n        p_nonempty=1.0,\n        ensure_centered=None,\n        point_balance_radius=1,\n        random_shift_key=None,\n    ):\n        self.min_masked = min_masked\n        self.mask = mask\n        self.mask_spec = None\n        self.mask_integral = None\n        self.ensure_nonempty = ensure_nonempty\n        self.points = None\n        self.p_nonempty = p_nonempty\n        self.upstream_spec = None\n        self.random_shift = None\n        self.ensure_centered = ensure_centered\n        self.point_balance_radius = point_balance_radius\n        self.random_shift_key = random_shift_key\n\n    def setup(self):\n        upstream = self.get_upstream_provider()\n        self.upstream_spec = upstream.spec\n\n        if self.mask and self.min_masked > 0:\n            assert self.mask in self.upstream_spec, (\n                \"Upstream provider does not have %s\" % self.mask\n            )\n            self.mask_spec = self.upstream_spec.array_specs[self.mask]\n\n            logger.info(\"requesting complete mask...\")\n\n            mask_request = BatchRequest({self.mask: self.mask_spec})\n            mask_batch = upstream.request_batch(mask_request)\n\n            logger.info(\"allocating mask integral array...\")\n\n            mask_data = mask_batch.arrays[self.mask].data\n            mask_integral_dtype = np.uint64\n            logger.debug(\"mask size is %s\", mask_data.size)\n            if mask_data.size < 2**32:\n                mask_integral_dtype = np.uint32\n            if mask_data.size < 2**16:\n                mask_integral_dtype = np.uint16\n            logger.debug(\"chose %s as integral array dtype\", mask_integral_dtype)\n\n            self.mask_integral = np.array(mask_data > 0, dtype=mask_integral_dtype)\n            self.mask_integral = integral_image(self.mask_integral).astype(\n                mask_integral_dtype\n            )\n\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in self.upstream_spec, (\n                \"Upstream provider does not have %s\" % self.ensure_nonempty\n            )\n            graph_spec = self.upstream_spec.graph_specs[self.ensure_nonempty]\n\n            logger.info(\"requesting all %s points...\", self.ensure_nonempty)\n\n            nonempty_request = BatchRequest({self.ensure_nonempty: graph_spec})\n            nonempty_batch = upstream.request_batch(nonempty_request)\n\n            self.points = cKDTree(\n                [p.location for p in nonempty_batch[self.ensure_nonempty].nodes]\n            )\n\n            point_counts = self.points.query_ball_point(\n                [p.location for p in nonempty_batch[self.ensure_nonempty].nodes],\n                r=self.point_balance_radius,\n            )\n            weights = [1 / len(point_count) for point_count in point_counts]\n            self.cumulative_weights = list(itertools.accumulate(weights))\n\n            logger.debug(\"retrieved %d points\", len(self.points.data))\n\n        # clear bounding boxes of all provided arrays and points --\n        # RandomLocation does not have limits (offsets are ignored)\n        for key, spec in self.spec.items():\n            if spec.roi is not None:\n                spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n                self.updates(key, spec)\n\n        # provide randomness if asked for\n        if self.random_shift_key is not None:\n            self.provides(self.random_shift_key, ArraySpec(nonspatial=True))\n\n    def prepare(self, request):\n\n        logger.debug(\"request: %s\", request.array_specs)\n        logger.debug(\"my spec: %s\", self.spec)\n\n        if request.array_specs.keys():\n            lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n        else:\n            lcm_voxel_size = Coordinate((1,) * request.get_total_roi().dims)\n\n        shift_roi = self.__get_possible_shifts(request, lcm_voxel_size)\n\n        if request.array_specs.keys():\n            shift_roi = shift_roi.snap_to_grid(lcm_voxel_size, mode=\"shrink\")\n            lcm_shift_roi = shift_roi / lcm_voxel_size\n            logger.debug(\n                \"restricting random locations to multiples of voxel size %s\",\n                lcm_voxel_size,\n            )\n\n        else:\n            lcm_shift_roi = shift_roi\n\n        assert not lcm_shift_roi.unbounded, (\n            \"Can not pick a random location, intersection of upstream ROIs is \"\n            \"unbounded.\"\n        )\n        assert not lcm_shift_roi.empty, (\n            \"Can not satisfy batch request, no location covers all requested \" \"ROIs.\"\n        )\n\n        random_shift = self.__select_random_shift(\n            request, lcm_shift_roi, lcm_voxel_size\n        )\n\n        self.random_shift = random_shift\n        self.__shift_request(request, random_shift)\n\n        return request\n\n    def process(self, batch, request):\n        if self.random_shift_key is not None:\n            batch[self.random_shift_key] = Array(\n                np.array(self.random_shift),\n                ArraySpec(nonspatial=True),\n            )\n\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n        for graph_key, spec in request.graph_specs.items():\n            batch.graphs[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.graphs[graph_key].shift(-self.random_shift)\n\n    def accepts(self, request):\n        \"\"\"Should return True if the randomly chosen location is acceptable\n        (besided meeting other criteria like ``min_masked`` and/or\n        ``ensure_nonempty``). Subclasses can overwrite this method to implement\n        additional tests for acceptable locations.\"\"\"\n\n        return True\n\n    def __get_possible_shifts(self, request, voxel_size):\n        total_shift_roi = None\n\n        for key, spec in request.items():\n            if spec.roi is None:\n                continue\n\n            request_roi = spec.roi\n            provided_roi = self.upstream_spec[key].roi\n\n            shift_roi = provided_roi.shift(-request_roi.begin).grow(\n                (0,) * request_roi.dims, -(request_roi.shape - voxel_size)\n            )\n\n            if total_shift_roi is None:\n                total_shift_roi = shift_roi\n            else:\n                if shift_roi != total_shift_roi:\n                    total_shift_roi = total_shift_roi.intersect(shift_roi)\n\n        logger.debug(\"valid shifts for request in \" + str(total_shift_roi))\n\n        return total_shift_roi\n\n    def __select_random_shift(self, request, lcm_shift_roi, lcm_voxel_size):\n        ensure_points = self.ensure_nonempty is not None and random() <= self.p_nonempty\n\n        while True:\n            if ensure_points:\n                random_shift = self.__select_random_location_with_points(\n                    request, lcm_shift_roi, lcm_voxel_size\n                )\n            else:\n                random_shift = self.__select_random_location(\n                    lcm_shift_roi, lcm_voxel_size\n                )\n\n            logger.debug(\"random shift: \" + str(random_shift))\n\n            if not self.__is_min_masked(random_shift, request):\n                logger.debug(\"random location does not meet 'min_masked' criterium\")\n                continue\n\n            if not self.__accepts(random_shift, request):\n                logger.debug(\"random location does not meet user-provided criterium\")\n                continue\n\n            return random_shift\n\n    def __is_min_masked(self, random_shift, request):\n        if not self.mask or self.min_masked == 0:\n            return True\n\n        # get randomly chosen mask ROI\n        request_mask_roi = request.array_specs[self.mask].roi\n        request_mask_roi = request_mask_roi.shift(random_shift)\n\n        # get coordinates inside mask array\n        mask_voxel_size = self.spec[self.mask].voxel_size\n        request_mask_roi_in_array = request_mask_roi / mask_voxel_size\n        request_mask_roi_in_array -= self.mask_spec.roi.offset / mask_voxel_size\n\n        # get number of masked-in voxels\n        num_masked_in = integrate(\n            self.mask_integral,\n            [request_mask_roi_in_array.begin],\n            [\n                request_mask_roi_in_array.end\n                - Coordinate((1,) * self.mask_integral.ndim)\n            ],\n        )[0]\n\n        mask_ratio = float(num_masked_in) / request_mask_roi_in_array.size\n        logger.debug(\"mask ratio is %f\", mask_ratio)\n\n        return mask_ratio >= self.min_masked\n\n    def __accepts(self, random_shift, request):\n        # create a shifted copy of the request\n        shifted_request = request.copy()\n        self.__shift_request(shifted_request, random_shift)\n\n        return self.accepts(shifted_request)\n\n    def __shift_request(self, request, shift):\n        # shift request ROIs\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                if spec.roi is None:\n                    continue\n                roi = spec.roi.shift(shift)\n                specs_type[key].roi = roi\n\n    def __select_random_location_with_points(\n        self, request, lcm_shift_roi, lcm_voxel_size\n    ):\n        request_points = request.graph_specs.get(self.ensure_nonempty)\n        if request_points is None:\n            total_roi = request.get_total_roi()\n            logger.warning(\n                f\"Requesting non empty {self.ensure_nonempty}, however {self.ensure_nonempty} \"\n                f\"has not been requested. Falling back on using the total roi of the \"\n                f\"request {total_roi} for {self.ensure_nonempty}.\"\n            )\n            request_points_roi = total_roi\n        else:\n            request_points_roi = request_points.roi\n\n        while True:\n            # How to pick shifts that ensure that a randomly chosen point is\n            # contained in the request ROI:\n            #\n            #\n            # request          point\n            # [---------)      .\n            # 0        +10     17\n            #\n            #         least shifted to contain point\n            #         [---------)\n            #         8        +10\n            #         ==\n            #         point-request.begin-request.shape+1\n            #\n            #                  most shifted to contain point:\n            #                  [---------)\n            #                  17       +10\n            #                  ==\n            #                  point-request.begin\n            #\n            #         all possible shifts\n            #         [---------)\n            #         8        +10\n            #         ==\n            #         point-request.begin-request.shape+1\n            #                   ==\n            #                   request.shape\n\n            # pick a random point\n            point = choices(self.points.data, cum_weights=self.cumulative_weights)[0]\n\n            logger.debug(\"select random point at %s\", point)\n\n            # get the lcm voxel that contains this point\n            lcm_location = Coordinate(point / lcm_voxel_size)\n            logger.debug(\"belongs to lcm voxel %s\", lcm_location)\n\n            # align the point request ROI with lcm voxel grid\n            lcm_roi = request_points_roi.snap_to_grid(\n                lcm_voxel_size,\n                mode=\"shrink\")\n            lcm_roi = lcm_roi / lcm_voxel_size\n            logger.debug(\"Point request ROI: %s\", request_points_roi)\n            logger.debug(\"Point request lcm ROI shape: %s\", lcm_roi.shape)\n\n            # get all possible starting points of lcm_roi.shape that contain\n            # lcm_location\n            if self.ensure_centered:\n                lcm_shift_roi_begin = (\n                    lcm_location\n                    - lcm_roi.begin\n                    - lcm_roi.shape / 2\n                    + Coordinate((1,) * len(lcm_location))\n                )\n                lcm_shift_roi_shape = Coordinate((1,) * len(lcm_location))\n            else:\n                lcm_shift_roi_begin = (\n                    lcm_location\n                    - lcm_roi.begin\n                    - lcm_roi.shape\n                    + Coordinate((1,) * len(lcm_location))\n                )\n                lcm_shift_roi_shape = lcm_roi.shape\n            lcm_point_shift_roi = Roi(lcm_shift_roi_begin, lcm_shift_roi_shape)\n\n            logger.debug(\"lcm point shift roi: %s\", lcm_point_shift_roi)\n\n            # intersect with total shift ROI\n            if not lcm_point_shift_roi.intersects(lcm_shift_roi):\n                logger.debug(\n                    \"reject random shift, random point %s shift ROI %s does \"\n                    \"not intersect total shift ROI %s\",\n                    point,\n                    lcm_point_shift_roi,\n                    lcm_shift_roi,\n                )\n                continue\n            lcm_point_shift_roi = lcm_point_shift_roi.intersect(lcm_shift_roi)\n\n            # select a random shift from all possible shifts\n            random_shift = self.__select_random_location(\n                lcm_point_shift_roi, lcm_voxel_size\n            )\n            logger.debug(\"random shift: %s\", random_shift)\n\n            # count all points inside the shifted ROI\n            points = self.__get_points_in_roi(request_points_roi.shift(random_shift))\n            assert (\n                point in points\n            ), \"Requested batch to contain point %s, but got points \" \"%s\" % (\n                point,\n                points,\n            )\n            num_points = len(points)\n\n            return random_shift\n\n    def __select_random_location(self, lcm_shift_roi, lcm_voxel_size):\n        # select a random point inside ROI\n        random_shift = Coordinate(\n            randint(begin, end - 1)\n            for begin, end in zip(lcm_shift_roi.begin, lcm_shift_roi.end)\n        )\n\n        random_shift *= lcm_voxel_size\n\n        return random_shift\n\n    def __get_points_in_roi(self, roi):\n        points = []\n\n        center = roi.center\n        radius = math.ceil(float(max(roi.shape)) / 2)\n        candidates = self.points.query_ball_point(center, radius, p=np.inf)\n\n        for i in candidates:\n            if roi.contains(self.points.data[i]):\n                points.append(self.points.data[i])\n\n        return np.array(points)",
  "def __init__(\n        self,\n        min_masked=0,\n        mask=None,\n        ensure_nonempty=None,\n        p_nonempty=1.0,\n        ensure_centered=None,\n        point_balance_radius=1,\n        random_shift_key=None,\n    ):\n        self.min_masked = min_masked\n        self.mask = mask\n        self.mask_spec = None\n        self.mask_integral = None\n        self.ensure_nonempty = ensure_nonempty\n        self.points = None\n        self.p_nonempty = p_nonempty\n        self.upstream_spec = None\n        self.random_shift = None\n        self.ensure_centered = ensure_centered\n        self.point_balance_radius = point_balance_radius\n        self.random_shift_key = random_shift_key",
  "def setup(self):\n        upstream = self.get_upstream_provider()\n        self.upstream_spec = upstream.spec\n\n        if self.mask and self.min_masked > 0:\n            assert self.mask in self.upstream_spec, (\n                \"Upstream provider does not have %s\" % self.mask\n            )\n            self.mask_spec = self.upstream_spec.array_specs[self.mask]\n\n            logger.info(\"requesting complete mask...\")\n\n            mask_request = BatchRequest({self.mask: self.mask_spec})\n            mask_batch = upstream.request_batch(mask_request)\n\n            logger.info(\"allocating mask integral array...\")\n\n            mask_data = mask_batch.arrays[self.mask].data\n            mask_integral_dtype = np.uint64\n            logger.debug(\"mask size is %s\", mask_data.size)\n            if mask_data.size < 2**32:\n                mask_integral_dtype = np.uint32\n            if mask_data.size < 2**16:\n                mask_integral_dtype = np.uint16\n            logger.debug(\"chose %s as integral array dtype\", mask_integral_dtype)\n\n            self.mask_integral = np.array(mask_data > 0, dtype=mask_integral_dtype)\n            self.mask_integral = integral_image(self.mask_integral).astype(\n                mask_integral_dtype\n            )\n\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in self.upstream_spec, (\n                \"Upstream provider does not have %s\" % self.ensure_nonempty\n            )\n            graph_spec = self.upstream_spec.graph_specs[self.ensure_nonempty]\n\n            logger.info(\"requesting all %s points...\", self.ensure_nonempty)\n\n            nonempty_request = BatchRequest({self.ensure_nonempty: graph_spec})\n            nonempty_batch = upstream.request_batch(nonempty_request)\n\n            self.points = cKDTree(\n                [p.location for p in nonempty_batch[self.ensure_nonempty].nodes]\n            )\n\n            point_counts = self.points.query_ball_point(\n                [p.location for p in nonempty_batch[self.ensure_nonempty].nodes],\n                r=self.point_balance_radius,\n            )\n            weights = [1 / len(point_count) for point_count in point_counts]\n            self.cumulative_weights = list(itertools.accumulate(weights))\n\n            logger.debug(\"retrieved %d points\", len(self.points.data))\n\n        # clear bounding boxes of all provided arrays and points --\n        # RandomLocation does not have limits (offsets are ignored)\n        for key, spec in self.spec.items():\n            if spec.roi is not None:\n                spec.roi.shape = Coordinate((None,) * spec.roi.dims)\n                self.updates(key, spec)\n\n        # provide randomness if asked for\n        if self.random_shift_key is not None:\n            self.provides(self.random_shift_key, ArraySpec(nonspatial=True))",
  "def prepare(self, request):\n\n        logger.debug(\"request: %s\", request.array_specs)\n        logger.debug(\"my spec: %s\", self.spec)\n\n        if request.array_specs.keys():\n            lcm_voxel_size = self.spec.get_lcm_voxel_size(request.array_specs.keys())\n        else:\n            lcm_voxel_size = Coordinate((1,) * request.get_total_roi().dims)\n\n        shift_roi = self.__get_possible_shifts(request, lcm_voxel_size)\n\n        if request.array_specs.keys():\n            shift_roi = shift_roi.snap_to_grid(lcm_voxel_size, mode=\"shrink\")\n            lcm_shift_roi = shift_roi / lcm_voxel_size\n            logger.debug(\n                \"restricting random locations to multiples of voxel size %s\",\n                lcm_voxel_size,\n            )\n\n        else:\n            lcm_shift_roi = shift_roi\n\n        assert not lcm_shift_roi.unbounded, (\n            \"Can not pick a random location, intersection of upstream ROIs is \"\n            \"unbounded.\"\n        )\n        assert not lcm_shift_roi.empty, (\n            \"Can not satisfy batch request, no location covers all requested \" \"ROIs.\"\n        )\n\n        random_shift = self.__select_random_shift(\n            request, lcm_shift_roi, lcm_voxel_size\n        )\n\n        self.random_shift = random_shift\n        self.__shift_request(request, random_shift)\n\n        return request",
  "def process(self, batch, request):\n        if self.random_shift_key is not None:\n            batch[self.random_shift_key] = Array(\n                np.array(self.random_shift),\n                ArraySpec(nonspatial=True),\n            )\n\n        # reset ROIs to request\n        for array_key, spec in request.array_specs.items():\n            batch.arrays[array_key].spec.roi = spec.roi\n        for graph_key, spec in request.graph_specs.items():\n            batch.graphs[graph_key].spec.roi = spec.roi\n\n        # change shift point locations to lie within roi\n        for graph_key in request.graph_specs.keys():\n            batch.graphs[graph_key].shift(-self.random_shift)",
  "def accepts(self, request):\n        \"\"\"Should return True if the randomly chosen location is acceptable\n        (besided meeting other criteria like ``min_masked`` and/or\n        ``ensure_nonempty``). Subclasses can overwrite this method to implement\n        additional tests for acceptable locations.\"\"\"\n\n        return True",
  "def __get_possible_shifts(self, request, voxel_size):\n        total_shift_roi = None\n\n        for key, spec in request.items():\n            if spec.roi is None:\n                continue\n\n            request_roi = spec.roi\n            provided_roi = self.upstream_spec[key].roi\n\n            shift_roi = provided_roi.shift(-request_roi.begin).grow(\n                (0,) * request_roi.dims, -(request_roi.shape - voxel_size)\n            )\n\n            if total_shift_roi is None:\n                total_shift_roi = shift_roi\n            else:\n                if shift_roi != total_shift_roi:\n                    total_shift_roi = total_shift_roi.intersect(shift_roi)\n\n        logger.debug(\"valid shifts for request in \" + str(total_shift_roi))\n\n        return total_shift_roi",
  "def __select_random_shift(self, request, lcm_shift_roi, lcm_voxel_size):\n        ensure_points = self.ensure_nonempty is not None and random() <= self.p_nonempty\n\n        while True:\n            if ensure_points:\n                random_shift = self.__select_random_location_with_points(\n                    request, lcm_shift_roi, lcm_voxel_size\n                )\n            else:\n                random_shift = self.__select_random_location(\n                    lcm_shift_roi, lcm_voxel_size\n                )\n\n            logger.debug(\"random shift: \" + str(random_shift))\n\n            if not self.__is_min_masked(random_shift, request):\n                logger.debug(\"random location does not meet 'min_masked' criterium\")\n                continue\n\n            if not self.__accepts(random_shift, request):\n                logger.debug(\"random location does not meet user-provided criterium\")\n                continue\n\n            return random_shift",
  "def __is_min_masked(self, random_shift, request):\n        if not self.mask or self.min_masked == 0:\n            return True\n\n        # get randomly chosen mask ROI\n        request_mask_roi = request.array_specs[self.mask].roi\n        request_mask_roi = request_mask_roi.shift(random_shift)\n\n        # get coordinates inside mask array\n        mask_voxel_size = self.spec[self.mask].voxel_size\n        request_mask_roi_in_array = request_mask_roi / mask_voxel_size\n        request_mask_roi_in_array -= self.mask_spec.roi.offset / mask_voxel_size\n\n        # get number of masked-in voxels\n        num_masked_in = integrate(\n            self.mask_integral,\n            [request_mask_roi_in_array.begin],\n            [\n                request_mask_roi_in_array.end\n                - Coordinate((1,) * self.mask_integral.ndim)\n            ],\n        )[0]\n\n        mask_ratio = float(num_masked_in) / request_mask_roi_in_array.size\n        logger.debug(\"mask ratio is %f\", mask_ratio)\n\n        return mask_ratio >= self.min_masked",
  "def __accepts(self, random_shift, request):\n        # create a shifted copy of the request\n        shifted_request = request.copy()\n        self.__shift_request(shifted_request, random_shift)\n\n        return self.accepts(shifted_request)",
  "def __shift_request(self, request, shift):\n        # shift request ROIs\n        for specs_type in [request.array_specs, request.graph_specs]:\n            for key, spec in specs_type.items():\n                if spec.roi is None:\n                    continue\n                roi = spec.roi.shift(shift)\n                specs_type[key].roi = roi",
  "def __select_random_location_with_points(\n        self, request, lcm_shift_roi, lcm_voxel_size\n    ):\n        request_points = request.graph_specs.get(self.ensure_nonempty)\n        if request_points is None:\n            total_roi = request.get_total_roi()\n            logger.warning(\n                f\"Requesting non empty {self.ensure_nonempty}, however {self.ensure_nonempty} \"\n                f\"has not been requested. Falling back on using the total roi of the \"\n                f\"request {total_roi} for {self.ensure_nonempty}.\"\n            )\n            request_points_roi = total_roi\n        else:\n            request_points_roi = request_points.roi\n\n        while True:\n            # How to pick shifts that ensure that a randomly chosen point is\n            # contained in the request ROI:\n            #\n            #\n            # request          point\n            # [---------)      .\n            # 0        +10     17\n            #\n            #         least shifted to contain point\n            #         [---------)\n            #         8        +10\n            #         ==\n            #         point-request.begin-request.shape+1\n            #\n            #                  most shifted to contain point:\n            #                  [---------)\n            #                  17       +10\n            #                  ==\n            #                  point-request.begin\n            #\n            #         all possible shifts\n            #         [---------)\n            #         8        +10\n            #         ==\n            #         point-request.begin-request.shape+1\n            #                   ==\n            #                   request.shape\n\n            # pick a random point\n            point = choices(self.points.data, cum_weights=self.cumulative_weights)[0]\n\n            logger.debug(\"select random point at %s\", point)\n\n            # get the lcm voxel that contains this point\n            lcm_location = Coordinate(point / lcm_voxel_size)\n            logger.debug(\"belongs to lcm voxel %s\", lcm_location)\n\n            # align the point request ROI with lcm voxel grid\n            lcm_roi = request_points_roi.snap_to_grid(\n                lcm_voxel_size,\n                mode=\"shrink\")\n            lcm_roi = lcm_roi / lcm_voxel_size\n            logger.debug(\"Point request ROI: %s\", request_points_roi)\n            logger.debug(\"Point request lcm ROI shape: %s\", lcm_roi.shape)\n\n            # get all possible starting points of lcm_roi.shape that contain\n            # lcm_location\n            if self.ensure_centered:\n                lcm_shift_roi_begin = (\n                    lcm_location\n                    - lcm_roi.begin\n                    - lcm_roi.shape / 2\n                    + Coordinate((1,) * len(lcm_location))\n                )\n                lcm_shift_roi_shape = Coordinate((1,) * len(lcm_location))\n            else:\n                lcm_shift_roi_begin = (\n                    lcm_location\n                    - lcm_roi.begin\n                    - lcm_roi.shape\n                    + Coordinate((1,) * len(lcm_location))\n                )\n                lcm_shift_roi_shape = lcm_roi.shape\n            lcm_point_shift_roi = Roi(lcm_shift_roi_begin, lcm_shift_roi_shape)\n\n            logger.debug(\"lcm point shift roi: %s\", lcm_point_shift_roi)\n\n            # intersect with total shift ROI\n            if not lcm_point_shift_roi.intersects(lcm_shift_roi):\n                logger.debug(\n                    \"reject random shift, random point %s shift ROI %s does \"\n                    \"not intersect total shift ROI %s\",\n                    point,\n                    lcm_point_shift_roi,\n                    lcm_shift_roi,\n                )\n                continue\n            lcm_point_shift_roi = lcm_point_shift_roi.intersect(lcm_shift_roi)\n\n            # select a random shift from all possible shifts\n            random_shift = self.__select_random_location(\n                lcm_point_shift_roi, lcm_voxel_size\n            )\n            logger.debug(\"random shift: %s\", random_shift)\n\n            # count all points inside the shifted ROI\n            points = self.__get_points_in_roi(request_points_roi.shift(random_shift))\n            assert (\n                point in points\n            ), \"Requested batch to contain point %s, but got points \" \"%s\" % (\n                point,\n                points,\n            )\n            num_points = len(points)\n\n            return random_shift",
  "def __select_random_location(self, lcm_shift_roi, lcm_voxel_size):\n        # select a random point inside ROI\n        random_shift = Coordinate(\n            randint(begin, end - 1)\n            for begin, end in zip(lcm_shift_roi.begin, lcm_shift_roi.end)\n        )\n\n        random_shift *= lcm_voxel_size\n\n        return random_shift",
  "def __get_points_in_roi(self, roi):\n        points = []\n\n        center = roi.center\n        radius = math.ceil(float(max(roi.shape)) / 2)\n        candidates = self.points.query_ball_point(center, radius, p=np.inf)\n\n        for i in candidates:\n            if roi.contains(self.points.data[i]):\n                points.append(self.points.data[i])\n\n        return np.array(points)",
  "class WorkersDiedException(Exception):\n    pass",
  "class PreCache(BatchFilter):\n    \"\"\"Pre-cache repeated equal batch requests. For the first of a series of\n    equal batch request, a set of workers is spawned to pre-cache the batches\n    in parallel processes. This way, subsequent requests can be served quickly.\n\n    A note on changing the requests sent to `PreCache`.\n    Given requests A and B, if requests are sent in the sequence:\n    A, ..., A, B, A, ..., A, B, A, ...\n    Precache will build a Queue of batches that satisfy A, and handle requests\n    B on demand. This prevents `PreCache` from discarding the queue on every\n    SnapshotRequest.\n    However if B request replace A as the most common request, i.e.:\n    A, A, A, ..., A, B, B, B, ...,\n    `PreCache` will discard the A queue and build a B queue after it has seen\n    more B requests than A requests out of the last 5 requests.\n\n    This node only makes sense if:\n\n    1. Incoming batch requests are repeatedly the same.\n    2. There is a source of randomness in upstream nodes.\n\n    Args:\n\n        cache_size (``int``):\n\n            How many batches to hold at most in the cache.\n\n        num_workers (``int``):\n\n            How many processes to spawn to fill the cache.\n    \"\"\"\n\n    def __init__(self, cache_size=50, num_workers=20):\n        self.current_request = None\n        self.workers = None\n        self.cache_size = cache_size\n        self.num_workers = num_workers\n\n        # keep track of recent requests\n        self.last_5 = deque(\n            [\n                None,\n            ]\n            * 5,\n            maxlen=5,\n        )\n\n    def teardown(self):\n        if self.workers is not None:\n            self.workers.stop()\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        # update recent requests\n        self.last_5.popleft()\n        self.last_5.append(request)\n\n        if request != self.current_request:\n            current_count = sum(\n                [\n                    recent_request == self.current_request\n                    for recent_request in self.last_5\n                ]\n            )\n            new_count = sum(\n                [recent_request == request for recent_request in self.last_5]\n            )\n            if new_count > current_count or self.current_request is None:\n                if self.workers is not None:\n                    logger.info(\"new request received, stopping current workers...\")\n                    self.workers.stop()\n\n                self.current_request = request.copy()\n\n                logger.info(\n                    \"starting new set of workers (%s, cache size %s)...\",\n                    self.num_workers,\n                    self.cache_size,\n                )\n                self.workers = ProducerPool(\n                    [self._run_worker for _ in range(self.num_workers)],\n                    queue_size=self.cache_size,\n                )\n                self.workers.start()\n\n                logger.debug(\"getting batch from queue...\")\n                batch = self.workers.get()\n\n                timing.stop()\n                batch.profiling_stats.add(timing)\n\n            else:\n                logger.debug(\"Resolving new request sequentially\")\n                batch = self.get_upstream_provider().request_batch(request)\n\n                timing.stop()\n                batch.profiling_stats.add(timing)\n\n        else:\n            logger.debug(\"getting batch from queue...\")\n            batch = self.workers.get()\n\n            timing.stop()\n            batch.profiling_stats.add(timing)\n\n        return batch\n\n    def _run_worker(self):\n        request = self.current_request.copy()\n        return self.get_upstream_provider().request_batch(request)",
  "def __init__(self, cache_size=50, num_workers=20):\n        self.current_request = None\n        self.workers = None\n        self.cache_size = cache_size\n        self.num_workers = num_workers\n\n        # keep track of recent requests\n        self.last_5 = deque(\n            [\n                None,\n            ]\n            * 5,\n            maxlen=5,\n        )",
  "def teardown(self):\n        if self.workers is not None:\n            self.workers.stop()",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        # update recent requests\n        self.last_5.popleft()\n        self.last_5.append(request)\n\n        if request != self.current_request:\n            current_count = sum(\n                [\n                    recent_request == self.current_request\n                    for recent_request in self.last_5\n                ]\n            )\n            new_count = sum(\n                [recent_request == request for recent_request in self.last_5]\n            )\n            if new_count > current_count or self.current_request is None:\n                if self.workers is not None:\n                    logger.info(\"new request received, stopping current workers...\")\n                    self.workers.stop()\n\n                self.current_request = request.copy()\n\n                logger.info(\n                    \"starting new set of workers (%s, cache size %s)...\",\n                    self.num_workers,\n                    self.cache_size,\n                )\n                self.workers = ProducerPool(\n                    [self._run_worker for _ in range(self.num_workers)],\n                    queue_size=self.cache_size,\n                )\n                self.workers.start()\n\n                logger.debug(\"getting batch from queue...\")\n                batch = self.workers.get()\n\n                timing.stop()\n                batch.profiling_stats.add(timing)\n\n            else:\n                logger.debug(\"Resolving new request sequentially\")\n                batch = self.get_upstream_provider().request_batch(request)\n\n                timing.stop()\n                batch.profiling_stats.add(timing)\n\n        else:\n            logger.debug(\"getting batch from queue...\")\n            batch = self.workers.get()\n\n            timing.stop()\n            batch.profiling_stats.add(timing)\n\n        return batch",
  "def _run_worker(self):\n        request = self.current_request.copy()\n        return self.get_upstream_provider().request_batch(request)",
  "class DefectAugment(BatchFilter):\n    \"\"\"Augment intensity arrays section-wise with artifacts like missing\n    sections, low-contrast sections, by blending in artifacts drawn from a\n    separate source, or by deforming a section.\n\n    Args:\n\n        intensities (:class:`ArrayKey`):\n\n            The key of the array of intensities to modify.\n\n        prob_missing(``float``):\n        prob_low_contrast(``float``):\n        prob_artifact(``float``):\n        prob_deform(``float``):\n\n            Probabilities of having a missing section, low-contrast section, an\n            artifact (see param ``artifact_source``) or a deformed slice. The\n            sum should not exceed 1. Values in missing sections will be set to\n            0.\n\n        contrast_scale (``float``, optional):\n\n            By how much to scale the intensities for a low-contrast section,\n            used if ``prob_low_contrast`` > 0.\n\n        artifact_source (class:`BatchProvider`, optional):\n\n            A gunpowder batch provider that delivers intensities (via\n            :class:`ArrayKey` ``artifacts``) and an alpha mask (via\n            :class:`ArrayKey` ``artifacts_mask``), used if ``prob_artifact`` > 0.\n\n        artifacts(:class:`ArrayKey`, optional):\n\n            The key to query ``artifact_source`` for to get the intensities\n            of the artifacts.\n\n        artifacts_mask(:class:`ArrayKey`, optional):\n\n            The key to query ``artifact_source`` for to get the alpha mask\n            of the artifacts to blend them with ``intensities``.\n\n        deformation_strength (``int``, optional):\n\n            Strength of the slice deformation in voxels, used if\n            ``prob_deform`` > 0. The deformation models a fold by shifting the\n            section contents towards a randomly oriented line in the section.\n            The line itself will be drawn with a value of 0.\n\n        axis (``int``, optional):\n\n            Along which axis sections are cut.\n    \"\"\"\n\n    def __init__(\n        self,\n        intensities,\n        prob_missing=0.05,\n        prob_low_contrast=0.05,\n        prob_artifact=0.0,\n        prob_deform=0.0,\n        contrast_scale=0.1,\n        artifact_source=None,\n        artifacts=None,\n        artifacts_mask=None,\n        deformation_strength=20,\n        axis=0,\n    ):\n        self.intensities = intensities\n        self.prob_missing = prob_missing\n        self.prob_low_contrast = prob_low_contrast\n        self.prob_artifact = prob_artifact\n        self.prob_deform = prob_deform\n        self.contrast_scale = contrast_scale\n        self.artifact_source = artifact_source\n        self.artifacts = artifacts\n        self.artifacts_mask = artifacts_mask\n        self.deformation_strength = deformation_strength\n        self.axis = axis\n\n    def setup(self):\n        if self.artifact_source is not None:\n            self.artifact_source.setup()\n\n    def teardown(self):\n        if self.artifact_source is not None:\n            self.artifact_source.teardown()\n\n    # send roi request to data-source upstream\n    def prepare(self, request):\n        deps = BatchRequest()\n\n        # we prepare the augmentations, by determining which slices\n        # will be augmented by which method\n        # If one of the slices is augmented with 'deform',\n        # we prepare these trafos already\n        # and request a bigger roi from upstream\n\n        prob_missing_threshold = self.prob_missing\n        prob_low_contrast_threshold = prob_missing_threshold + self.prob_low_contrast\n        prob_artifact_threshold = prob_low_contrast_threshold + self.prob_artifact\n        prob_deform_slice = prob_artifact_threshold + self.prob_deform\n\n        spec = request[self.intensities].copy()\n        roi = spec.roi\n        logger.debug(\"downstream request ROI is %s\" % roi)\n        raw_voxel_size = self.spec[self.intensities].voxel_size\n\n        # store the mapping slice to augmentation type in a dict\n        self.slice_to_augmentation = {}\n        # store the transformations for deform slice\n        self.deform_slice_transformations = {}\n        for c in range((roi / raw_voxel_size).shape[self.axis]):\n            r = random.random()\n\n            if r < prob_missing_threshold:\n                logger.debug(\"Zero-out \" + str(c))\n                self.slice_to_augmentation[c] = \"zero_out\"\n\n            elif r < prob_low_contrast_threshold:\n                logger.debug(\"Lower contrast \" + str(c))\n                self.slice_to_augmentation[c] = \"lower_contrast\"\n\n            elif r < prob_artifact_threshold:\n                logger.debug(\"Add artifact \" + str(c))\n                self.slice_to_augmentation[c] = \"artifact\"\n\n            elif r < prob_deform_slice:\n                logger.debug(\"Add deformed slice \" + str(c))\n                self.slice_to_augmentation[c] = \"deformed_slice\"\n                # get the shape of a single slice\n                slice_shape = (roi / raw_voxel_size).shape\n                slice_shape = slice_shape[: self.axis] + slice_shape[self.axis + 1 :]\n                self.deform_slice_transformations[c] = self.__prepare_deform_slice(\n                    slice_shape\n                )\n\n        # prepare transformation and\n        # request bigger upstream roi for deformed slice\n        if \"deformed_slice\" in self.slice_to_augmentation.values():\n            # create roi sufficiently large to feed deformation\n            logger.debug(\"before growth: %s\" % spec.roi)\n            growth = Coordinate(\n                tuple(\n                    0\n                    if d == self.axis\n                    else raw_voxel_size[d] * self.deformation_strength\n                    for d in range(spec.roi.dims)\n                )\n            )\n            logger.debug(\"growing request by %s\" % str(growth))\n            source_roi = roi.grow(growth, growth)\n\n            # update request ROI to get all voxels necessary to perfrom\n            # transformation\n            spec.roi = source_roi\n            logger.debug(\"upstream request roi is %s\" % spec.roi)\n\n        deps[self.intensities] = spec\n\n    def process(self, batch, request):\n        assert batch.get_total_roi().dims == 3, \"defectaugment works on 3d batches only\"\n\n        raw = batch.arrays[self.intensities]\n        raw_voxel_size = self.spec[self.intensities].voxel_size\n\n        for c, augmentation_type in self.slice_to_augmentation.items():\n            section_selector = tuple(\n                slice(None if d != self.axis else c, None if d != self.axis else c + 1)\n                for d in range(raw.spec.roi.dims)\n            )\n\n            if augmentation_type == \"zero_out\":\n                raw.data[section_selector] = 0\n\n            elif augmentation_type == \"low_contrast\":\n                section = raw.data[section_selector]\n\n                mean = section.mean()\n                section -= mean\n                section *= self.contrast_scale\n                section += mean\n\n                raw.data[section_selector] = section\n\n            elif augmentation_type == \"artifact\":\n                section = raw.data[section_selector]\n\n                alpha_voxel_size = self.artifact_source.spec[\n                    self.artifacts_mask\n                ].voxel_size\n\n                assert raw_voxel_size == alpha_voxel_size, (\n                    \"Can only alpha blend RAW with \"\n                    \"ALPHA_MASK if both have the same \"\n                    \"voxel size\"\n                )\n\n                artifact_request = BatchRequest()\n                artifact_request.add(\n                    self.artifacts,\n                    Coordinate(section.shape) * raw_voxel_size,\n                    voxel_size=raw_voxel_size,\n                )\n                artifact_request.add(\n                    self.artifacts_mask,\n                    Coordinate(section.shape) * alpha_voxel_size,\n                    voxel_size=raw_voxel_size,\n                )\n                logger.debug(\"Requesting artifact batch %s\", artifact_request)\n\n                artifact_batch = self.artifact_source.request_batch(artifact_request)\n                artifact_alpha = artifact_batch.arrays[self.artifacts_mask].data\n                artifact_raw = artifact_batch.arrays[self.artifacts].data\n\n                assert artifact_alpha.dtype == np.float32\n                assert artifact_alpha.min() >= 0.0\n                assert artifact_alpha.max() <= 1.0\n\n                raw.data[section_selector] = (\n                    section * (1.0 - artifact_alpha) + artifact_raw * artifact_alpha\n                )\n\n            elif augmentation_type == \"deformed_slice\":\n                section = raw.data[section_selector].squeeze()\n\n                # set interpolation to cubic, spec interploatable is true, else to 0\n                interpolation = 3 if self.spec[self.intensities].interpolatable else 0\n\n                # load the deformation fields that were prepared for this slice\n                flow_x, flow_y, line_mask = self.deform_slice_transformations[c]\n\n                # apply the deformation fields\n                shape = section.shape\n                section = map_coordinates(\n                    section, (flow_y, flow_x), mode=\"constant\", order=interpolation\n                ).reshape(shape)\n\n                # things can get smaller than 0 at the boundary, so we clip\n                section = np.clip(section, 0.0, 1.0)\n\n                # zero-out data below the line mask\n                section[line_mask] = 0.0\n\n                raw.data[section_selector] = section\n\n        # in case we needed to change the ROI due to a deformation augment,\n        # restore original ROI and crop the array data\n        if \"deformed_slice\" in self.slice_to_augmentation.values():\n            old_roi = request[self.intensities].roi\n            logger.debug(\"resetting roi to %s\" % old_roi)\n            crop = tuple(\n                slice(None)\n                if d == self.axis\n                else slice(self.deformation_strength, -self.deformation_strength)\n                for d in range(raw.spec.roi.dims)\n            )\n            raw.data = raw.data[crop]\n            raw.spec.roi = old_roi\n\n    def __prepare_deform_slice(self, slice_shape):\n        # grow slice shape by 2 x deformation strength\n        grow_by = 2 * self.deformation_strength\n        shape = (slice_shape[0] + grow_by, slice_shape[1] + grow_by)\n\n        # randomly choose fixed x or fixed y with p = 1/2\n        fixed_x = random.random() < 0.5\n        if fixed_x:\n            x0, y0 = 0, np.random.randint(1, shape[1] - 2)\n            x1, y1 = shape[0] - 1, np.random.randint(1, shape[1] - 2)\n        else:\n            x0, y0 = np.random.randint(1, shape[0] - 2), 0\n            x1, y1 = np.random.randint(1, shape[0] - 2), shape[1] - 1\n\n        ## generate the mask of the line that should be blacked out\n        line_mask = np.zeros(shape, dtype=\"bool\")\n        rr, cc = line(x0, y0, x1, y1)\n        line_mask[rr, cc] = 1\n\n        # generate vectorfield pointing towards the line to compress the image\n        # first we get the unit vector representing the line\n        line_vector = np.array([x1 - x0, y1 - y0], dtype=\"float32\")\n        line_vector /= np.linalg.norm(line_vector)\n        # next, we generate the normal to the line\n        normal_vector = np.zeros_like(line_vector)\n        normal_vector[0] = -line_vector[1]\n        normal_vector[1] = line_vector[0]\n\n        # make meshgrid\n        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n        # generate the vector field\n        flow_x, flow_y = np.zeros(shape), np.zeros(shape)\n\n        # find the 2 components where coordinates are bigger / smaller than the line\n        # to apply normal vector in the correct direction\n        components, n_components = label(np.logical_not(line_mask).view(\"uint8\"))\n        assert n_components == 2, \"%i\" % n_components\n        neg_val = components[0, 0] if fixed_x else components[-1, -1]\n        pos_val = components[-1, -1] if fixed_x else components[0, 0]\n\n        flow_x[components == pos_val] = self.deformation_strength * normal_vector[1]\n        flow_y[components == pos_val] = self.deformation_strength * normal_vector[0]\n        flow_x[components == neg_val] = -self.deformation_strength * normal_vector[1]\n        flow_y[components == neg_val] = -self.deformation_strength * normal_vector[0]\n\n        # generate the flow fields\n        flow_x, flow_y = (x + flow_x).reshape(-1, 1), (y + flow_y).reshape(-1, 1)\n\n        # dilate the line mask\n        line_mask = binary_dilation(line_mask, iterations=10)\n\n        return flow_x, flow_y, line_mask",
  "def __init__(\n        self,\n        intensities,\n        prob_missing=0.05,\n        prob_low_contrast=0.05,\n        prob_artifact=0.0,\n        prob_deform=0.0,\n        contrast_scale=0.1,\n        artifact_source=None,\n        artifacts=None,\n        artifacts_mask=None,\n        deformation_strength=20,\n        axis=0,\n    ):\n        self.intensities = intensities\n        self.prob_missing = prob_missing\n        self.prob_low_contrast = prob_low_contrast\n        self.prob_artifact = prob_artifact\n        self.prob_deform = prob_deform\n        self.contrast_scale = contrast_scale\n        self.artifact_source = artifact_source\n        self.artifacts = artifacts\n        self.artifacts_mask = artifacts_mask\n        self.deformation_strength = deformation_strength\n        self.axis = axis",
  "def setup(self):\n        if self.artifact_source is not None:\n            self.artifact_source.setup()",
  "def teardown(self):\n        if self.artifact_source is not None:\n            self.artifact_source.teardown()",
  "def prepare(self, request):\n        deps = BatchRequest()\n\n        # we prepare the augmentations, by determining which slices\n        # will be augmented by which method\n        # If one of the slices is augmented with 'deform',\n        # we prepare these trafos already\n        # and request a bigger roi from upstream\n\n        prob_missing_threshold = self.prob_missing\n        prob_low_contrast_threshold = prob_missing_threshold + self.prob_low_contrast\n        prob_artifact_threshold = prob_low_contrast_threshold + self.prob_artifact\n        prob_deform_slice = prob_artifact_threshold + self.prob_deform\n\n        spec = request[self.intensities].copy()\n        roi = spec.roi\n        logger.debug(\"downstream request ROI is %s\" % roi)\n        raw_voxel_size = self.spec[self.intensities].voxel_size\n\n        # store the mapping slice to augmentation type in a dict\n        self.slice_to_augmentation = {}\n        # store the transformations for deform slice\n        self.deform_slice_transformations = {}\n        for c in range((roi / raw_voxel_size).shape[self.axis]):\n            r = random.random()\n\n            if r < prob_missing_threshold:\n                logger.debug(\"Zero-out \" + str(c))\n                self.slice_to_augmentation[c] = \"zero_out\"\n\n            elif r < prob_low_contrast_threshold:\n                logger.debug(\"Lower contrast \" + str(c))\n                self.slice_to_augmentation[c] = \"lower_contrast\"\n\n            elif r < prob_artifact_threshold:\n                logger.debug(\"Add artifact \" + str(c))\n                self.slice_to_augmentation[c] = \"artifact\"\n\n            elif r < prob_deform_slice:\n                logger.debug(\"Add deformed slice \" + str(c))\n                self.slice_to_augmentation[c] = \"deformed_slice\"\n                # get the shape of a single slice\n                slice_shape = (roi / raw_voxel_size).shape\n                slice_shape = slice_shape[: self.axis] + slice_shape[self.axis + 1 :]\n                self.deform_slice_transformations[c] = self.__prepare_deform_slice(\n                    slice_shape\n                )\n\n        # prepare transformation and\n        # request bigger upstream roi for deformed slice\n        if \"deformed_slice\" in self.slice_to_augmentation.values():\n            # create roi sufficiently large to feed deformation\n            logger.debug(\"before growth: %s\" % spec.roi)\n            growth = Coordinate(\n                tuple(\n                    0\n                    if d == self.axis\n                    else raw_voxel_size[d] * self.deformation_strength\n                    for d in range(spec.roi.dims)\n                )\n            )\n            logger.debug(\"growing request by %s\" % str(growth))\n            source_roi = roi.grow(growth, growth)\n\n            # update request ROI to get all voxels necessary to perfrom\n            # transformation\n            spec.roi = source_roi\n            logger.debug(\"upstream request roi is %s\" % spec.roi)\n\n        deps[self.intensities] = spec",
  "def process(self, batch, request):\n        assert batch.get_total_roi().dims == 3, \"defectaugment works on 3d batches only\"\n\n        raw = batch.arrays[self.intensities]\n        raw_voxel_size = self.spec[self.intensities].voxel_size\n\n        for c, augmentation_type in self.slice_to_augmentation.items():\n            section_selector = tuple(\n                slice(None if d != self.axis else c, None if d != self.axis else c + 1)\n                for d in range(raw.spec.roi.dims)\n            )\n\n            if augmentation_type == \"zero_out\":\n                raw.data[section_selector] = 0\n\n            elif augmentation_type == \"low_contrast\":\n                section = raw.data[section_selector]\n\n                mean = section.mean()\n                section -= mean\n                section *= self.contrast_scale\n                section += mean\n\n                raw.data[section_selector] = section\n\n            elif augmentation_type == \"artifact\":\n                section = raw.data[section_selector]\n\n                alpha_voxel_size = self.artifact_source.spec[\n                    self.artifacts_mask\n                ].voxel_size\n\n                assert raw_voxel_size == alpha_voxel_size, (\n                    \"Can only alpha blend RAW with \"\n                    \"ALPHA_MASK if both have the same \"\n                    \"voxel size\"\n                )\n\n                artifact_request = BatchRequest()\n                artifact_request.add(\n                    self.artifacts,\n                    Coordinate(section.shape) * raw_voxel_size,\n                    voxel_size=raw_voxel_size,\n                )\n                artifact_request.add(\n                    self.artifacts_mask,\n                    Coordinate(section.shape) * alpha_voxel_size,\n                    voxel_size=raw_voxel_size,\n                )\n                logger.debug(\"Requesting artifact batch %s\", artifact_request)\n\n                artifact_batch = self.artifact_source.request_batch(artifact_request)\n                artifact_alpha = artifact_batch.arrays[self.artifacts_mask].data\n                artifact_raw = artifact_batch.arrays[self.artifacts].data\n\n                assert artifact_alpha.dtype == np.float32\n                assert artifact_alpha.min() >= 0.0\n                assert artifact_alpha.max() <= 1.0\n\n                raw.data[section_selector] = (\n                    section * (1.0 - artifact_alpha) + artifact_raw * artifact_alpha\n                )\n\n            elif augmentation_type == \"deformed_slice\":\n                section = raw.data[section_selector].squeeze()\n\n                # set interpolation to cubic, spec interploatable is true, else to 0\n                interpolation = 3 if self.spec[self.intensities].interpolatable else 0\n\n                # load the deformation fields that were prepared for this slice\n                flow_x, flow_y, line_mask = self.deform_slice_transformations[c]\n\n                # apply the deformation fields\n                shape = section.shape\n                section = map_coordinates(\n                    section, (flow_y, flow_x), mode=\"constant\", order=interpolation\n                ).reshape(shape)\n\n                # things can get smaller than 0 at the boundary, so we clip\n                section = np.clip(section, 0.0, 1.0)\n\n                # zero-out data below the line mask\n                section[line_mask] = 0.0\n\n                raw.data[section_selector] = section\n\n        # in case we needed to change the ROI due to a deformation augment,\n        # restore original ROI and crop the array data\n        if \"deformed_slice\" in self.slice_to_augmentation.values():\n            old_roi = request[self.intensities].roi\n            logger.debug(\"resetting roi to %s\" % old_roi)\n            crop = tuple(\n                slice(None)\n                if d == self.axis\n                else slice(self.deformation_strength, -self.deformation_strength)\n                for d in range(raw.spec.roi.dims)\n            )\n            raw.data = raw.data[crop]\n            raw.spec.roi = old_roi",
  "def __prepare_deform_slice(self, slice_shape):\n        # grow slice shape by 2 x deformation strength\n        grow_by = 2 * self.deformation_strength\n        shape = (slice_shape[0] + grow_by, slice_shape[1] + grow_by)\n\n        # randomly choose fixed x or fixed y with p = 1/2\n        fixed_x = random.random() < 0.5\n        if fixed_x:\n            x0, y0 = 0, np.random.randint(1, shape[1] - 2)\n            x1, y1 = shape[0] - 1, np.random.randint(1, shape[1] - 2)\n        else:\n            x0, y0 = np.random.randint(1, shape[0] - 2), 0\n            x1, y1 = np.random.randint(1, shape[0] - 2), shape[1] - 1\n\n        ## generate the mask of the line that should be blacked out\n        line_mask = np.zeros(shape, dtype=\"bool\")\n        rr, cc = line(x0, y0, x1, y1)\n        line_mask[rr, cc] = 1\n\n        # generate vectorfield pointing towards the line to compress the image\n        # first we get the unit vector representing the line\n        line_vector = np.array([x1 - x0, y1 - y0], dtype=\"float32\")\n        line_vector /= np.linalg.norm(line_vector)\n        # next, we generate the normal to the line\n        normal_vector = np.zeros_like(line_vector)\n        normal_vector[0] = -line_vector[1]\n        normal_vector[1] = line_vector[0]\n\n        # make meshgrid\n        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n        # generate the vector field\n        flow_x, flow_y = np.zeros(shape), np.zeros(shape)\n\n        # find the 2 components where coordinates are bigger / smaller than the line\n        # to apply normal vector in the correct direction\n        components, n_components = label(np.logical_not(line_mask).view(\"uint8\"))\n        assert n_components == 2, \"%i\" % n_components\n        neg_val = components[0, 0] if fixed_x else components[-1, -1]\n        pos_val = components[-1, -1] if fixed_x else components[0, 0]\n\n        flow_x[components == pos_val] = self.deformation_strength * normal_vector[1]\n        flow_y[components == pos_val] = self.deformation_strength * normal_vector[0]\n        flow_x[components == neg_val] = -self.deformation_strength * normal_vector[1]\n        flow_y[components == neg_val] = -self.deformation_strength * normal_vector[0]\n\n        # generate the flow fields\n        flow_x, flow_y = (x + flow_x).reshape(-1, 1), (y + flow_y).reshape(-1, 1)\n\n        # dilate the line mask\n        line_mask = binary_dilation(line_mask, iterations=10)\n\n        return flow_x, flow_y, line_mask",
  "class KlbSource(BatchProvider):\n    \"\"\"A `KLB <https://bitbucket.org/fernandoamat/keller-lab-block-filetype>`_\n    data source.\n\n    Provides a single array from the given KLB dataset.\n\n    Args:\n\n        filename (``string``):\n\n            The name of the KLB file. This string can be a glob expression\n            (e.g., ``frame_*.klb``), in which case all files that match are\n            sorted and stacked together to form an additional dimension (like\n            time). The additional dimension will start at 0 and have a default\n            voxel size of 1 (which can be overwritten using the ``array_spec``\n            argument).\n\n        array (:class:`ArrayKey`):\n\n            ArrayKey that this source offers.\n\n        array_spec (:class:`ArraySpec`, optional):\n\n            An optional :class:`ArraySpec` to overwrite the array specs\n            automatically determined from the KLB file. This is useful to set\n            ``voxel_size``, for example. Only fields that are not ``None`` in\n            the given :class:`ArraySpec` will be used.\n\n        num_threads (``int``):\n\n            An optional integer to pass to pyklb reader indicating the number\n            of threads to use when reading klb files. Entering None causes\n            uses the pyklb default, which now is based on the number of cores\n            in the machine. This pyklb default is bad for jobs on the cluster that\n            are limited to the number of cores requested, and 1 is recommended.\n\n    \"\"\"\n\n    def __init__(self, filename, array, array_spec=None, num_threads=1):\n        self.filename = filename\n        self.array = array\n        self.array_spec = array_spec\n        self.num_threads = num_threads\n\n        self.files = None\n        self.ndims = None\n\n    def setup(self):\n        self.files = glob.glob(self.filename)\n        self.files.sort()\n\n        logger.info(\"Reading KLB headers of %d files...\", len(self.files))\n        headers = [pyklb.readheader(f) for f in self.files]\n        spec = self.__read_spec(headers)\n\n        self.provides(self.array, spec)\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        request_spec = request[self.array]\n\n        logger.debug(\"Reading %s in %s...\", self.array, request_spec.roi)\n\n        voxel_size = self.spec[self.array].voxel_size\n\n        # scale request roi to voxel units\n        dataset_roi = request_spec.roi / voxel_size\n\n        # shift request roi into dataset\n        dataset_roi = dataset_roi - self.spec[self.array].roi.offset / voxel_size\n\n        # create array spec\n        array_spec = self.spec[self.array].copy()\n        array_spec.roi = request_spec.roi\n\n        # add array to batch\n        batch.arrays[self.array] = Array(self.__read(dataset_roi), array_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def __read_spec(self, headers):\n        num_files = len(headers)\n        assert num_files > 0\n        common_header = headers[0]\n        for header in headers:\n            for attr in [\"imagesize_tczyx\", \"pixelspacing_tczyx\"]:\n                assert (common_header[attr] == header[attr]).all(), (\n                    \"Headers of provided KLB files differ in attribute %s\" % attr\n                )\n            assert (\n                common_header[\"datatype\"] == header[\"datatype\"]\n            ), \"Headers of provided KLB files differ in attribute datatype\"\n\n        size = Coordinate(common_header[\"imagesize_tczyx\"])\n        voxel_size = Coordinate(common_header[\"pixelspacing_tczyx\"])\n        dtype = common_header[\"datatype\"]\n\n        # strip leading 1 dimensions\n        while size[0] == 1 and len(size) > 1:\n            size = size[1:]\n            voxel_size = voxel_size[1:]\n\n        # append num_files dimension\n        if num_files > 1:\n            size = (num_files,) + size\n            voxel_size = (1,) + voxel_size\n\n        dims = Coordinate(size)\n        self.ndims = len(dims)\n\n        if self.array_spec is not None:\n            spec = self.array_spec\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            spec.voxel_size = Coordinate(voxel_size)\n\n        if spec.roi is None:\n            offset = Coordinate((0,) * self.ndims)\n            spec.roi = Roi(offset, dims * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dtype, (\n                \"dtype %s provided in array_specs for %s, but differs from \"\n                \"dataset dtype %s\"\n                % (self.array_specs[self.array].dtype, self.array, dataset.dtype)\n            )\n        else:\n            spec.dtype = dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s. \"\n                \"Based on the dtype %s, it has been set to %s. \"\n                \"This might not be what you want.\",\n                self.array,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec\n\n    def __read(self, roi):\n        if len(self.files) == 1:\n            return self.__read_file(self.files[0], roi)\n\n        else:\n            file_indices = range(roi.begin[0], roi.end[0])\n\n            file_roi = Roi(roi.begin[1:], roi.shape[1:])\n\n            return np.array(\n                [self.__read_file(self.files[i], file_roi) for i in file_indices]\n            )\n\n    def __read_file(self, filename, roi):\n        # pyklb reads max-inclusive, gunpowder rois are max exclusive ->\n        # subtract (1, 1, ...) from max coordinate\n        if self.num_threads:\n            return pyklb.readroi(\n                filename,\n                roi.begin,\n                roi.end - (1,) * roi.dims,\n                numthreads=self.num_threads,\n            )\n        else:\n            return pyklb.readroi(filename, roi.begin, roi.end - (1,) * roi.dims)\n\n    def __repr__(self):\n        return self.filename",
  "def __init__(self, filename, array, array_spec=None, num_threads=1):\n        self.filename = filename\n        self.array = array\n        self.array_spec = array_spec\n        self.num_threads = num_threads\n\n        self.files = None\n        self.ndims = None",
  "def setup(self):\n        self.files = glob.glob(self.filename)\n        self.files.sort()\n\n        logger.info(\"Reading KLB headers of %d files...\", len(self.files))\n        headers = [pyklb.readheader(f) for f in self.files]\n        spec = self.__read_spec(headers)\n\n        self.provides(self.array, spec)",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        request_spec = request[self.array]\n\n        logger.debug(\"Reading %s in %s...\", self.array, request_spec.roi)\n\n        voxel_size = self.spec[self.array].voxel_size\n\n        # scale request roi to voxel units\n        dataset_roi = request_spec.roi / voxel_size\n\n        # shift request roi into dataset\n        dataset_roi = dataset_roi - self.spec[self.array].roi.offset / voxel_size\n\n        # create array spec\n        array_spec = self.spec[self.array].copy()\n        array_spec.roi = request_spec.roi\n\n        # add array to batch\n        batch.arrays[self.array] = Array(self.__read(dataset_roi), array_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __read_spec(self, headers):\n        num_files = len(headers)\n        assert num_files > 0\n        common_header = headers[0]\n        for header in headers:\n            for attr in [\"imagesize_tczyx\", \"pixelspacing_tczyx\"]:\n                assert (common_header[attr] == header[attr]).all(), (\n                    \"Headers of provided KLB files differ in attribute %s\" % attr\n                )\n            assert (\n                common_header[\"datatype\"] == header[\"datatype\"]\n            ), \"Headers of provided KLB files differ in attribute datatype\"\n\n        size = Coordinate(common_header[\"imagesize_tczyx\"])\n        voxel_size = Coordinate(common_header[\"pixelspacing_tczyx\"])\n        dtype = common_header[\"datatype\"]\n\n        # strip leading 1 dimensions\n        while size[0] == 1 and len(size) > 1:\n            size = size[1:]\n            voxel_size = voxel_size[1:]\n\n        # append num_files dimension\n        if num_files > 1:\n            size = (num_files,) + size\n            voxel_size = (1,) + voxel_size\n\n        dims = Coordinate(size)\n        self.ndims = len(dims)\n\n        if self.array_spec is not None:\n            spec = self.array_spec\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            spec.voxel_size = Coordinate(voxel_size)\n\n        if spec.roi is None:\n            offset = Coordinate((0,) * self.ndims)\n            spec.roi = Roi(offset, dims * spec.voxel_size)\n\n        if spec.dtype is not None:\n            assert spec.dtype == dtype, (\n                \"dtype %s provided in array_specs for %s, but differs from \"\n                \"dataset dtype %s\"\n                % (self.array_specs[self.array].dtype, self.array, dataset.dtype)\n            )\n        else:\n            spec.dtype = dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s. \"\n                \"Based on the dtype %s, it has been set to %s. \"\n                \"This might not be what you want.\",\n                self.array,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec",
  "def __read(self, roi):\n        if len(self.files) == 1:\n            return self.__read_file(self.files[0], roi)\n\n        else:\n            file_indices = range(roi.begin[0], roi.end[0])\n\n            file_roi = Roi(roi.begin[1:], roi.shape[1:])\n\n            return np.array(\n                [self.__read_file(self.files[i], file_roi) for i in file_indices]\n            )",
  "def __read_file(self, filename, roi):\n        # pyklb reads max-inclusive, gunpowder rois are max exclusive ->\n        # subtract (1, 1, ...) from max coordinate\n        if self.num_threads:\n            return pyklb.readroi(\n                filename,\n                roi.begin,\n                roi.end - (1,) * roi.dims,\n                numthreads=self.num_threads,\n            )\n        else:\n            return pyklb.readroi(filename, roi.begin, roi.end - (1,) * roi.dims)",
  "def __repr__(self):\n        return self.filename",
  "class Crop(BatchFilter):\n    \"\"\"Limits provided ROIs by either giving a new :class:`Roi` or crop\n    fractions from either face of the provided ROI.\n\n    Args:\n\n        key (:class:`ArrayKey` or :class:`GraphKey`):\n\n            The key of the array or points set to modify.\n\n        roi (:class:`Roi` or ``None``):\n\n            The ROI to crop to.\n\n        fraction_negative (``tuple`` of ``float``):\n\n            Relative crop starting from the negative end of the provided ROI.\n\n        fraction_positive (``tuple`` of ``float``):\n\n            Relative crop starting from the positive end of the provided ROI.\n    \"\"\"\n\n    def __init__(self, key, roi=None, fraction_negative=None, fraction_positive=None):\n        if roi is not None and (\n            fraction_positive is not None or fraction_negative is not None\n        ):\n            raise RuntimeError(\n                \"'roi' and 'fraction_...' arguments can not be given together\"\n            )\n\n        if (roi, fraction_positive, fraction_negative) == (None, None, None):\n            raise RuntimeError(\"One of 'roi' and 'fraction_...' has to be given\")\n\n        if fraction_negative is not None and fraction_positive is None:\n            fraction_positive = (0.0,) * len(fraction_negative)\n        if fraction_positive is not None and fraction_negative is None:\n            fraction_negative = (0.0,) * len(fraction_positive)\n\n        self.key = key\n        self.roi = roi\n        self.fraction_negative = fraction_negative\n        self.fraction_positive = fraction_positive\n\n    def setup(self):\n        spec = self.spec[self.key]\n\n        if self.roi is not None:\n            assert spec.roi.contains(\n                self.roi\n            ), \"Crop ROI is not contained in upstream ROI.\"\n\n            cropped_roi = self.roi\n\n        else:\n            total_fraction = tuple(\n                n + p for n, p in zip(self.fraction_negative, self.fraction_positive)\n            )\n            if max(total_fraction) >= 1:\n                raise RuntimeError(\"Sum of crop fractions exeeds 1\")\n\n            crop_positive = Coordinate(\n                a * b for a, b in zip(spec.roi.shape, self.fraction_positive)\n            )\n            crop_negative = Coordinate(\n                a * b for a, b in zip(spec.roi.shape, self.fraction_negative)\n            )\n            cropped_roi = spec.roi.grow(-crop_positive, -crop_negative)\n\n        spec.roi = cropped_roi\n        self.updates(self.key, spec)\n\n    def process(self, batch, request):\n        pass",
  "def __init__(self, key, roi=None, fraction_negative=None, fraction_positive=None):\n        if roi is not None and (\n            fraction_positive is not None or fraction_negative is not None\n        ):\n            raise RuntimeError(\n                \"'roi' and 'fraction_...' arguments can not be given together\"\n            )\n\n        if (roi, fraction_positive, fraction_negative) == (None, None, None):\n            raise RuntimeError(\"One of 'roi' and 'fraction_...' has to be given\")\n\n        if fraction_negative is not None and fraction_positive is None:\n            fraction_positive = (0.0,) * len(fraction_negative)\n        if fraction_positive is not None and fraction_negative is None:\n            fraction_negative = (0.0,) * len(fraction_positive)\n\n        self.key = key\n        self.roi = roi\n        self.fraction_negative = fraction_negative\n        self.fraction_positive = fraction_positive",
  "def setup(self):\n        spec = self.spec[self.key]\n\n        if self.roi is not None:\n            assert spec.roi.contains(\n                self.roi\n            ), \"Crop ROI is not contained in upstream ROI.\"\n\n            cropped_roi = self.roi\n\n        else:\n            total_fraction = tuple(\n                n + p for n, p in zip(self.fraction_negative, self.fraction_positive)\n            )\n            if max(total_fraction) >= 1:\n                raise RuntimeError(\"Sum of crop fractions exeeds 1\")\n\n            crop_positive = Coordinate(\n                a * b for a, b in zip(spec.roi.shape, self.fraction_positive)\n            )\n            crop_negative = Coordinate(\n                a * b for a, b in zip(spec.roi.shape, self.fraction_negative)\n            )\n            cropped_roi = spec.roi.grow(-crop_positive, -crop_negative)\n\n        spec.roi = cropped_roi\n        self.updates(self.key, spec)",
  "def process(self, batch, request):\n        pass",
  "class ZarrWrite(BatchFilter):\n    \"\"\"Assemble arrays of passing batches in one zarr container. This is useful\n    to store chunks produced by :class:`Scan` on disk without keeping the\n    larger array in memory. The ROIs of the passing arrays will be used to\n    determine the position where to store the data in the dataset.\n\n    Args:\n\n        dataset_names (``dict``, :class:`ArrayKey` -> ``string``):\n\n            A dictionary from array keys to names of the datasets to store them\n            in.\n\n        store (``string`` or ``BaseStore``):\n\n            The directory to save the zarr container. Will be created, if it does\n            not exist.\n\n        compression_type (``string`` or ``int``):\n\n            Compression strategy.  Legal values are ``gzip``, ``szip``,\n            ``lzf``. If an integer between 1 and 10, this indicates ``gzip``\n            compression level.\n\n        dataset_dtypes (``dict``, :class:`ArrayKey` -> data type):\n\n            A dictionary from array keys to datatype (eg. ``np.int8``). If\n            given, arrays are stored using this type. The original arrays\n            within the pipeline remain unchanged.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_names,\n        output_dir=\".\",\n        output_filename=\"output.hdf\",\n        compression_type=None,\n        dataset_dtypes=None,\n        store: Union[BaseStore, MutableMapping, str] = None,\n    ):\n        self.store = store if store is not None else f\"{output_dir}/{output_filename}\"\n        if store is None:\n            warnings.warn(\n                \"Argument 'output_dir' and `output_filename` will be replaced in v2.0, \"\n                \"use 'store' instead\",\n                DeprecationWarning,\n            )\n        self.dataset_names = dataset_names\n        self.compression_type = compression_type\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.dataset_offsets = {}\n\n    def _get_voxel_size(self, dataset):\n        if \"resolution\" not in dataset.attrs:\n            return None\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"resolution\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"resolution\"])\n\n    def _get_offset(self, dataset):\n        if \"offset\" not in dataset.attrs:\n            return None\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"offset\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"offset\"])\n\n    def _set_voxel_size(self, dataset, voxel_size):\n        if self._rev_metadata():\n            dataset.attrs[\"resolution\"] = voxel_size[::-1]\n        else:\n            dataset.attrs[\"resolution\"] = voxel_size\n\n    def _set_offset(self, dataset, offset):\n        if self._rev_metadata():\n            dataset.attrs[\"offset\"] = offset[::-1]\n        else:\n            dataset.attrs[\"offset\"] = offset\n\n    def _rev_metadata(self):\n        with ZarrFile(self.store, mode=\"a\") as store:\n            return isinstance(store, N5Store) or isinstance(store, N5FSStore)\n\n    def _open_file(self, store):\n        return ZarrFile(store, mode=\"a\")\n\n    def setup(self):\n        for key in self.dataset_names.keys():\n            self.updates(key, self.spec[key])\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.dataset_names.keys():\n            deps[key] = request[key]\n        return deps\n\n    def init_datasets(self, batch):\n        with self._open_file(self.store) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                logger.debug(\"Initializing dataset for %s\", array_key)\n\n                assert array_key in self.spec, (\n                    \"Asked to store %s, but is not provided upstream.\" % array_key\n                )\n                assert array_key in batch.arrays, (\n                    \"Asked to store %s, but is not part of batch.\" % array_key\n                )\n\n                array = batch.arrays[array_key]\n                dims = array.spec.roi.dims\n                batch_shape = array.data.shape\n\n                # if a dataset already exists, read its meta-information (if\n                # present)\n                if dataset_name in data_file:\n                    offset = self._get_offset(data_file[dataset_name]) or Coordinate(\n                        (0,) * dims\n                    )\n\n                else:\n                    provided_roi = self.spec[array_key].roi\n\n                    if provided_roi is None:\n                        raise RuntimeError(\n                            \"Dataset %s does not exist in %s, and no ROI is \"\n                            \"provided for %s. I don't know how to initialize \"\n                            \"the dataset.\" % (dataset_name, self.store, array_key)\n                        )\n\n                    offset = provided_roi.offset\n                    voxel_size = array.spec.voxel_size\n                    data_shape = provided_roi.shape // voxel_size\n\n                    logger.debug(\"Shape in voxels: %s\", data_shape)\n                    # add channel dimensions (if present)\n                    data_shape = batch_shape[:-dims] + data_shape\n                    logger.debug(\"Shape with channel dimensions: %s\", data_shape)\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                    else:\n                        dtype = batch.arrays[array_key].data.dtype\n\n                    logger.debug(\n                        \"create_dataset: %s, %s, %s, %s, offset=%s, resolution=%s\",\n                        dataset_name,\n                        data_shape,\n                        self.compression_type,\n                        dtype,\n                        offset,\n                        voxel_size,\n                    )\n\n                    dataset = data_file.create_dataset(\n                        name=dataset_name,\n                        shape=data_shape,\n                        compression=self.compression_type,\n                        dtype=dtype,\n                    )\n\n                    self._set_offset(dataset, offset)\n                    self._set_voxel_size(dataset, voxel_size)\n\n                logger.debug(\n                    \"%s (%s in %s) has offset %s\",\n                    array_key,\n                    dataset_name,\n                    self.store,\n                    offset,\n                )\n                self.dataset_offsets[array_key] = offset\n\n    def process(self, batch, request):\n        if not self.dataset_offsets:\n            self.init_datasets(batch)\n\n        with self._open_file(self.store) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                dataset = data_file[dataset_name]\n\n                array_roi = batch.arrays[array_key].spec.roi\n                voxel_size = self.spec[array_key].voxel_size\n                dims = array_roi.dims\n                channel_slices = (slice(None),) * max(0, len(dataset.shape) - dims)\n\n                dataset_roi = Roi(\n                    self.dataset_offsets[array_key],\n                    Coordinate(dataset.shape[-dims:]) * voxel_size,\n                )\n                common_roi = array_roi.intersect(dataset_roi)\n\n                if common_roi.empty:\n                    logger.warn(\n                        \"array %s with ROI %s lies outside of dataset ROI %s, \"\n                        \"skipping writing\" % (array_key, array_roi, dataset_roi)\n                    )\n                    continue\n\n                dataset_voxel_roi = (\n                    common_roi - self.dataset_offsets[array_key]\n                ) // voxel_size\n                dataset_voxel_slices = dataset_voxel_roi.to_slices()\n                array_voxel_roi = (common_roi - array_roi.offset) // voxel_size\n                array_voxel_slices = array_voxel_roi.to_slices()\n\n                logger.debug(\n                    \"writing %s to voxel coordinates %s\"\n                    % (array_key, dataset_voxel_roi)\n                )\n\n                data = batch.arrays[array_key].data[channel_slices + array_voxel_slices]\n                dataset[channel_slices + dataset_voxel_slices] = data",
  "def __init__(\n        self,\n        dataset_names,\n        output_dir=\".\",\n        output_filename=\"output.hdf\",\n        compression_type=None,\n        dataset_dtypes=None,\n        store: Union[BaseStore, MutableMapping, str] = None,\n    ):\n        self.store = store if store is not None else f\"{output_dir}/{output_filename}\"\n        if store is None:\n            warnings.warn(\n                \"Argument 'output_dir' and `output_filename` will be replaced in v2.0, \"\n                \"use 'store' instead\",\n                DeprecationWarning,\n            )\n        self.dataset_names = dataset_names\n        self.compression_type = compression_type\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.dataset_offsets = {}",
  "def _get_voxel_size(self, dataset):\n        if \"resolution\" not in dataset.attrs:\n            return None\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"resolution\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"resolution\"])",
  "def _get_offset(self, dataset):\n        if \"offset\" not in dataset.attrs:\n            return None\n        if self._rev_metadata():\n            return Coordinate(dataset.attrs[\"offset\"][::-1])\n        else:\n            return Coordinate(dataset.attrs[\"offset\"])",
  "def _set_voxel_size(self, dataset, voxel_size):\n        if self._rev_metadata():\n            dataset.attrs[\"resolution\"] = voxel_size[::-1]\n        else:\n            dataset.attrs[\"resolution\"] = voxel_size",
  "def _set_offset(self, dataset, offset):\n        if self._rev_metadata():\n            dataset.attrs[\"offset\"] = offset[::-1]\n        else:\n            dataset.attrs[\"offset\"] = offset",
  "def _rev_metadata(self):\n        with ZarrFile(self.store, mode=\"a\") as store:\n            return isinstance(store, N5Store) or isinstance(store, N5FSStore)",
  "def _open_file(self, store):\n        return ZarrFile(store, mode=\"a\")",
  "def setup(self):\n        for key in self.dataset_names.keys():\n            self.updates(key, self.spec[key])\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.dataset_names.keys():\n            deps[key] = request[key]\n        return deps",
  "def init_datasets(self, batch):\n        with self._open_file(self.store) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                logger.debug(\"Initializing dataset for %s\", array_key)\n\n                assert array_key in self.spec, (\n                    \"Asked to store %s, but is not provided upstream.\" % array_key\n                )\n                assert array_key in batch.arrays, (\n                    \"Asked to store %s, but is not part of batch.\" % array_key\n                )\n\n                array = batch.arrays[array_key]\n                dims = array.spec.roi.dims\n                batch_shape = array.data.shape\n\n                # if a dataset already exists, read its meta-information (if\n                # present)\n                if dataset_name in data_file:\n                    offset = self._get_offset(data_file[dataset_name]) or Coordinate(\n                        (0,) * dims\n                    )\n\n                else:\n                    provided_roi = self.spec[array_key].roi\n\n                    if provided_roi is None:\n                        raise RuntimeError(\n                            \"Dataset %s does not exist in %s, and no ROI is \"\n                            \"provided for %s. I don't know how to initialize \"\n                            \"the dataset.\" % (dataset_name, self.store, array_key)\n                        )\n\n                    offset = provided_roi.offset\n                    voxel_size = array.spec.voxel_size\n                    data_shape = provided_roi.shape // voxel_size\n\n                    logger.debug(\"Shape in voxels: %s\", data_shape)\n                    # add channel dimensions (if present)\n                    data_shape = batch_shape[:-dims] + data_shape\n                    logger.debug(\"Shape with channel dimensions: %s\", data_shape)\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                    else:\n                        dtype = batch.arrays[array_key].data.dtype\n\n                    logger.debug(\n                        \"create_dataset: %s, %s, %s, %s, offset=%s, resolution=%s\",\n                        dataset_name,\n                        data_shape,\n                        self.compression_type,\n                        dtype,\n                        offset,\n                        voxel_size,\n                    )\n\n                    dataset = data_file.create_dataset(\n                        name=dataset_name,\n                        shape=data_shape,\n                        compression=self.compression_type,\n                        dtype=dtype,\n                    )\n\n                    self._set_offset(dataset, offset)\n                    self._set_voxel_size(dataset, voxel_size)\n\n                logger.debug(\n                    \"%s (%s in %s) has offset %s\",\n                    array_key,\n                    dataset_name,\n                    self.store,\n                    offset,\n                )\n                self.dataset_offsets[array_key] = offset",
  "def process(self, batch, request):\n        if not self.dataset_offsets:\n            self.init_datasets(batch)\n\n        with self._open_file(self.store) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                dataset = data_file[dataset_name]\n\n                array_roi = batch.arrays[array_key].spec.roi\n                voxel_size = self.spec[array_key].voxel_size\n                dims = array_roi.dims\n                channel_slices = (slice(None),) * max(0, len(dataset.shape) - dims)\n\n                dataset_roi = Roi(\n                    self.dataset_offsets[array_key],\n                    Coordinate(dataset.shape[-dims:]) * voxel_size,\n                )\n                common_roi = array_roi.intersect(dataset_roi)\n\n                if common_roi.empty:\n                    logger.warn(\n                        \"array %s with ROI %s lies outside of dataset ROI %s, \"\n                        \"skipping writing\" % (array_key, array_roi, dataset_roi)\n                    )\n                    continue\n\n                dataset_voxel_roi = (\n                    common_roi - self.dataset_offsets[array_key]\n                ) // voxel_size\n                dataset_voxel_slices = dataset_voxel_roi.to_slices()\n                array_voxel_roi = (common_roi - array_roi.offset) // voxel_size\n                array_voxel_slices = array_voxel_roi.to_slices()\n\n                logger.debug(\n                    \"writing %s to voxel coordinates %s\"\n                    % (array_key, dataset_voxel_roi)\n                )\n\n                data = batch.arrays[array_key].data[channel_slices + array_voxel_slices]\n                dataset[channel_slices + dataset_voxel_slices] = data",
  "class BatchFilterError(Exception):\n    def __init__(self, batch_filter, msg):\n        self.batch_filter = batch_filter\n        self.msg = msg\n\n    def __str__(self):\n        return f\"Error in {self.batch_filter.name()}: {self.msg}\"",
  "class BatchFilter(BatchProvider):\n    \"\"\"Convenience wrapper for :class:`BatchProviders<BatchProvider>` with\n    exactly one input provider.\n\n    By default, a node of this class will expose the same :class:`ProviderSpec`\n    as the upstream provider. You can modify the provider spec by calling\n    :func:`provides` and :func:`updates` in :func:`setup`.\n\n    Subclasses need to implement at least :func:`process` to modify a passed\n    batch (downstream). Optionally, the following methods can be implemented:\n\n        :func:`setup`\n\n            Initialize this filter. Called after setup of the DAG. All upstream\n            providers will be set up already.\n\n        :func:`teardown`\n\n            Destruct this filter, free resources, stop worker processes.\n\n        :func:`prepare`\n\n            Prepare for a batch request. Always called before each\n            :func:`process`. Used to communicate dependencies.\n    \"\"\"\n\n    @property\n    def remove_placeholders(self):\n        if not hasattr(self, \"_remove_placeholders\"):\n            return False\n        return self._remove_placeholders\n\n    def get_upstream_provider(self):\n        if len(self.get_upstream_providers()) != 1:\n            raise BatchFilterError(\n                self,\n                \"BatchFilters need to have exactly one upstream provider, \"\n                f\"this one has {len(self.get_upstream_providers())}: \"\n                f\"({[b.name() for b in self.get_upstream_providers()]}\",\n            )\n        return self.get_upstream_providers()[0]\n\n    def updates(self, key, spec):\n        \"\"\"Update an output provided by this :class:`BatchFilter`.\n\n        Implementations should call this in their :func:`setup` method, which\n        will be called when the pipeline is build.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The array or point set key this filter updates.\n\n            spec (:class:`ArraySpec` or :class:`GraphSpec`):\n\n                The updated spec of the array or point set.\n        \"\"\"\n\n        if key not in self.spec:\n            raise BatchFilterError(\n                self,\n                f\"BatchFilter {self} is trying to change the spec for {key}, \"\n                f\"but {key} is not provided upstream. Upstream offers: \"\n                f\"{self.get_upstream_provider().spec}\",\n            )\n        self.spec[key] = spec.copy()\n        self.updated_items.append(key)\n\n        logger.debug(\"%s updates %s with %s\" % (self.name(), key, spec))\n\n    def enable_autoskip(self, skip=True):\n        \"\"\"Enable automatic skipping of this :class:`BatchFilter`, based on\n        given :func:`updates` and :func:`provides` calls. Has to be called in\n        :func:`setup`.\n\n        By default, :class:`BatchFilters<BatchFilter>` are not skipped\n        automatically, regardless of what they update or provide. If autskip is\n        enabled, :class:`BatchFilters<BatchFilter>` will only be run if the\n        request contains at least one key reported earlier with\n        :func:`updates` or :func:`provides`.\n        \"\"\"\n\n        self._autoskip_enabled = skip\n\n    def _init_spec(self):\n        # default for BatchFilters is to provide the same as upstream\n        if not hasattr(self, \"_spec\") or self._spec is None:\n            if len(self.get_upstream_providers()) != 0:\n                self._spec = self.get_upstream_provider().spec.copy()\n            else:\n                self._spec = None\n\n    def internal_teardown(self):\n        logger.debug(\"Resetting spec of %s\", self.name())\n        self._spec = None\n        self._updated_items = []\n\n        self.teardown()\n\n    @property\n    def updated_items(self):\n        \"\"\"Get a list of the keys that are updated by this `BatchFilter`.\n\n        This list is only available after the pipeline has been build. Before\n        that, it is empty.\n        \"\"\"\n\n        if not hasattr(self, \"_updated_items\"):\n            self._updated_items = []\n\n        return self._updated_items\n\n    @property\n    def autoskip_enabled(self):\n        if not hasattr(self, \"_autoskip_enabled\"):\n            self._autoskip_enabled = False\n\n        return self._autoskip_enabled\n\n    def provide(self, request):\n        skip = self.__can_skip(request)\n\n        timing_prepare = Timing(self, \"prepare\")\n        timing_prepare.start()\n\n        downstream_request = request.copy()\n\n        if not skip:\n            dependencies = self.prepare(request)\n            if isinstance(dependencies, BatchRequest):\n                upstream_request = request.update_with(dependencies)\n            elif dependencies is None:\n                upstream_request = request.copy()\n            else:\n                raise BatchFilterError(\n                    self,\n                    f\"This BatchFilter returned a {type(dependencies)}! \"\n                    \"Supported return types are: `BatchRequest` containing your exact \"\n                    \"dependencies or `None`, indicating a dependency on the full request.\",\n                )\n            self.remove_provided(upstream_request)\n        else:\n            upstream_request = request.copy()\n        self.remove_provided(upstream_request)\n\n        timing_prepare.stop()\n\n        batch = self.get_upstream_provider().request_batch(upstream_request)\n\n        timing_process = Timing(self, \"process\")\n        timing_process.start()\n\n        if not skip:\n            if dependencies is not None:\n                dependencies.remove_placeholders()\n                node_batch = batch.crop(dependencies)\n            else:\n                node_batch = batch\n            downstream_request.remove_placeholders()\n            processed_batch = self.process(node_batch, downstream_request)\n            if processed_batch is None:\n                processed_batch = node_batch\n            batch = batch.merge(processed_batch, merge_profiling_stats=False).crop(\n                downstream_request\n            )\n\n        timing_process.stop()\n\n        batch.profiling_stats.add(timing_prepare)\n        batch.profiling_stats.add(timing_process)\n\n        return batch\n\n    def __can_skip(self, request):\n        \"\"\"Check if this filter needs to be run for the given request.\"\"\"\n\n        if not self.autoskip_enabled:\n            return False\n\n        for key, spec in request.items():\n            if spec.placeholder:\n                continue\n            if key in self.provided_items:\n                return False\n            if key in self.updated_items:\n                return False\n\n        return True\n\n    def setup(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during initialization of the DAG. Callees can assume that all\n        upstream providers are set up already.\n\n        In setup, call :func:`provides` or :func:`updates` to announce the\n        arrays and points provided or changed by this node.\n        \"\"\"\n        pass\n\n    def prepare(self, request):\n        \"\"\"To be implemented in subclasses.\n\n        Prepare for a batch request. Should return a :class:`BatchRequest` of\n        needed dependencies. If None is returned, it will be assumed that all\n        of request is needed.\n        \"\"\"\n        return None\n\n    def process(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        Filter a batch, will be called after :func:`prepare`. Should return a\n        :class:`Batch` containing modified Arrays and Graphs. Keys in the returned\n        batch will replace the associated data in the original batch. If None is\n        returned it is assumed that the batch has been modified in place. ``request``\n        is the same as passed to :func:`prepare`, provided for convenience.\n\n        Args:\n\n            batch (:class:`Batch`):\n\n                The batch received from upstream to be modified by this node.\n\n            request (:class:`BatchRequest`):\n\n                The request this node received. The updated batch should meet\n                this request.\n        \"\"\"\n        raise BatchFilterError(self, \"does not implement 'process'\")",
  "def __init__(self, batch_filter, msg):\n        self.batch_filter = batch_filter\n        self.msg = msg",
  "def __str__(self):\n        return f\"Error in {self.batch_filter.name()}: {self.msg}\"",
  "def remove_placeholders(self):\n        if not hasattr(self, \"_remove_placeholders\"):\n            return False\n        return self._remove_placeholders",
  "def get_upstream_provider(self):\n        if len(self.get_upstream_providers()) != 1:\n            raise BatchFilterError(\n                self,\n                \"BatchFilters need to have exactly one upstream provider, \"\n                f\"this one has {len(self.get_upstream_providers())}: \"\n                f\"({[b.name() for b in self.get_upstream_providers()]}\",\n            )\n        return self.get_upstream_providers()[0]",
  "def updates(self, key, spec):\n        \"\"\"Update an output provided by this :class:`BatchFilter`.\n\n        Implementations should call this in their :func:`setup` method, which\n        will be called when the pipeline is build.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The array or point set key this filter updates.\n\n            spec (:class:`ArraySpec` or :class:`GraphSpec`):\n\n                The updated spec of the array or point set.\n        \"\"\"\n\n        if key not in self.spec:\n            raise BatchFilterError(\n                self,\n                f\"BatchFilter {self} is trying to change the spec for {key}, \"\n                f\"but {key} is not provided upstream. Upstream offers: \"\n                f\"{self.get_upstream_provider().spec}\",\n            )\n        self.spec[key] = spec.copy()\n        self.updated_items.append(key)\n\n        logger.debug(\"%s updates %s with %s\" % (self.name(), key, spec))",
  "def enable_autoskip(self, skip=True):\n        \"\"\"Enable automatic skipping of this :class:`BatchFilter`, based on\n        given :func:`updates` and :func:`provides` calls. Has to be called in\n        :func:`setup`.\n\n        By default, :class:`BatchFilters<BatchFilter>` are not skipped\n        automatically, regardless of what they update or provide. If autskip is\n        enabled, :class:`BatchFilters<BatchFilter>` will only be run if the\n        request contains at least one key reported earlier with\n        :func:`updates` or :func:`provides`.\n        \"\"\"\n\n        self._autoskip_enabled = skip",
  "def _init_spec(self):\n        # default for BatchFilters is to provide the same as upstream\n        if not hasattr(self, \"_spec\") or self._spec is None:\n            if len(self.get_upstream_providers()) != 0:\n                self._spec = self.get_upstream_provider().spec.copy()\n            else:\n                self._spec = None",
  "def internal_teardown(self):\n        logger.debug(\"Resetting spec of %s\", self.name())\n        self._spec = None\n        self._updated_items = []\n\n        self.teardown()",
  "def updated_items(self):\n        \"\"\"Get a list of the keys that are updated by this `BatchFilter`.\n\n        This list is only available after the pipeline has been build. Before\n        that, it is empty.\n        \"\"\"\n\n        if not hasattr(self, \"_updated_items\"):\n            self._updated_items = []\n\n        return self._updated_items",
  "def autoskip_enabled(self):\n        if not hasattr(self, \"_autoskip_enabled\"):\n            self._autoskip_enabled = False\n\n        return self._autoskip_enabled",
  "def provide(self, request):\n        skip = self.__can_skip(request)\n\n        timing_prepare = Timing(self, \"prepare\")\n        timing_prepare.start()\n\n        downstream_request = request.copy()\n\n        if not skip:\n            dependencies = self.prepare(request)\n            if isinstance(dependencies, BatchRequest):\n                upstream_request = request.update_with(dependencies)\n            elif dependencies is None:\n                upstream_request = request.copy()\n            else:\n                raise BatchFilterError(\n                    self,\n                    f\"This BatchFilter returned a {type(dependencies)}! \"\n                    \"Supported return types are: `BatchRequest` containing your exact \"\n                    \"dependencies or `None`, indicating a dependency on the full request.\",\n                )\n            self.remove_provided(upstream_request)\n        else:\n            upstream_request = request.copy()\n        self.remove_provided(upstream_request)\n\n        timing_prepare.stop()\n\n        batch = self.get_upstream_provider().request_batch(upstream_request)\n\n        timing_process = Timing(self, \"process\")\n        timing_process.start()\n\n        if not skip:\n            if dependencies is not None:\n                dependencies.remove_placeholders()\n                node_batch = batch.crop(dependencies)\n            else:\n                node_batch = batch\n            downstream_request.remove_placeholders()\n            processed_batch = self.process(node_batch, downstream_request)\n            if processed_batch is None:\n                processed_batch = node_batch\n            batch = batch.merge(processed_batch, merge_profiling_stats=False).crop(\n                downstream_request\n            )\n\n        timing_process.stop()\n\n        batch.profiling_stats.add(timing_prepare)\n        batch.profiling_stats.add(timing_process)\n\n        return batch",
  "def __can_skip(self, request):\n        \"\"\"Check if this filter needs to be run for the given request.\"\"\"\n\n        if not self.autoskip_enabled:\n            return False\n\n        for key, spec in request.items():\n            if spec.placeholder:\n                continue\n            if key in self.provided_items:\n                return False\n            if key in self.updated_items:\n                return False\n\n        return True",
  "def setup(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during initialization of the DAG. Callees can assume that all\n        upstream providers are set up already.\n\n        In setup, call :func:`provides` or :func:`updates` to announce the\n        arrays and points provided or changed by this node.\n        \"\"\"\n        pass",
  "def prepare(self, request):\n        \"\"\"To be implemented in subclasses.\n\n        Prepare for a batch request. Should return a :class:`BatchRequest` of\n        needed dependencies. If None is returned, it will be assumed that all\n        of request is needed.\n        \"\"\"\n        return None",
  "def process(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        Filter a batch, will be called after :func:`prepare`. Should return a\n        :class:`Batch` containing modified Arrays and Graphs. Keys in the returned\n        batch will replace the associated data in the original batch. If None is\n        returned it is assumed that the batch has been modified in place. ``request``\n        is the same as passed to :func:`prepare`, provided for convenience.\n\n        Args:\n\n            batch (:class:`Batch`):\n\n                The batch received from upstream to be modified by this node.\n\n            request (:class:`BatchRequest`):\n\n                The request this node received. The updated batch should meet\n                this request.\n        \"\"\"\n        raise BatchFilterError(self, \"does not implement 'process'\")",
  "class ExcludeLabels(BatchFilter):\n    \"\"\"Excludes several labels from the ground-truth.\n\n    The labels will be replaced by background_value. An optional ignore mask\n    will be created and set to 0 for the excluded locations that are further\n    than a threshold away from not excluded locations.\n\n    Args:\n\n        labels (:class:`ArrayKey`):\n\n            The array containing the labels.\n\n        exclude (``list`` of ``int``):\n\n            The labels to exclude from ``labels``.\n\n        ignore_mask (:class:`ArrayKey`, optional):\n\n            The ignore mask to create.\n\n        ignore_mask_erode (``float``, optional):\n\n            By how much (in world units) to erode the ignore mask.\n\n        background_value (``int``, optional):\n\n            Value to replace excluded IDs, defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self, labels, exclude, ignore_mask=None, ignore_mask_erode=0, background_value=0\n    ):\n        self.labels = labels\n        self.exclude = set(exclude)\n        self.ignore_mask = ignore_mask\n        self.ignore_mask_erode = ignore_mask_erode\n        self.background_value = background_value\n\n    def setup(self):\n        assert (\n            self.labels in self.spec\n        ), \"ExcludeLabels can only be used if GT_LABELS is provided upstream.\"\n        if self.ignore_mask:\n            self.provides(self.ignore_mask, self.spec[self.labels])\n\n    def process(self, batch, request):\n        gt = batch.arrays[self.labels]\n\n        # 0 marks included regions (to be used directly with distance transform\n        # later)\n        include_mask = np.ones(gt.data.shape)\n\n        gt_labels = np.unique(gt.data)\n        logger.debug(\"batch contains GT labels: \" + str(gt_labels))\n        for label in gt_labels:\n            if label in self.exclude:\n                logger.debug(\"excluding label \" + str(label))\n                gt.data[gt.data == label] = self.background_value\n            else:\n                include_mask[gt.data == label] = 0\n\n        # if no ignore mask is provided or requested, we are done\n        if not self.ignore_mask or not self.ignore_mask in request:\n            return\n\n        voxel_size = self.spec[self.labels].voxel_size\n        distance_to_include = distance_transform_edt(include_mask, sampling=voxel_size)\n        logger.debug(\"max distance to foreground is \" + str(distance_to_include.max()))\n\n        # 1 marks included regions, plus a context area around them\n        include_mask = distance_to_include < self.ignore_mask_erode\n\n        # include mask was computed on labels ROI, we need to copy it to\n        # the requested ignore_mask ROI\n        gt_ignore_roi = request[self.ignore_mask].roi\n\n        intersection = gt.spec.roi.intersect(gt_ignore_roi)\n        intersection_in_gt = intersection - gt.spec.roi.offset\n        intersection_in_gt_ignore = intersection - gt_ignore_roi.offset\n\n        # to voxel coordinates\n        intersection_in_gt //= voxel_size\n        intersection_in_gt_ignore //= voxel_size\n\n        gt_ignore = np.zeros((gt_ignore_roi // voxel_size).get_shape(), dtype=np.uint8)\n        gt_ignore[intersection_in_gt_ignore.get_bounding_box()] = include_mask[\n            intersection_in_gt.get_bounding_box()\n        ]\n\n        spec = self.spec[self.labels].copy()\n        spec.roi = gt_ignore_roi\n        spec.dtype = np.uint8\n        batch.arrays[self.ignore_mask] = Array(gt_ignore, spec)",
  "def __init__(\n        self, labels, exclude, ignore_mask=None, ignore_mask_erode=0, background_value=0\n    ):\n        self.labels = labels\n        self.exclude = set(exclude)\n        self.ignore_mask = ignore_mask\n        self.ignore_mask_erode = ignore_mask_erode\n        self.background_value = background_value",
  "def setup(self):\n        assert (\n            self.labels in self.spec\n        ), \"ExcludeLabels can only be used if GT_LABELS is provided upstream.\"\n        if self.ignore_mask:\n            self.provides(self.ignore_mask, self.spec[self.labels])",
  "def process(self, batch, request):\n        gt = batch.arrays[self.labels]\n\n        # 0 marks included regions (to be used directly with distance transform\n        # later)\n        include_mask = np.ones(gt.data.shape)\n\n        gt_labels = np.unique(gt.data)\n        logger.debug(\"batch contains GT labels: \" + str(gt_labels))\n        for label in gt_labels:\n            if label in self.exclude:\n                logger.debug(\"excluding label \" + str(label))\n                gt.data[gt.data == label] = self.background_value\n            else:\n                include_mask[gt.data == label] = 0\n\n        # if no ignore mask is provided or requested, we are done\n        if not self.ignore_mask or not self.ignore_mask in request:\n            return\n\n        voxel_size = self.spec[self.labels].voxel_size\n        distance_to_include = distance_transform_edt(include_mask, sampling=voxel_size)\n        logger.debug(\"max distance to foreground is \" + str(distance_to_include.max()))\n\n        # 1 marks included regions, plus a context area around them\n        include_mask = distance_to_include < self.ignore_mask_erode\n\n        # include mask was computed on labels ROI, we need to copy it to\n        # the requested ignore_mask ROI\n        gt_ignore_roi = request[self.ignore_mask].roi\n\n        intersection = gt.spec.roi.intersect(gt_ignore_roi)\n        intersection_in_gt = intersection - gt.spec.roi.offset\n        intersection_in_gt_ignore = intersection - gt_ignore_roi.offset\n\n        # to voxel coordinates\n        intersection_in_gt //= voxel_size\n        intersection_in_gt_ignore //= voxel_size\n\n        gt_ignore = np.zeros((gt_ignore_roi // voxel_size).get_shape(), dtype=np.uint8)\n        gt_ignore[intersection_in_gt_ignore.get_bounding_box()] = include_mask[\n            intersection_in_gt.get_bounding_box()\n        ]\n\n        spec = self.spec[self.labels].copy()\n        spec.roi = gt_ignore_roi\n        spec.dtype = np.uint8\n        batch.arrays[self.ignore_mask] = Array(gt_ignore, spec)",
  "class TrainProcessDied(Exception):\n    pass",
  "class GenericTrain(BatchFilter):\n    \"\"\"Generic train node to perform one training iteration for each batch that\n    passes through. This node alone does nothing and should be subclassed for\n    concrete implementations.\n\n    Args:\n\n        inputs (dict): Dictionary from names of input layers in the network to\n            :class:``ArrayKey`` or batch attribute name as string.\n\n        outputs (dict): Dictionary from the names of output layers in the\n            network to :class:``ArrayKey``. New arrays will be generated by\n            this node for each entry (if requested downstream).\n\n        gradients (dict): Dictionary from the names of output layers in the\n            network to :class:``ArrayKey``. New arrays containing the\n            gradient of an output with respect to the loss will be generated by\n            this node for each entry (if requested downstream).\n\n        array_specs (dict, optional): An optional dictionary of\n            :class:`ArrayKey` to :class:`ArraySpec` to set the array specs\n            generated arrays (``outputs`` and ``gradients``). This is useful\n            to set the ``voxel_size``, for example, if they differ from the\n            voxel size of the input arrays. Only fields that are not ``None``\n            in the given :class:`ArraySpec` will be used.\n\n        spawn_subprocess (bool, optional): Whether to run the ``train_step`` in\n            a separate process. Default is false.\n    \"\"\"\n\n    def __init__(\n        self, inputs, outputs, gradients, array_specs=None, spawn_subprocess=False\n    ):\n        self.initialized = False\n\n        self.inputs = inputs\n        self.outputs = outputs\n        self.gradients = gradients\n        self.array_specs = {} if array_specs is None else array_specs\n        self.spawn_subprocess = spawn_subprocess\n\n        self.provided_arrays = list(self.outputs.values()) + list(\n            self.gradients.values()\n        )\n\n    def setup(self):\n        # get common voxel size of inputs, or None if they differ\n        common_voxel_size = None\n        for key in self.inputs.values():\n            if not isinstance(key, ArrayKey):\n                continue\n            if self.spec[key].nonspatial:\n                continue\n\n            voxel_size = self.spec[key].voxel_size\n\n            if common_voxel_size is None:\n                common_voxel_size = voxel_size\n            elif common_voxel_size != voxel_size:\n                common_voxel_size = None\n                break\n\n        # announce provided outputs\n        for key in self.provided_arrays:\n            if key in self.array_specs:\n                spec = self.array_specs[key].copy()\n            else:\n                spec = ArraySpec()\n\n            if spec.voxel_size is None and not spec.nonspatial:\n                assert common_voxel_size is not None, (\n                    \"There is no common voxel size of the inputs, and no \"\n                    \"ArraySpec has been given for %s that defines \"\n                    \"voxel_size.\" % key\n                )\n\n                spec.voxel_size = common_voxel_size\n\n            if spec.interpolatable is None:\n                # default for predictions\n                spec.interpolatable = False\n\n            self.provides(key, spec)\n\n        if self.spawn_subprocess:\n            # start training as a producer pool, so that we can gracefully exit if\n            # anything goes wrong\n            self.worker = ProducerPool([self.__produce_train_batch], queue_size=1)\n            self.batch_in = multiprocessing.Queue(maxsize=1)\n            self.worker.start()\n        else:\n            self.start()\n            self.initialized = True\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.inputs.values():\n            deps[key] = request[key]\n        return deps\n\n    def teardown(self):\n        if self.spawn_subprocess:\n            # signal \"stop\"\n            self.batch_in.put((None, None))\n            try:\n                self.worker.get(timeout=2)\n            except NoResult:\n                pass\n            self.worker.stop()\n        else:\n            self.stop()\n\n    def process(self, batch, request):\n        start = time.time()\n\n        if self.spawn_subprocess:\n            self.batch_in.put((batch, request))\n\n            try:\n                out = self.worker.get()\n            except WorkersDied:\n                raise TrainProcessDied()\n\n            for array_key in self.provided_arrays:\n                if array_key in request:\n                    batch.arrays[array_key] = out.arrays[array_key]\n\n            batch.loss = out.loss\n            batch.iteration = out.iteration\n\n        else:\n            self.train_step(batch, request)\n\n        time_of_iteration = time.time() - start\n\n        logger.info(\n            \"Train process: iteration=%d loss=%f time=%f\",\n            batch.iteration,\n            batch.loss,\n            time_of_iteration,\n        )\n\n    def start(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called before the first call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to initialize you solver and training hardware.\n        \"\"\"\n        pass\n\n    def train_step(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        In this method, an implementation should perform one training iteration\n        on the given batch. ``batch.loss`` and ``batch.iteration`` should be\n        set. Output arrays should be created according to the given request\n        and added to ``batch``.\"\"\"\n        raise NotImplementedError(\n            \"Class %s does not implement 'train_step'\" % self.name()\n        )\n\n    def stop(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called after the last call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to tear down you solver and free training hardware.\n        \"\"\"\n        pass\n\n    def _checkpoint_name(self, basename, iteration):\n        return basename + \"_checkpoint_\" + \"%i\" % iteration\n\n    def _get_latest_checkpoint(self, basename):\n        def atoi(text):\n            return int(text) if text.isdigit() else text\n\n        def natural_keys(text):\n            return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n\n        checkpoints = glob.glob(basename + \"_checkpoint_*\")\n        checkpoints.sort(key=natural_keys)\n\n        if len(checkpoints) > 0:\n            checkpoint = checkpoints[-1]\n            iteration = int(checkpoint.split(\"_\")[-1])\n            return checkpoint, iteration\n\n        return None, 0\n\n    def __produce_train_batch(self):\n        \"\"\"Process one train batch.\"\"\"\n\n        if not self.initialized:\n            self.start()\n            self.initialized = True\n\n        batch, request = self.batch_in.get()\n\n        # stop signal\n        if batch is None:\n            self.stop()\n            return None\n\n        self.train_step(batch, request)\n\n        return batch",
  "def __init__(\n        self, inputs, outputs, gradients, array_specs=None, spawn_subprocess=False\n    ):\n        self.initialized = False\n\n        self.inputs = inputs\n        self.outputs = outputs\n        self.gradients = gradients\n        self.array_specs = {} if array_specs is None else array_specs\n        self.spawn_subprocess = spawn_subprocess\n\n        self.provided_arrays = list(self.outputs.values()) + list(\n            self.gradients.values()\n        )",
  "def setup(self):\n        # get common voxel size of inputs, or None if they differ\n        common_voxel_size = None\n        for key in self.inputs.values():\n            if not isinstance(key, ArrayKey):\n                continue\n            if self.spec[key].nonspatial:\n                continue\n\n            voxel_size = self.spec[key].voxel_size\n\n            if common_voxel_size is None:\n                common_voxel_size = voxel_size\n            elif common_voxel_size != voxel_size:\n                common_voxel_size = None\n                break\n\n        # announce provided outputs\n        for key in self.provided_arrays:\n            if key in self.array_specs:\n                spec = self.array_specs[key].copy()\n            else:\n                spec = ArraySpec()\n\n            if spec.voxel_size is None and not spec.nonspatial:\n                assert common_voxel_size is not None, (\n                    \"There is no common voxel size of the inputs, and no \"\n                    \"ArraySpec has been given for %s that defines \"\n                    \"voxel_size.\" % key\n                )\n\n                spec.voxel_size = common_voxel_size\n\n            if spec.interpolatable is None:\n                # default for predictions\n                spec.interpolatable = False\n\n            self.provides(key, spec)\n\n        if self.spawn_subprocess:\n            # start training as a producer pool, so that we can gracefully exit if\n            # anything goes wrong\n            self.worker = ProducerPool([self.__produce_train_batch], queue_size=1)\n            self.batch_in = multiprocessing.Queue(maxsize=1)\n            self.worker.start()\n        else:\n            self.start()\n            self.initialized = True",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.inputs.values():\n            deps[key] = request[key]\n        return deps",
  "def teardown(self):\n        if self.spawn_subprocess:\n            # signal \"stop\"\n            self.batch_in.put((None, None))\n            try:\n                self.worker.get(timeout=2)\n            except NoResult:\n                pass\n            self.worker.stop()\n        else:\n            self.stop()",
  "def process(self, batch, request):\n        start = time.time()\n\n        if self.spawn_subprocess:\n            self.batch_in.put((batch, request))\n\n            try:\n                out = self.worker.get()\n            except WorkersDied:\n                raise TrainProcessDied()\n\n            for array_key in self.provided_arrays:\n                if array_key in request:\n                    batch.arrays[array_key] = out.arrays[array_key]\n\n            batch.loss = out.loss\n            batch.iteration = out.iteration\n\n        else:\n            self.train_step(batch, request)\n\n        time_of_iteration = time.time() - start\n\n        logger.info(\n            \"Train process: iteration=%d loss=%f time=%f\",\n            batch.iteration,\n            batch.loss,\n            time_of_iteration,\n        )",
  "def start(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called before the first call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to initialize you solver and training hardware.\n        \"\"\"\n        pass",
  "def train_step(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        In this method, an implementation should perform one training iteration\n        on the given batch. ``batch.loss`` and ``batch.iteration`` should be\n        set. Output arrays should be created according to the given request\n        and added to ``batch``.\"\"\"\n        raise NotImplementedError(\n            \"Class %s does not implement 'train_step'\" % self.name()\n        )",
  "def stop(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called after the last call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to tear down you solver and free training hardware.\n        \"\"\"\n        pass",
  "def _checkpoint_name(self, basename, iteration):\n        return basename + \"_checkpoint_\" + \"%i\" % iteration",
  "def _get_latest_checkpoint(self, basename):\n        def atoi(text):\n            return int(text) if text.isdigit() else text\n\n        def natural_keys(text):\n            return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n\n        checkpoints = glob.glob(basename + \"_checkpoint_*\")\n        checkpoints.sort(key=natural_keys)\n\n        if len(checkpoints) > 0:\n            checkpoint = checkpoints[-1]\n            iteration = int(checkpoint.split(\"_\")[-1])\n            return checkpoint, iteration\n\n        return None, 0",
  "def __produce_train_batch(self):\n        \"\"\"Process one train batch.\"\"\"\n\n        if not self.initialized:\n            self.start()\n            self.initialized = True\n\n        batch, request = self.batch_in.get()\n\n        # stop signal\n        if batch is None:\n            self.stop()\n            return None\n\n        self.train_step(batch, request)\n\n        return batch",
  "def atoi(text):\n            return int(text) if text.isdigit() else text",
  "def natural_keys(text):\n            return [atoi(c) for c in re.split(r\"(\\d+)\", text)]",
  "class AsType(BatchFilter):\n    \"\"\"Cast arrays to a different datatype (ex: np.float32 --> np.uint8).\n\n    Args:\n\n        source (:class:`ArrayKey`):\n\n            The key of the array to cast.\n\n        target_dtype (str or dtype):\n\n            The voxel size of the target.\n\n        target (:class:`ArrayKey`, optional):\n\n            The key of the array to store the cast ``source``.\n\n    \"\"\"\n\n    def __init__(self, source, target_dtype, target=None):\n        assert isinstance(source, ArrayKey)\n        if target is not None:\n            assert isinstance(target, ArrayKey)\n            self.target = target\n        else:\n            self.target = source\n\n        self.source = source\n        self.target_dtype = target_dtype\n\n    def setup(self):\n        spec = self.spec[self.source].copy()\n        spec.dtype = self.target_dtype\n        if self.target is not self.source:\n            self.provides(self.target, spec)\n        else:\n            self.updates(self.source, spec)\n        self.enable_autoskip()\n\n    def process(self, batch, request):\n        source = batch.arrays[self.source]\n        source_data = source.data\n\n        cast_data = source_data.astype(self.target_dtype)\n\n        target_spec = source.spec.copy()\n        target_spec.dtype = cast_data.dtype\n        target_array = Array(cast_data, target_spec)\n\n        # create output array\n        outputs = Batch()\n        outputs.arrays[self.target] = target_array\n\n        return outputs",
  "def __init__(self, source, target_dtype, target=None):\n        assert isinstance(source, ArrayKey)\n        if target is not None:\n            assert isinstance(target, ArrayKey)\n            self.target = target\n        else:\n            self.target = source\n\n        self.source = source\n        self.target_dtype = target_dtype",
  "def setup(self):\n        spec = self.spec[self.source].copy()\n        spec.dtype = self.target_dtype\n        if self.target is not self.source:\n            self.provides(self.target, spec)\n        else:\n            self.updates(self.source, spec)\n        self.enable_autoskip()",
  "def process(self, batch, request):\n        source = batch.arrays[self.source]\n        source_data = source.data\n\n        cast_data = source_data.astype(self.target_dtype)\n\n        target_spec = source.spec.copy()\n        target_spec.dtype = cast_data.dtype\n        target_array = Array(cast_data, target_spec)\n\n        # create output array\n        outputs = Batch()\n        outputs.arrays[self.target] = target_array\n\n        return outputs",
  "class BatchRequestError(Exception):\n    def __init__(self, provider, request, batch):\n        self.provider = provider\n        self.request = request\n        self.batch = batch\n\n    def __str__(self):\n        return (\n            f\"Exception in {self.provider.name()} while processing request\"\n            f\"{self.request} \\n\"\n            \"Batch returned so far:\\n\"\n            f\"{self.batch}\"\n        )",
  "class BatchProvider(object):\n    \"\"\"Superclass for all nodes in a `gunpowder` graph.\n\n    A :class:`BatchProvider` provides :class:`Batches<Batch>` containing\n    :class:`Arrays<Array>` and/or :class:`Graph`. The available data is\n    specified in a :class:`ProviderSpec` instance, accessible via :attr:`spec`.\n\n    To create a new node, subclass this class and implement (at least)\n    :func:`setup` and :func:`provide`.\n\n    A :class:`BatchProvider` can be linked to any number of other\n    :class:`BatchProviders<BatchProvider>` upstream. If your node accepts\n    exactly one upstream provider, consider subclassing :class:`BatchFilter`\n    instead.\n    \"\"\"\n\n    def add_upstream_provider(self, provider):\n        self.get_upstream_providers().append(provider)\n        return provider\n\n    def remove_upstream_providers(self):\n        self.upstream_providers = []\n\n    def get_upstream_providers(self):\n        if not hasattr(self, \"upstream_providers\"):\n            self.upstream_providers = []\n        return self.upstream_providers\n\n    @property\n    def remove_placeholders(self):\n        if not hasattr(self, \"_remove_placeholders\"):\n            return True\n        return self._remove_placeholders\n\n    def setup(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during initialization of the DAG. Callees can assume that all\n        upstream providers are set up already.\n\n        In setup, call :func:`provides` to announce the arrays and points\n        provided by this node.\n        \"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'setup'\" % self.name())\n\n    def teardown(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during destruction of the DAG. Subclasses should use this to\n        stop worker processes, if they used some.\n        \"\"\"\n        pass\n\n    def provides(self, key, spec):\n        \"\"\"Introduce a new output provided by this :class:`BatchProvider`.\n\n        Implementations should call this in their :func:`setup` method, which\n        will be called when the pipeline is build.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The array or point set key provided.\n\n            spec (:class:`ArraySpec` or :class:`GraphSpec`):\n\n                The spec of the array or point set provided.\n        \"\"\"\n\n        logger.debug(\"Current spec of %s:\\n%s\", self.name(), self.spec)\n\n        if self.spec is None:\n            self._spec = ProviderSpec()\n\n        assert (\n            key not in self.spec\n        ), \"Node %s is trying to add spec for %s, but is already \" \"provided.\" % (\n            type(self).__name__,\n            key,\n        )\n\n        self.spec[key] = copy.deepcopy(spec)\n        self.provided_items.append(key)\n\n        logger.debug(\"%s provides %s with spec %s\", self.name(), key, spec)\n\n    def _init_spec(self):\n        if not hasattr(self, \"_spec\"):\n            self._spec = None\n\n    def internal_teardown(self):\n        logger.debug(\"Resetting spec of %s\", self.name())\n        self._spec = None\n        self._provided_items = []\n\n        self.teardown()\n\n    @property\n    def spec(self):\n        \"\"\"Get the :class:`ProviderSpec` of this :class:`BatchProvider`.\n\n        Note that the spec is only available after the pipeline has been build.\n        Before that, it is ``None``.\n        \"\"\"\n        self._init_spec()\n        return self._spec\n\n    @property\n    def provided_items(self):\n        \"\"\"Get a list of the keys provided by this :class:`BatchProvider`.\n\n        This list is only available after the pipeline has been build. Before\n        that, it is empty.\n        \"\"\"\n\n        if not hasattr(self, \"_provided_items\"):\n            self._provided_items = []\n\n        return self._provided_items\n\n    def remove_provided(self, request):\n        \"\"\"Remove keys from `request` that are provided by this\n        :class:`BatchProvider`.\n        \"\"\"\n\n        for key in self.provided_items:\n            if key in request:\n                del request[key]\n\n    def request_batch(self, request):\n        \"\"\"Request a batch from this provider.\n\n        Args:\n\n            request (:class:`BatchRequest`):\n\n                A request containing (possibly partial)\n                :class:`ArraySpecs<ArraySpec>` and\n                :class:`GraphSpecs<GraphSpec>`.\n        \"\"\"\n\n        batch = None\n\n        try:\n\n            self.set_seeds(request)\n\n            logger.debug(\"%s got request %s\", self.name(), request)\n\n            self.check_request_consistency(request)\n\n            upstream_request = request.copy()\n            if self.remove_placeholders:\n                upstream_request.remove_placeholders()\n            batch = self.provide(upstream_request)\n\n            request.remove_placeholders()\n\n            self.check_batch_consistency(batch, request)\n\n            self.remove_unneeded(batch, request)\n\n            logger.debug(\"%s provides %s\", self.name(), batch)\n\n        except Exception as e:\n            raise BatchRequestError(self, request, batch) from e\n\n        return batch\n\n    def set_seeds(self, request):\n        seed = request.random_seed\n        random.seed(seed)\n        # augment uses numpy for its randomness\n        np.random.seed(seed)\n\n    def check_request_consistency(self, request):\n        for key, request_spec in request.items():\n            assert (\n                key in self.spec\n            ), \"%s: Asked for %s which this node does not provide\" % (self.name(), key)\n            assert (\n                isinstance(request_spec, ArraySpec)\n                or isinstance(request_spec, GraphSpec)\n                or isinstance(request_spec, GraphSpec)\n            ), \"spec for %s is of type\" \"%s\" % (key, type(request_spec))\n\n            provided_spec = self.spec[key]\n\n            provided_roi = provided_spec.roi\n            request_roi = request_spec.roi\n\n            if provided_roi is not None:\n                assert provided_roi.contains(\n                    request_roi\n                ), \"%s: %s's ROI %s outside of my ROI %s\" % (\n                    self.name(),\n                    key,\n                    request_roi,\n                    provided_roi,\n                )\n\n            if isinstance(key, ArrayKey):\n                if request_spec.voxel_size is not None:\n                    assert provided_spec.voxel_size == request_spec.voxel_size, (\n                        \"%s: voxel size %s requested for %s, but this node provides %s\"\n                        % (\n                            self.name(),\n                            request_spec.voxel_size,\n                            key,\n                            provided_spec.voxel_size,\n                        )\n                    )\n\n                if request_roi is not None and provided_spec.voxel_size is not None:\n                    for d in range(request_roi.dims):\n                        assert (\n                            request_roi.shape[d] % provided_spec.voxel_size[d] == 0\n                        ), (\n                            \"in request %s, dimension %d of request %s is not a multiple of voxel_size %d\"\n                            % (request, d, key, provided_spec.voxel_size[d])\n                        )\n\n            if isinstance(key, GraphKey):\n                if request_spec.directed is not None:\n                    assert request_spec.directed == provided_spec.directed, (\n                        f\"asked for {key}:  directed={request_spec.directed} but \"\n                        f\"{self.name()} provides directed={provided_spec.directed}\"\n                    )\n\n    def check_batch_consistency(self, batch, request):\n        for array_key, request_spec in request.array_specs.items():\n            assert (\n                array_key in batch.arrays\n            ), \"%s requested, but %s did not provide it.\" % (array_key, self.name())\n            array = batch.arrays[array_key]\n            assert (\n                array.spec.roi == request_spec.roi\n            ), \"%s ROI %s requested, but ROI %s provided by %s.\" % (\n                array_key,\n                request_spec.roi,\n                array.spec.roi,\n                self.name(),\n            )\n            assert (\n                array.spec.voxel_size == self.spec[array_key].voxel_size\n            ), \"voxel size of %s announced, but %s \" \"delivered for %s\" % (\n                self.spec[array_key].voxel_size,\n                array.spec.voxel_size,\n                array_key,\n            )\n            # ensure that the spatial dimensions are the same (other dimensions\n            # on top are okay, e.g., for affinities)\n            if request_spec.roi is not None:\n                dims = request_spec.roi.dims\n                data_shape = Coordinate(array.data.shape[-dims:])\n                voxel_size = self.spec[array_key].voxel_size\n                assert data_shape == request_spec.roi.shape / voxel_size, (\n                    \"%s ROI %s requested, but size of array is %s*%s=%s provided by %s.\"\n                    % (\n                        array_key,\n                        request_spec.roi,\n                        data_shape,\n                        voxel_size,\n                        data_shape * voxel_size,\n                        self.name(),\n                    )\n                )\n            if request_spec.dtype is not None:\n                assert (\n                    batch[array_key].data.dtype == request_spec.dtype\n                ), \"dtype of array %s (%s) does not match requested dtype %s by %s\" % (\n                    array_key,\n                    batch[array_key].data.dtype,\n                    request_spec.dtype,\n                    self.name(),\n                )\n\n        for graph_key, request_spec in request.graph_specs.items():\n            assert (\n                graph_key in batch.graphs\n            ), \"%s requested, but %s did not provide it.\" % (graph_key, self.name())\n            graph = batch.graphs[graph_key]\n            assert (\n                graph.spec.roi == request_spec.roi\n            ), \"%s ROI %s requested, but ROI %s provided by %s.\" % (\n                graph_key,\n                request_spec.roi,\n                graph.spec.roi,\n                self.name(),\n            )\n\n            if request_spec.directed is not None:\n                assert request_spec.directed == graph.directed, (\n                    f\"Recieved {graph_key}:  directed={graph.directed} but \"\n                    f\"{self.name()} should provide directed={request_spec.directed}\"\n                )\n\n            for node in graph.nodes:\n                contained = graph.spec.roi.contains(node.location)\n                dangling = not contained and all(\n                    [graph.spec.roi.contains(v.location) for v in graph.neighbors(node)]\n                )\n                assert contained or dangling, (\n                    f\"graph {graph_key} provided by {self.name()} with ROI {graph.spec.roi} \"\n                    f\"contain point at {node.location} which is neither contained nor \"\n                    f\"'dangling'\"\n                )\n\n    def remove_unneeded(self, batch, request):\n        batch_keys = set(list(batch.arrays.keys()) + list(batch.graphs.keys()))\n        for key in batch_keys:\n            if key not in request:\n                del batch[key]\n\n    def enable_placeholders(self):\n        self._remove_placeholders = False\n\n    def provide(self, request):\n        \"\"\"To be implemented in subclasses.\n\n        This function takes a :class:`BatchRequest` and should return the\n        corresponding :class:`Batch`.\n\n        Args:\n\n            request(:class:`BatchRequest`):\n\n                The request to process.\n        \"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'provide'\" % self.name())\n\n    def name(self):\n        return type(self).__name__\n\n    def __repr__(self):\n        return self.name() + \", providing: \" + str(self.spec)\n\n    def __add__(self, other):\n        \"\"\"Support ``self + other`` operator. Return a :class:`Pipeline`.\"\"\"\n        from gunpowder import Pipeline\n\n        if isinstance(other, BatchProvider):\n            other = Pipeline(other)\n\n        if not isinstance(other, Pipeline):\n            raise RuntimeError(\n                f\"Don't know how to add {type(other)} to BatchProvider \"\n                f\"{self.name()}\"\n            )\n\n        return Pipeline(self) + other\n\n    def __radd__(self, other):\n        \"\"\"Support ``other + self`` operator. Return a :class:`Pipeline`.\"\"\"\n        from gunpowder import Pipeline\n\n        if isinstance(other, BatchProvider):\n            return Pipeline(other) + Pipeline(self)\n\n        if isinstance(other, tuple):\n            return other + Pipeline(self)\n\n        raise RuntimeError(\n            f\"Don't know how to radd {type(other)} to BatchProvider\" f\"{self.name()}\"\n        )",
  "def __init__(self, provider, request, batch):\n        self.provider = provider\n        self.request = request\n        self.batch = batch",
  "def __str__(self):\n        return (\n            f\"Exception in {self.provider.name()} while processing request\"\n            f\"{self.request} \\n\"\n            \"Batch returned so far:\\n\"\n            f\"{self.batch}\"\n        )",
  "def add_upstream_provider(self, provider):\n        self.get_upstream_providers().append(provider)\n        return provider",
  "def remove_upstream_providers(self):\n        self.upstream_providers = []",
  "def get_upstream_providers(self):\n        if not hasattr(self, \"upstream_providers\"):\n            self.upstream_providers = []\n        return self.upstream_providers",
  "def remove_placeholders(self):\n        if not hasattr(self, \"_remove_placeholders\"):\n            return True\n        return self._remove_placeholders",
  "def setup(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during initialization of the DAG. Callees can assume that all\n        upstream providers are set up already.\n\n        In setup, call :func:`provides` to announce the arrays and points\n        provided by this node.\n        \"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'setup'\" % self.name())",
  "def teardown(self):\n        \"\"\"To be implemented in subclasses.\n\n        Called during destruction of the DAG. Subclasses should use this to\n        stop worker processes, if they used some.\n        \"\"\"\n        pass",
  "def provides(self, key, spec):\n        \"\"\"Introduce a new output provided by this :class:`BatchProvider`.\n\n        Implementations should call this in their :func:`setup` method, which\n        will be called when the pipeline is build.\n\n        Args:\n\n            key (:class:`ArrayKey` or :class:`GraphKey`):\n\n                The array or point set key provided.\n\n            spec (:class:`ArraySpec` or :class:`GraphSpec`):\n\n                The spec of the array or point set provided.\n        \"\"\"\n\n        logger.debug(\"Current spec of %s:\\n%s\", self.name(), self.spec)\n\n        if self.spec is None:\n            self._spec = ProviderSpec()\n\n        assert (\n            key not in self.spec\n        ), \"Node %s is trying to add spec for %s, but is already \" \"provided.\" % (\n            type(self).__name__,\n            key,\n        )\n\n        self.spec[key] = copy.deepcopy(spec)\n        self.provided_items.append(key)\n\n        logger.debug(\"%s provides %s with spec %s\", self.name(), key, spec)",
  "def _init_spec(self):\n        if not hasattr(self, \"_spec\"):\n            self._spec = None",
  "def internal_teardown(self):\n        logger.debug(\"Resetting spec of %s\", self.name())\n        self._spec = None\n        self._provided_items = []\n\n        self.teardown()",
  "def spec(self):\n        \"\"\"Get the :class:`ProviderSpec` of this :class:`BatchProvider`.\n\n        Note that the spec is only available after the pipeline has been build.\n        Before that, it is ``None``.\n        \"\"\"\n        self._init_spec()\n        return self._spec",
  "def provided_items(self):\n        \"\"\"Get a list of the keys provided by this :class:`BatchProvider`.\n\n        This list is only available after the pipeline has been build. Before\n        that, it is empty.\n        \"\"\"\n\n        if not hasattr(self, \"_provided_items\"):\n            self._provided_items = []\n\n        return self._provided_items",
  "def remove_provided(self, request):\n        \"\"\"Remove keys from `request` that are provided by this\n        :class:`BatchProvider`.\n        \"\"\"\n\n        for key in self.provided_items:\n            if key in request:\n                del request[key]",
  "def request_batch(self, request):\n        \"\"\"Request a batch from this provider.\n\n        Args:\n\n            request (:class:`BatchRequest`):\n\n                A request containing (possibly partial)\n                :class:`ArraySpecs<ArraySpec>` and\n                :class:`GraphSpecs<GraphSpec>`.\n        \"\"\"\n\n        batch = None\n\n        try:\n\n            self.set_seeds(request)\n\n            logger.debug(\"%s got request %s\", self.name(), request)\n\n            self.check_request_consistency(request)\n\n            upstream_request = request.copy()\n            if self.remove_placeholders:\n                upstream_request.remove_placeholders()\n            batch = self.provide(upstream_request)\n\n            request.remove_placeholders()\n\n            self.check_batch_consistency(batch, request)\n\n            self.remove_unneeded(batch, request)\n\n            logger.debug(\"%s provides %s\", self.name(), batch)\n\n        except Exception as e:\n            raise BatchRequestError(self, request, batch) from e\n\n        return batch",
  "def set_seeds(self, request):\n        seed = request.random_seed\n        random.seed(seed)\n        # augment uses numpy for its randomness\n        np.random.seed(seed)",
  "def check_request_consistency(self, request):\n        for key, request_spec in request.items():\n            assert (\n                key in self.spec\n            ), \"%s: Asked for %s which this node does not provide\" % (self.name(), key)\n            assert (\n                isinstance(request_spec, ArraySpec)\n                or isinstance(request_spec, GraphSpec)\n                or isinstance(request_spec, GraphSpec)\n            ), \"spec for %s is of type\" \"%s\" % (key, type(request_spec))\n\n            provided_spec = self.spec[key]\n\n            provided_roi = provided_spec.roi\n            request_roi = request_spec.roi\n\n            if provided_roi is not None:\n                assert provided_roi.contains(\n                    request_roi\n                ), \"%s: %s's ROI %s outside of my ROI %s\" % (\n                    self.name(),\n                    key,\n                    request_roi,\n                    provided_roi,\n                )\n\n            if isinstance(key, ArrayKey):\n                if request_spec.voxel_size is not None:\n                    assert provided_spec.voxel_size == request_spec.voxel_size, (\n                        \"%s: voxel size %s requested for %s, but this node provides %s\"\n                        % (\n                            self.name(),\n                            request_spec.voxel_size,\n                            key,\n                            provided_spec.voxel_size,\n                        )\n                    )\n\n                if request_roi is not None and provided_spec.voxel_size is not None:\n                    for d in range(request_roi.dims):\n                        assert (\n                            request_roi.shape[d] % provided_spec.voxel_size[d] == 0\n                        ), (\n                            \"in request %s, dimension %d of request %s is not a multiple of voxel_size %d\"\n                            % (request, d, key, provided_spec.voxel_size[d])\n                        )\n\n            if isinstance(key, GraphKey):\n                if request_spec.directed is not None:\n                    assert request_spec.directed == provided_spec.directed, (\n                        f\"asked for {key}:  directed={request_spec.directed} but \"\n                        f\"{self.name()} provides directed={provided_spec.directed}\"\n                    )",
  "def check_batch_consistency(self, batch, request):\n        for array_key, request_spec in request.array_specs.items():\n            assert (\n                array_key in batch.arrays\n            ), \"%s requested, but %s did not provide it.\" % (array_key, self.name())\n            array = batch.arrays[array_key]\n            assert (\n                array.spec.roi == request_spec.roi\n            ), \"%s ROI %s requested, but ROI %s provided by %s.\" % (\n                array_key,\n                request_spec.roi,\n                array.spec.roi,\n                self.name(),\n            )\n            assert (\n                array.spec.voxel_size == self.spec[array_key].voxel_size\n            ), \"voxel size of %s announced, but %s \" \"delivered for %s\" % (\n                self.spec[array_key].voxel_size,\n                array.spec.voxel_size,\n                array_key,\n            )\n            # ensure that the spatial dimensions are the same (other dimensions\n            # on top are okay, e.g., for affinities)\n            if request_spec.roi is not None:\n                dims = request_spec.roi.dims\n                data_shape = Coordinate(array.data.shape[-dims:])\n                voxel_size = self.spec[array_key].voxel_size\n                assert data_shape == request_spec.roi.shape / voxel_size, (\n                    \"%s ROI %s requested, but size of array is %s*%s=%s provided by %s.\"\n                    % (\n                        array_key,\n                        request_spec.roi,\n                        data_shape,\n                        voxel_size,\n                        data_shape * voxel_size,\n                        self.name(),\n                    )\n                )\n            if request_spec.dtype is not None:\n                assert (\n                    batch[array_key].data.dtype == request_spec.dtype\n                ), \"dtype of array %s (%s) does not match requested dtype %s by %s\" % (\n                    array_key,\n                    batch[array_key].data.dtype,\n                    request_spec.dtype,\n                    self.name(),\n                )\n\n        for graph_key, request_spec in request.graph_specs.items():\n            assert (\n                graph_key in batch.graphs\n            ), \"%s requested, but %s did not provide it.\" % (graph_key, self.name())\n            graph = batch.graphs[graph_key]\n            assert (\n                graph.spec.roi == request_spec.roi\n            ), \"%s ROI %s requested, but ROI %s provided by %s.\" % (\n                graph_key,\n                request_spec.roi,\n                graph.spec.roi,\n                self.name(),\n            )\n\n            if request_spec.directed is not None:\n                assert request_spec.directed == graph.directed, (\n                    f\"Recieved {graph_key}:  directed={graph.directed} but \"\n                    f\"{self.name()} should provide directed={request_spec.directed}\"\n                )\n\n            for node in graph.nodes:\n                contained = graph.spec.roi.contains(node.location)\n                dangling = not contained and all(\n                    [graph.spec.roi.contains(v.location) for v in graph.neighbors(node)]\n                )\n                assert contained or dangling, (\n                    f\"graph {graph_key} provided by {self.name()} with ROI {graph.spec.roi} \"\n                    f\"contain point at {node.location} which is neither contained nor \"\n                    f\"'dangling'\"\n                )",
  "def remove_unneeded(self, batch, request):\n        batch_keys = set(list(batch.arrays.keys()) + list(batch.graphs.keys()))\n        for key in batch_keys:\n            if key not in request:\n                del batch[key]",
  "def enable_placeholders(self):\n        self._remove_placeholders = False",
  "def provide(self, request):\n        \"\"\"To be implemented in subclasses.\n\n        This function takes a :class:`BatchRequest` and should return the\n        corresponding :class:`Batch`.\n\n        Args:\n\n            request(:class:`BatchRequest`):\n\n                The request to process.\n        \"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'provide'\" % self.name())",
  "def name(self):\n        return type(self).__name__",
  "def __repr__(self):\n        return self.name() + \", providing: \" + str(self.spec)",
  "def __add__(self, other):\n        \"\"\"Support ``self + other`` operator. Return a :class:`Pipeline`.\"\"\"\n        from gunpowder import Pipeline\n\n        if isinstance(other, BatchProvider):\n            other = Pipeline(other)\n\n        if not isinstance(other, Pipeline):\n            raise RuntimeError(\n                f\"Don't know how to add {type(other)} to BatchProvider \"\n                f\"{self.name()}\"\n            )\n\n        return Pipeline(self) + other",
  "def __radd__(self, other):\n        \"\"\"Support ``other + self`` operator. Return a :class:`Pipeline`.\"\"\"\n        from gunpowder import Pipeline\n\n        if isinstance(other, BatchProvider):\n            return Pipeline(other) + Pipeline(self)\n\n        if isinstance(other, tuple):\n            return other + Pipeline(self)\n\n        raise RuntimeError(\n            f\"Don't know how to radd {type(other)} to BatchProvider\" f\"{self.name()}\"\n        )",
  "def seg_to_affgraph(seg, nhood):\n    nhood = np.array(nhood)\n\n    # constructs an affinity graph from a segmentation\n    # assume affinity graph is represented as:\n    # shape = (e, z, y, x)\n    # nhood.shape = (edges, 3)\n    shape = seg.shape\n    nEdge = nhood.shape[0]\n    dims = nhood.shape[1]\n    aff = np.zeros((nEdge,) + shape, dtype=np.int32)\n\n    if dims == 2:\n        for e in range(nEdge):\n            aff[\n                e,\n                max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n            ] = (\n                (\n                    seg[\n                        max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                        max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n                    ]\n                    == seg[\n                        max(0, nhood[e, 0]) : min(shape[0], shape[0] + nhood[e, 0]),\n                        max(0, nhood[e, 1]) : min(shape[1], shape[1] + nhood[e, 1]),\n                    ]\n                )\n                * (\n                    seg[\n                        max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                        max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n                    ]\n                    > 0\n                )\n                * (\n                    seg[\n                        max(0, nhood[e, 0]) : min(shape[0], shape[0] + nhood[e, 0]),\n                        max(0, nhood[e, 1]) : min(shape[1], shape[1] + nhood[e, 1]),\n                    ]\n                    > 0\n                )\n            )\n\n    elif dims == 3:\n        for e in range(nEdge):\n            aff[\n                e,\n                max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n                max(0, -nhood[e, 2]) : min(shape[2], shape[2] - nhood[e, 2]),\n            ] = (\n                (\n                    seg[\n                        max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                        max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n                        max(0, -nhood[e, 2]) : min(shape[2], shape[2] - nhood[e, 2]),\n                    ]\n                    == seg[\n                        max(0, nhood[e, 0]) : min(shape[0], shape[0] + nhood[e, 0]),\n                        max(0, nhood[e, 1]) : min(shape[1], shape[1] + nhood[e, 1]),\n                        max(0, nhood[e, 2]) : min(shape[2], shape[2] + nhood[e, 2]),\n                    ]\n                )\n                * (\n                    seg[\n                        max(0, -nhood[e, 0]) : min(shape[0], shape[0] - nhood[e, 0]),\n                        max(0, -nhood[e, 1]) : min(shape[1], shape[1] - nhood[e, 1]),\n                        max(0, -nhood[e, 2]) : min(shape[2], shape[2] - nhood[e, 2]),\n                    ]\n                    > 0\n                )\n                * (\n                    seg[\n                        max(0, nhood[e, 0]) : min(shape[0], shape[0] + nhood[e, 0]),\n                        max(0, nhood[e, 1]) : min(shape[1], shape[1] + nhood[e, 1]),\n                        max(0, nhood[e, 2]) : min(shape[2], shape[2] + nhood[e, 2]),\n                    ]\n                    > 0\n                )\n            )\n\n    else:\n        raise RuntimeError(f\"AddAffinities works only in 2 or 3 dimensions, not {dims}\")\n\n    return aff",
  "class AddAffinities(BatchFilter):\n    \"\"\"Add an array with affinities for a given label array and neighborhood to\n    the batch. Affinity values are created one for each voxel and entry in the\n    neighborhood list, i.e., for each voxel and each neighbor of this voxel.\n    Values are 1 iff both labels (of the voxel and the neighbor) are equal and\n    non-zero.\n\n    Args:\n\n        affinity_neighborhood (``list`` of array-like):\n\n            List of offsets for the affinities to consider for each voxel.\n\n        labels (:class:`ArrayKey`):\n\n            The array to read the labels from.\n\n        affinities (:class:`ArrayKey`):\n\n            The array to generate containing the affinities.\n\n        labels_mask (:class:`ArrayKey`, optional):\n\n            The array to use as a mask for ``labels``. Affinities connecting at\n            least one masked out label will be masked out in\n            ``affinities_mask``. If not given, ``affinities_mask`` will contain\n            ones everywhere (if requested).\n\n        unlabelled (:class:`ArrayKey`, optional):\n\n            A binary array to indicate unlabelled areas with 0. Affinities from\n            labelled to unlabelled voxels are set to 0, affinities between\n            unlabelled voxels are masked out (they will not be used for\n            training).\n\n        affinities_mask (:class:`ArrayKey`, optional):\n\n            The array to generate containing the affinitiy mask, as derived\n            from parameter ``labels_mask``.\n    \"\"\"\n\n    def __init__(\n        self,\n        affinity_neighborhood,\n        labels,\n        affinities,\n        labels_mask=None,\n        unlabelled=None,\n        affinities_mask=None,\n        dtype=np.uint8,\n    ):\n        self.affinity_neighborhood = np.array(affinity_neighborhood)\n        self.labels = labels\n        self.unlabelled = unlabelled\n        self.labels_mask = labels_mask\n        self.affinities = affinities\n        self.affinities_mask = affinities_mask\n        self.dtype = dtype\n\n    def setup(self):\n        assert self.labels in self.spec, (\n            \"Upstream does not provide %s needed by \" \"AddAffinities\" % self.labels\n        )\n\n        voxel_size = self.spec[self.labels].voxel_size\n\n        dims = self.affinity_neighborhood.shape[1]\n        self.padding_neg = (\n            Coordinate(\n                min([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        self.padding_pos = (\n            Coordinate(\n                max([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        logger.debug(\"padding neg: \" + str(self.padding_neg))\n        logger.debug(\"padding pos: \" + str(self.padding_pos))\n\n        spec = self.spec[self.labels].copy()\n        if spec.roi is not None:\n            spec.roi = spec.roi.grow(self.padding_neg, -self.padding_pos)\n        spec.dtype = self.dtype\n\n        self.provides(self.affinities, spec)\n        if self.affinities_mask:\n            self.provides(self.affinities_mask, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n\n        # grow labels ROI to accomodate padding\n        labels_roi = request[self.affinities].roi.grow(\n            -self.padding_neg, self.padding_pos\n        )\n        deps[self.labels] = request[self.affinities].copy()\n        deps[self.labels].dtype = None\n        deps[self.labels].roi = labels_roi\n\n        if self.labels_mask:\n            deps[self.labels_mask] = deps[self.labels].copy()\n        if self.unlabelled:\n            deps[self.unlabelled] = deps[self.labels].copy()\n\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n\n        affinities_roi = request[self.affinities].roi\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        affinities = seg_to_affgraph(\n            batch.arrays[self.labels].data.astype(np.int32), self.affinity_neighborhood\n        ).astype(self.dtype)\n\n        # crop affinities to requested ROI\n        offset = affinities_roi.offset\n        shift = -offset - self.padding_neg\n        crop_roi = affinities_roi.shift(shift)\n        crop_roi /= self.spec[self.labels].voxel_size\n        crop = crop_roi.get_bounding_box()\n\n        logger.debug(\"cropping with \" + str(crop))\n        affinities = affinities[(slice(None),) + crop]\n\n        spec = self.spec[self.affinities].copy()\n        spec.roi = affinities_roi\n        outputs.arrays[self.affinities] = Array(affinities, spec)\n\n        if self.affinities_mask and self.affinities_mask in request:\n            if self.labels_mask:\n                logger.debug(\n                    \"computing ground-truth affinities mask from \" \"labels mask\"\n                )\n                affinities_mask = seg_to_affgraph(\n                    batch.arrays[self.labels_mask].data.astype(np.int32),\n                    self.affinity_neighborhood,\n                )\n                affinities_mask = affinities_mask[(slice(None),) + crop]\n\n            else:\n                affinities_mask = np.ones_like(affinities)\n\n            if self.unlabelled:\n                # 1 for all affinities between unlabelled voxels\n                unlabelled = 1 - batch.arrays[self.unlabelled].data\n                unlabelled_mask = seg_to_affgraph(\n                    unlabelled.astype(np.int32), self.affinity_neighborhood\n                )\n                unlabelled_mask = unlabelled_mask[(slice(None),) + crop]\n\n                # 0 for all affinities between unlabelled voxels\n                unlabelled_mask = 1 - unlabelled_mask\n\n                # combine with mask\n                affinities_mask = affinities_mask * unlabelled_mask\n\n            affinities_mask = affinities_mask.astype(affinities.dtype)\n            outputs.arrays[self.affinities_mask] = Array(affinities_mask, spec)\n\n        else:\n            if self.labels_mask is not None:\n                logger.warning(\n                    \"GT labels does have a mask, but affinities \"\n                    \"mask is not requested.\"\n                )\n\n        # Should probably have a better way of handling arbitrary batch attributes\n        batch.affinity_neighborhood = self.affinity_neighborhood\n\n        return outputs",
  "def __init__(\n        self,\n        affinity_neighborhood,\n        labels,\n        affinities,\n        labels_mask=None,\n        unlabelled=None,\n        affinities_mask=None,\n        dtype=np.uint8,\n    ):\n        self.affinity_neighborhood = np.array(affinity_neighborhood)\n        self.labels = labels\n        self.unlabelled = unlabelled\n        self.labels_mask = labels_mask\n        self.affinities = affinities\n        self.affinities_mask = affinities_mask\n        self.dtype = dtype",
  "def setup(self):\n        assert self.labels in self.spec, (\n            \"Upstream does not provide %s needed by \" \"AddAffinities\" % self.labels\n        )\n\n        voxel_size = self.spec[self.labels].voxel_size\n\n        dims = self.affinity_neighborhood.shape[1]\n        self.padding_neg = (\n            Coordinate(\n                min([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        self.padding_pos = (\n            Coordinate(\n                max([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        logger.debug(\"padding neg: \" + str(self.padding_neg))\n        logger.debug(\"padding pos: \" + str(self.padding_pos))\n\n        spec = self.spec[self.labels].copy()\n        if spec.roi is not None:\n            spec.roi = spec.roi.grow(self.padding_neg, -self.padding_pos)\n        spec.dtype = self.dtype\n\n        self.provides(self.affinities, spec)\n        if self.affinities_mask:\n            self.provides(self.affinities_mask, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n\n        # grow labels ROI to accomodate padding\n        labels_roi = request[self.affinities].roi.grow(\n            -self.padding_neg, self.padding_pos\n        )\n        deps[self.labels] = request[self.affinities].copy()\n        deps[self.labels].dtype = None\n        deps[self.labels].roi = labels_roi\n\n        if self.labels_mask:\n            deps[self.labels_mask] = deps[self.labels].copy()\n        if self.unlabelled:\n            deps[self.unlabelled] = deps[self.labels].copy()\n\n        return deps",
  "def process(self, batch, request):\n        outputs = Batch()\n\n        affinities_roi = request[self.affinities].roi\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        affinities = seg_to_affgraph(\n            batch.arrays[self.labels].data.astype(np.int32), self.affinity_neighborhood\n        ).astype(self.dtype)\n\n        # crop affinities to requested ROI\n        offset = affinities_roi.offset\n        shift = -offset - self.padding_neg\n        crop_roi = affinities_roi.shift(shift)\n        crop_roi /= self.spec[self.labels].voxel_size\n        crop = crop_roi.get_bounding_box()\n\n        logger.debug(\"cropping with \" + str(crop))\n        affinities = affinities[(slice(None),) + crop]\n\n        spec = self.spec[self.affinities].copy()\n        spec.roi = affinities_roi\n        outputs.arrays[self.affinities] = Array(affinities, spec)\n\n        if self.affinities_mask and self.affinities_mask in request:\n            if self.labels_mask:\n                logger.debug(\n                    \"computing ground-truth affinities mask from \" \"labels mask\"\n                )\n                affinities_mask = seg_to_affgraph(\n                    batch.arrays[self.labels_mask].data.astype(np.int32),\n                    self.affinity_neighborhood,\n                )\n                affinities_mask = affinities_mask[(slice(None),) + crop]\n\n            else:\n                affinities_mask = np.ones_like(affinities)\n\n            if self.unlabelled:\n                # 1 for all affinities between unlabelled voxels\n                unlabelled = 1 - batch.arrays[self.unlabelled].data\n                unlabelled_mask = seg_to_affgraph(\n                    unlabelled.astype(np.int32), self.affinity_neighborhood\n                )\n                unlabelled_mask = unlabelled_mask[(slice(None),) + crop]\n\n                # 0 for all affinities between unlabelled voxels\n                unlabelled_mask = 1 - unlabelled_mask\n\n                # combine with mask\n                affinities_mask = affinities_mask * unlabelled_mask\n\n            affinities_mask = affinities_mask.astype(affinities.dtype)\n            outputs.arrays[self.affinities_mask] = Array(affinities_mask, spec)\n\n        else:\n            if self.labels_mask is not None:\n                logger.warning(\n                    \"GT labels does have a mask, but affinities \"\n                    \"mask is not requested.\"\n                )\n\n        # Should probably have a better way of handling arbitrary batch attributes\n        batch.affinity_neighborhood = self.affinity_neighborhood\n\n        return outputs",
  "class GrowBoundary(BatchFilter):\n    \"\"\"Grow a boundary between regions in a label array. Does not grow at the\n    border of the batch or an optionally provided mask.\n\n    Args:\n\n        labels (:class:`ArrayKey`):\n\n            The array containing labels.\n\n        mask (:class:`ArrayKey`, optional):\n\n            A mask indicating unknown regions. This is to avoid boundaries to\n            grow between labelled and unknown regions.\n\n        steps (``int``, optional):\n\n            Number of voxels (not world units!) to grow.\n\n        background (``int``, optional):\n\n            The label to assign to the boundary voxels.\n\n        only_xy (``bool``, optional):\n\n            Do not grow a boundary in the z direction.\n    \"\"\"\n\n    def __init__(self, labels, mask=None, steps=1, background=0, only_xy=False):\n        self.labels = labels\n        self.mask = mask\n        self.steps = steps\n        self.background = background\n        self.only_xy = only_xy\n\n    def process(self, batch, request):\n        gt = batch.arrays[self.labels]\n        gt_mask = None if not self.mask else batch.arrays[self.mask]\n\n        if gt_mask is not None:\n            # grow only in area where mask and gt are defined\n            crop = gt_mask.spec.roi.intersect(gt.spec.roi)\n\n            if crop is None:\n                raise RuntimeError(\n                    \"GT_LABELS %s and GT_MASK %s ROIs don't intersect.\"\n                    % (gt.spec.roi, gt_mask.spec.roi)\n                )\n            voxel_size = self.spec[self.labels].voxel_size\n            crop_in_gt = (\n                crop.shift(-gt.spec.roi.offset) / voxel_size\n            ).get_bounding_box()\n            crop_in_gt_mask = (\n                crop.shift(-gt_mask.spec.roi.offset) / voxel_size\n            ).get_bounding_box()\n\n            self.__grow(\n                gt.data[crop_in_gt], gt_mask.data[crop_in_gt_mask], self.only_xy\n            )\n\n        else:\n            self.__grow(gt.data, only_xy=self.only_xy)\n\n    def __grow(self, gt, gt_mask=None, only_xy=False):\n        if gt_mask is not None:\n            assert (\n                gt.shape == gt_mask.shape\n            ), \"GT_LABELS and GT_MASK do not have the same size.\"\n\n        if only_xy:\n            assert len(gt.shape) == 3\n            for z in range(gt.shape[0]):\n                self.__grow(gt[z], None if gt_mask is None else gt_mask[z])\n            return\n\n        # get all foreground voxels by erosion of each component\n        foreground = np.zeros(shape=gt.shape, dtype=bool)\n        masked = None\n        if gt_mask is not None:\n            masked = np.equal(gt_mask, 0)\n        for label in np.unique(gt):\n            if label == self.background:\n                continue\n            label_mask = gt == label\n            # Assume that masked out values are the same as the label we are\n            # eroding in this iteration. This ensures that at the boundary to\n            # a masked region the value blob is not shrinking.\n            if masked is not None:\n                label_mask = np.logical_or(label_mask, masked)\n            eroded_label_mask = ndimage.binary_erosion(\n                label_mask, iterations=self.steps, border_value=1\n            )\n            foreground = np.logical_or(eroded_label_mask, foreground)\n\n        # label new background\n        background = np.logical_not(foreground)\n        gt[background] = self.background",
  "def __init__(self, labels, mask=None, steps=1, background=0, only_xy=False):\n        self.labels = labels\n        self.mask = mask\n        self.steps = steps\n        self.background = background\n        self.only_xy = only_xy",
  "def process(self, batch, request):\n        gt = batch.arrays[self.labels]\n        gt_mask = None if not self.mask else batch.arrays[self.mask]\n\n        if gt_mask is not None:\n            # grow only in area where mask and gt are defined\n            crop = gt_mask.spec.roi.intersect(gt.spec.roi)\n\n            if crop is None:\n                raise RuntimeError(\n                    \"GT_LABELS %s and GT_MASK %s ROIs don't intersect.\"\n                    % (gt.spec.roi, gt_mask.spec.roi)\n                )\n            voxel_size = self.spec[self.labels].voxel_size\n            crop_in_gt = (\n                crop.shift(-gt.spec.roi.offset) / voxel_size\n            ).get_bounding_box()\n            crop_in_gt_mask = (\n                crop.shift(-gt_mask.spec.roi.offset) / voxel_size\n            ).get_bounding_box()\n\n            self.__grow(\n                gt.data[crop_in_gt], gt_mask.data[crop_in_gt_mask], self.only_xy\n            )\n\n        else:\n            self.__grow(gt.data, only_xy=self.only_xy)",
  "def __grow(self, gt, gt_mask=None, only_xy=False):\n        if gt_mask is not None:\n            assert (\n                gt.shape == gt_mask.shape\n            ), \"GT_LABELS and GT_MASK do not have the same size.\"\n\n        if only_xy:\n            assert len(gt.shape) == 3\n            for z in range(gt.shape[0]):\n                self.__grow(gt[z], None if gt_mask is None else gt_mask[z])\n            return\n\n        # get all foreground voxels by erosion of each component\n        foreground = np.zeros(shape=gt.shape, dtype=bool)\n        masked = None\n        if gt_mask is not None:\n            masked = np.equal(gt_mask, 0)\n        for label in np.unique(gt):\n            if label == self.background:\n                continue\n            label_mask = gt == label\n            # Assume that masked out values are the same as the label we are\n            # eroding in this iteration. This ensures that at the boundary to\n            # a masked region the value blob is not shrinking.\n            if masked is not None:\n                label_mask = np.logical_or(label_mask, masked)\n            eroded_label_mask = ndimage.binary_erosion(\n                label_mask, iterations=self.steps, border_value=1\n            )\n            foreground = np.logical_or(eroded_label_mask, foreground)\n\n        # label new background\n        background = np.logical_not(foreground)\n        gt[background] = self.background",
  "class Normalize(BatchFilter):\n    \"\"\"Normalize the values of an array to be floats between 0 and 1, based on\n    the type of the array.\n\n    Args:\n\n        array (:class:`ArrayKey`):\n\n            The key of the array to modify.\n\n        factor (scalar, optional):\n\n            The factor to use. If not given, a factor is chosen based on the\n            ``dtype`` of the array (e.g., ``np.uint8`` would result in a factor\n            of ``1.0/255``).\n\n        dtype (data-type, optional):\n\n            The datatype of the normalized array. Defaults to ``np.float32``.\n    \"\"\"\n\n    def __init__(self, array, factor=None, dtype=np.float32):\n        self.array = array\n        self.factor = factor\n        self.dtype = dtype\n\n    def setup(self):\n        self.enable_autoskip()\n        array_spec = self.spec[self.array].copy()\n        array_spec.dtype = self.dtype\n        self.updates(self.array, array_spec)\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array]\n        deps[self.array].dtype = None\n        return deps\n\n    def process(self, batch, request):\n        if self.array not in batch.arrays:\n            return\n\n        factor = self.factor\n        array = batch.arrays[self.array]\n        array.spec.dtype = self.dtype\n\n        if factor is None:\n            logger.debug(\n                \"automatically normalizing %s with dtype=%s\",\n                self.array,\n                array.data.dtype,\n            )\n\n            if array.data.dtype == np.uint8:\n                factor = 1.0 / 255\n            elif array.data.dtype == np.uint16:\n                factor = 1.0 / (256 * 256 - 1)\n            elif array.data.dtype == np.float32:\n                assert array.data.min() >= 0 and array.data.max() <= 1, (\n                    \"Values are float but not in [0,1], I don't know how \"\n                    \"to normalize. Please provide a factor.\"\n                )\n                factor = 1.0\n            else:\n                raise RuntimeError(\n                    \"Automatic normalization for \"\n                    + str(array.data.dtype)\n                    + \" not implemented, please \"\n                    \"provide a factor.\"\n                )\n\n        logger.debug(\"scaling %s with %f\", self.array, factor)\n        array.data = array.data.astype(self.dtype) * factor",
  "def __init__(self, array, factor=None, dtype=np.float32):\n        self.array = array\n        self.factor = factor\n        self.dtype = dtype",
  "def setup(self):\n        self.enable_autoskip()\n        array_spec = self.spec[self.array].copy()\n        array_spec.dtype = self.dtype\n        self.updates(self.array, array_spec)",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array]\n        deps[self.array].dtype = None\n        return deps",
  "def process(self, batch, request):\n        if self.array not in batch.arrays:\n            return\n\n        factor = self.factor\n        array = batch.arrays[self.array]\n        array.spec.dtype = self.dtype\n\n        if factor is None:\n            logger.debug(\n                \"automatically normalizing %s with dtype=%s\",\n                self.array,\n                array.data.dtype,\n            )\n\n            if array.data.dtype == np.uint8:\n                factor = 1.0 / 255\n            elif array.data.dtype == np.uint16:\n                factor = 1.0 / (256 * 256 - 1)\n            elif array.data.dtype == np.float32:\n                assert array.data.min() >= 0 and array.data.max() <= 1, (\n                    \"Values are float but not in [0,1], I don't know how \"\n                    \"to normalize. Please provide a factor.\"\n                )\n                factor = 1.0\n            else:\n                raise RuntimeError(\n                    \"Automatic normalization for \"\n                    + str(array.data.dtype)\n                    + \" not implemented, please \"\n                    \"provide a factor.\"\n                )\n\n        logger.debug(\"scaling %s with %f\", self.array, factor)\n        array.data = array.data.astype(self.dtype) * factor",
  "class Hdf5Source(Hdf5LikeSource):\n    \"\"\"An HDF5 data source.\n\n    Provides arrays from HDF5 datasets. If the attribute ``resolution`` is set\n    in a HDF5 dataset, it will be used as the array's ``voxel_size``. If the\n    attribute ``offset`` is set in a dataset, it will be used as the offset of\n    the :class:`Roi` for this array. It is assumed that the offset is given in\n    world units.\n\n    Args:\n\n        filename (``string``):\n\n            The HDF5 file.\n\n        datasets (``dict``, :class:`ArrayKey` -> ``string``):\n\n            Dictionary of array keys to dataset names that this source offers.\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            An optional dictionary of array keys to array specs to overwrite\n            the array specs automatically determined from the data file. This\n            is useful to set a missing ``voxel_size``, for example. Only fields\n            that are not ``None`` in the given :class:`ArraySpec` will be used.\n\n        channels_first (``bool``, optional):\n\n            Specifies the ordering of the dimensions of the HDF5-like data source.\n            If channels_first is set (default), then the input shape is expected\n            to be (channels, spatial dimensions). This is recommended because of\n            better performance. If channels_first is set to false, then the input\n            data is read in channels_last manner and converted to channels_first.\n    \"\"\"\n\n    def _open_file(self, filename):\n        return h5py.File(filename, \"r\")",
  "def _open_file(self, filename):\n        return h5py.File(filename, \"r\")",
  "class Unsqueeze(BatchFilter):\n    \"\"\"Unsqueeze a batch at a given axis\n\n    Args:\n        arrays (List[ArrayKey]): ArrayKeys to unsqueeze.\n        axis: Position where the new axis is placed, defaults to 0.\n    \"\"\"\n\n    def __init__(self, arrays: List[ArrayKey], axis: int = 0):\n        self.arrays = arrays\n        self.axis = axis\n\n    def setup(self):\n        self.enable_autoskip()\n        for array in self.arrays:\n            self.updates(array, self.spec[array].copy())\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for array in self.arrays:\n            if array in request:\n                deps[array] = request[array].copy()\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n        for array in self.arrays:\n            if array in batch:\n                if not batch[array].spec.nonspatial:\n                    spatial_dims = request[array].roi.dims\n                    if self.axis > batch[array].data.ndim - spatial_dims:\n                        raise ValueError(\n                            (\n                                f\"Unsqueeze.axis={self.axis} not permitted. \"\n                                \"Unsqueeze only supported for \"\n                                \"non-spatial dimensions of Array.\"\n                            )\n                        )\n\n                outputs[array] = batch[array]\n                outputs[array].data = np.expand_dims(batch[array].data, self.axis)\n        return outputs",
  "def __init__(self, arrays: List[ArrayKey], axis: int = 0):\n        self.arrays = arrays\n        self.axis = axis",
  "def setup(self):\n        self.enable_autoskip()\n        for array in self.arrays:\n            self.updates(array, self.spec[array].copy())",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for array in self.arrays:\n            if array in request:\n                deps[array] = request[array].copy()\n        return deps",
  "def process(self, batch, request):\n        outputs = Batch()\n        for array in self.arrays:\n            if array in batch:\n                if not batch[array].spec.nonspatial:\n                    spatial_dims = request[array].roi.dims\n                    if self.axis > batch[array].data.ndim - spatial_dims:\n                        raise ValueError(\n                            (\n                                f\"Unsqueeze.axis={self.axis} not permitted. \"\n                                \"Unsqueeze only supported for \"\n                                \"non-spatial dimensions of Array.\"\n                            )\n                        )\n\n                outputs[array] = batch[array]\n                outputs[array].data = np.expand_dims(batch[array].data, self.axis)\n        return outputs",
  "class SimpleAugment(BatchFilter):\n    \"\"\"Randomly mirror and transpose all :class:`Arrays<Array>` and\n    :class:`Graph` in a batch.\n\n    Args:\n\n        mirror_only (``list`` of ``int``, optional):\n\n            If set, only mirror between the given axes. This is useful to\n            exclude channels that have a set direction, like time.\n\n        transpose_only (``list`` of ``int``, optional):\n\n            If set, only transpose between the given axes. This is useful to\n            limit the transpose to axes with the same resolution or to exclude\n            non-spatial dimensions.\n\n        mirror_probs (``list`` of ``float``, optional):\n\n            If set, provides the probability for mirroring given axes. Default\n            is 0.5 per axis. If given, must be given for every axis. i.e.\n            [0,1,0] for 100% chance of mirroring axis 1 an no others.\n\n        transpose_probs (``dict`` of ``tuple`` -> ``float``\n        or ``list`` of ``float``, optional):\n\n            The probability of transposing. If None, each transpose is equally\n            likely.\n            Can also be a dictionary of for ``tuple`` -> ``float``. For example\n            {(0,1,2):0.5, (1,0,2):0.5} to define a 50% chance of transposing axes\n            0 and 1. Note that if a provided option violates the `transpose_only`\n            arg it will be dropped and remaining options will be reweighted.\n            Can also be provided as a list of ``float``. i.e. [0.3, 0.5, 0.7].\n            This will automatically generate a list of possible permutations\n            and attempt to weight them appropriately. A weight of 0 means\n            this axis will never be transposed, a weight of 1 means this axis\n            will always be transposed.\n    \"\"\"\n\n    def __init__(\n        self,\n        mirror_only=None,\n        transpose_only=None,\n        mirror_probs=None,\n        transpose_probs=None,\n    ):\n        self.mirror_only = mirror_only\n        self.mirror_probs = mirror_probs\n        self.transpose_only = transpose_only\n        self.transpose_probs = transpose_probs\n        self.mirror_mask = None\n        self.dims = None\n        self.transpose_dims = None\n\n    def setup(self):\n        self.dims = self.spec.get_total_roi().dims\n\n        # mirror_mask and transpose_dims refer to the indices of the spatial\n        # dimensions only, starting counting at 0 for the first spatial\n        # dimension\n\n        if self.mirror_only is None:\n            self.mirror_mask = [True] * self.dims\n        else:\n            self.mirror_mask = [d in self.mirror_only for d in range(self.dims)]\n        if self.mirror_probs is None:\n            self.mirror_probs = [0.5] * self.dims\n\n        if self.transpose_only is None:\n            self.transpose_dims = list(range(self.dims))\n        else:\n            self.transpose_dims = self.transpose_only\n        if self.transpose_probs is None:\n            self.permutation_dict = None\n        elif isinstance(self.transpose_probs, list):\n            self.permutation_dict = {}\n            for permutation in itertools.permutations(range(self.dims), self.dims):\n                total_prob = 1\n                for i, j, p in zip(range(self.dims), permutation, self.transpose_probs):\n                    if i not in self.transpose_dims and i != j:\n                        total_prob = 0\n                    else:\n                        total_prob *= (1 - p) if i == j else p\n                if total_prob > 0:\n                    self.permutation_dict[permutation] = total_prob\n        elif isinstance(self.transpose_probs, dict):\n            self.permutation_dict = {}\n            for k, v in self.transpose_probs.items():\n                valid = True\n                for i, j in enumerate(k):\n                    if i not in self.transpose_only and i != j:\n                        valid = False\n                if valid:\n                    self.permutation_dict[k] = v\n\n    def prepare(self, request):\n\n        self.mirror = [\n            random.random() < self.mirror_probs[d] if self.mirror_mask[d] else 0\n            for d in range(self.dims)\n        ]\n\n        if self.permutation_dict is not None:\n            t = random.choices(\n                list(self.permutation_dict.keys()),\n                weights=list(self.permutation_dict.values()),\n                k=1,\n            )[0]\n        else:\n            t = random.sample(self.transpose_dims, k=len(self.transpose_dims))\n\n        self.transpose = list(range(self.dims))\n        for o, n in zip(self.transpose_dims, t):\n            self.transpose[o] = n\n\n        logger.debug(\"mirror = %s\", self.mirror)\n        logger.debug(\"transpose = %s\", self.transpose)\n\n        reverse_transpose = [0] * self.dims\n        for d in range(self.dims):\n            reverse_transpose[self.transpose[d]] = d\n\n        logger.debug(\"downstream request = %s\", request)\n\n        self.__transpose_request(request, reverse_transpose)\n        self.__mirror_request(request, self.mirror)\n\n        logger.debug(\"upstream request = %s\", request)\n\n        return request\n\n    def process(self, batch, request):\n        # mirror and transpose ROIs of arrays & points in batch\n        total_roi = batch.get_total_roi().copy()\n        requested_keys = request.array_specs.keys()\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(requested_keys)\n\n        for collection_type in [batch.arrays, batch.graphs]:\n            for key, collector in collection_type.items():\n                if key not in request:\n                    continue\n                if collector.spec.roi is None:\n                    continue\n                logger.debug(\"total ROI = %s\", batch.get_total_roi())\n                logger.debug(\"upstream %s ROI = %s\", key, collector.spec.roi)\n                self.__mirror_roi(collector.spec.roi, total_roi, self.mirror)\n                logger.debug(\"mirrored %s ROI = %s\", key, collector.spec.roi)\n                self.__transpose_roi(\n                    collector.spec.roi, total_roi, self.transpose, lcm_voxel_size\n                )\n                logger.debug(\"transposed %s ROI = %s\", key, collector.spec.roi)\n\n        mirror = tuple(slice(None, None, -1 if m else 1) for m in self.mirror)\n        # arrays\n        for array_key, array in batch.arrays.items():\n            if array_key not in request:\n                continue\n\n            if array.spec.nonspatial:\n                continue\n\n            num_channels = len(array.data.shape) - self.dims\n            channel_slices = (slice(None, None),) * num_channels\n\n            array.data = array.data[channel_slices + mirror]\n\n            transpose = [t + num_channels for t in self.transpose]\n            array.data = array.data = array.data.transpose(\n                list(range(num_channels)) + transpose\n            )\n\n        # graphs\n        total_roi_offset = total_roi.offset\n        total_roi_center = total_roi.center\n        if lcm_voxel_size is not None:\n            nearest_voxel_shift = Coordinate(\n                (d % v) for d, v in zip(total_roi_center, lcm_voxel_size)\n            )\n            total_roi_center = total_roi_center - nearest_voxel_shift\n        total_roi_end = total_roi.end\n        logger.debug(\"augmenting in %s and center %s\", total_roi, total_roi_center)\n\n        for graph_key, graph in batch.graphs.items():\n            if graph_key not in request:\n                continue\n\n            logger.debug(\"converting nodes in graph %s\", graph_key)\n            for node in list(graph.nodes):\n                logger.debug(\"old location: %s, %s\", node.id, node.location)\n\n                # mirror\n                location_in_total_offset = np.asarray(node.location) - total_roi_offset\n                node.location = np.asarray(\n                    [\n                        total_roi_end[dim] - location_in_total_offset[dim]\n                        if m\n                        else node.location[dim]\n                        for dim, m in enumerate(self.mirror)\n                    ],\n                    dtype=graph.spec.dtype,\n                )\n\n                logger.debug(\"after mirror: %s, %s\", node.id, node.location)\n\n                # transpose\n                location_in_total_center = np.asarray(node.location) - total_roi_center\n\n                if self.transpose != list(range(self.dims)):\n                    for d in range(self.dims):\n                        node.location[d] = (\n                            location_in_total_center[self.transpose[d]]\n                            + total_roi_center[d]\n                        )\n\n                logger.debug(\"after transpose: %s, %s\", node.id, node.location)\n\n                # due to the mirroring, points at the lower boundary of the ROI\n                # could fall on the upper one, which excludes them from the ROI\n                if not graph.spec.roi.contains(node.location):\n                    graph.remove_node(node)\n\n    def __mirror_request(self, request, mirror):\n        total_roi = request.get_total_roi().copy()\n        for key, spec in request.items():\n            if spec.roi is not None:\n                self.__mirror_roi(spec.roi, total_roi, mirror)\n\n    def __transpose_request(self, request, transpose):\n        total_roi = request.get_total_roi().copy()\n        requested_keys = request.array_specs.keys()\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(requested_keys)\n        for key, spec in request.items():\n            if spec.roi is not None:\n                self.__transpose_roi(spec.roi, total_roi, transpose, lcm_voxel_size)\n\n    def __mirror_roi(self, roi, total_roi, mirror):\n        total_roi_offset = total_roi.offset\n        total_roi_shape = total_roi.shape\n\n        roi_offset = roi.offset\n        roi_shape = roi.shape\n\n        roi_in_total_offset = roi_offset - total_roi_offset\n        end_of_roi_in_total = roi_in_total_offset + roi_shape\n        roi_in_total_offset_mirrored = total_roi_shape - end_of_roi_in_total\n        roi_offset = Coordinate(\n            total_roi_offset[d] + roi_in_total_offset_mirrored[d]\n            if mirror[d]\n            else roi_offset[d]\n            for d in range(self.dims)\n        )\n\n        roi.offset = roi_offset\n\n    def __transpose_roi(self, roi, total_roi, transpose, lcm_voxel_size):\n        logger.debug(\"original roi = %s\", roi)\n\n        center = total_roi.center\n        if lcm_voxel_size is not None:\n            nearest_voxel_shift = Coordinate(\n                (d % v) for d, v in zip(center, lcm_voxel_size)\n            )\n            center = center - nearest_voxel_shift\n        logger.debug(\"center = %s\", center)\n\n        # Get distance from center, then transpose\n        dist_to_center = center - roi.offset\n        dist_to_center = Coordinate(\n            dist_to_center[transpose[d]] for d in range(self.dims)\n        )\n        logger.debug(\"dist_to_center = %s\", dist_to_center)\n\n        # Using the tranposed distance to center, get the correct offset.\n        new_offset = center - dist_to_center\n        logger.debug(\"new_offset = %s\", new_offset)\n\n        shape = tuple(roi.shape[transpose[d]] for d in range(self.dims))\n        roi.offset = new_offset\n        roi.shape = shape\n        logger.debug(\"tranposed roi = %s\", roi)",
  "def __init__(\n        self,\n        mirror_only=None,\n        transpose_only=None,\n        mirror_probs=None,\n        transpose_probs=None,\n    ):\n        self.mirror_only = mirror_only\n        self.mirror_probs = mirror_probs\n        self.transpose_only = transpose_only\n        self.transpose_probs = transpose_probs\n        self.mirror_mask = None\n        self.dims = None\n        self.transpose_dims = None",
  "def setup(self):\n        self.dims = self.spec.get_total_roi().dims\n\n        # mirror_mask and transpose_dims refer to the indices of the spatial\n        # dimensions only, starting counting at 0 for the first spatial\n        # dimension\n\n        if self.mirror_only is None:\n            self.mirror_mask = [True] * self.dims\n        else:\n            self.mirror_mask = [d in self.mirror_only for d in range(self.dims)]\n        if self.mirror_probs is None:\n            self.mirror_probs = [0.5] * self.dims\n\n        if self.transpose_only is None:\n            self.transpose_dims = list(range(self.dims))\n        else:\n            self.transpose_dims = self.transpose_only\n        if self.transpose_probs is None:\n            self.permutation_dict = None\n        elif isinstance(self.transpose_probs, list):\n            self.permutation_dict = {}\n            for permutation in itertools.permutations(range(self.dims), self.dims):\n                total_prob = 1\n                for i, j, p in zip(range(self.dims), permutation, self.transpose_probs):\n                    if i not in self.transpose_dims and i != j:\n                        total_prob = 0\n                    else:\n                        total_prob *= (1 - p) if i == j else p\n                if total_prob > 0:\n                    self.permutation_dict[permutation] = total_prob\n        elif isinstance(self.transpose_probs, dict):\n            self.permutation_dict = {}\n            for k, v in self.transpose_probs.items():\n                valid = True\n                for i, j in enumerate(k):\n                    if i not in self.transpose_only and i != j:\n                        valid = False\n                if valid:\n                    self.permutation_dict[k] = v",
  "def prepare(self, request):\n\n        self.mirror = [\n            random.random() < self.mirror_probs[d] if self.mirror_mask[d] else 0\n            for d in range(self.dims)\n        ]\n\n        if self.permutation_dict is not None:\n            t = random.choices(\n                list(self.permutation_dict.keys()),\n                weights=list(self.permutation_dict.values()),\n                k=1,\n            )[0]\n        else:\n            t = random.sample(self.transpose_dims, k=len(self.transpose_dims))\n\n        self.transpose = list(range(self.dims))\n        for o, n in zip(self.transpose_dims, t):\n            self.transpose[o] = n\n\n        logger.debug(\"mirror = %s\", self.mirror)\n        logger.debug(\"transpose = %s\", self.transpose)\n\n        reverse_transpose = [0] * self.dims\n        for d in range(self.dims):\n            reverse_transpose[self.transpose[d]] = d\n\n        logger.debug(\"downstream request = %s\", request)\n\n        self.__transpose_request(request, reverse_transpose)\n        self.__mirror_request(request, self.mirror)\n\n        logger.debug(\"upstream request = %s\", request)\n\n        return request",
  "def process(self, batch, request):\n        # mirror and transpose ROIs of arrays & points in batch\n        total_roi = batch.get_total_roi().copy()\n        requested_keys = request.array_specs.keys()\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(requested_keys)\n\n        for collection_type in [batch.arrays, batch.graphs]:\n            for key, collector in collection_type.items():\n                if key not in request:\n                    continue\n                if collector.spec.roi is None:\n                    continue\n                logger.debug(\"total ROI = %s\", batch.get_total_roi())\n                logger.debug(\"upstream %s ROI = %s\", key, collector.spec.roi)\n                self.__mirror_roi(collector.spec.roi, total_roi, self.mirror)\n                logger.debug(\"mirrored %s ROI = %s\", key, collector.spec.roi)\n                self.__transpose_roi(\n                    collector.spec.roi, total_roi, self.transpose, lcm_voxel_size\n                )\n                logger.debug(\"transposed %s ROI = %s\", key, collector.spec.roi)\n\n        mirror = tuple(slice(None, None, -1 if m else 1) for m in self.mirror)\n        # arrays\n        for array_key, array in batch.arrays.items():\n            if array_key not in request:\n                continue\n\n            if array.spec.nonspatial:\n                continue\n\n            num_channels = len(array.data.shape) - self.dims\n            channel_slices = (slice(None, None),) * num_channels\n\n            array.data = array.data[channel_slices + mirror]\n\n            transpose = [t + num_channels for t in self.transpose]\n            array.data = array.data = array.data.transpose(\n                list(range(num_channels)) + transpose\n            )\n\n        # graphs\n        total_roi_offset = total_roi.offset\n        total_roi_center = total_roi.center\n        if lcm_voxel_size is not None:\n            nearest_voxel_shift = Coordinate(\n                (d % v) for d, v in zip(total_roi_center, lcm_voxel_size)\n            )\n            total_roi_center = total_roi_center - nearest_voxel_shift\n        total_roi_end = total_roi.end\n        logger.debug(\"augmenting in %s and center %s\", total_roi, total_roi_center)\n\n        for graph_key, graph in batch.graphs.items():\n            if graph_key not in request:\n                continue\n\n            logger.debug(\"converting nodes in graph %s\", graph_key)\n            for node in list(graph.nodes):\n                logger.debug(\"old location: %s, %s\", node.id, node.location)\n\n                # mirror\n                location_in_total_offset = np.asarray(node.location) - total_roi_offset\n                node.location = np.asarray(\n                    [\n                        total_roi_end[dim] - location_in_total_offset[dim]\n                        if m\n                        else node.location[dim]\n                        for dim, m in enumerate(self.mirror)\n                    ],\n                    dtype=graph.spec.dtype,\n                )\n\n                logger.debug(\"after mirror: %s, %s\", node.id, node.location)\n\n                # transpose\n                location_in_total_center = np.asarray(node.location) - total_roi_center\n\n                if self.transpose != list(range(self.dims)):\n                    for d in range(self.dims):\n                        node.location[d] = (\n                            location_in_total_center[self.transpose[d]]\n                            + total_roi_center[d]\n                        )\n\n                logger.debug(\"after transpose: %s, %s\", node.id, node.location)\n\n                # due to the mirroring, points at the lower boundary of the ROI\n                # could fall on the upper one, which excludes them from the ROI\n                if not graph.spec.roi.contains(node.location):\n                    graph.remove_node(node)",
  "def __mirror_request(self, request, mirror):\n        total_roi = request.get_total_roi().copy()\n        for key, spec in request.items():\n            if spec.roi is not None:\n                self.__mirror_roi(spec.roi, total_roi, mirror)",
  "def __transpose_request(self, request, transpose):\n        total_roi = request.get_total_roi().copy()\n        requested_keys = request.array_specs.keys()\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(requested_keys)\n        for key, spec in request.items():\n            if spec.roi is not None:\n                self.__transpose_roi(spec.roi, total_roi, transpose, lcm_voxel_size)",
  "def __mirror_roi(self, roi, total_roi, mirror):\n        total_roi_offset = total_roi.offset\n        total_roi_shape = total_roi.shape\n\n        roi_offset = roi.offset\n        roi_shape = roi.shape\n\n        roi_in_total_offset = roi_offset - total_roi_offset\n        end_of_roi_in_total = roi_in_total_offset + roi_shape\n        roi_in_total_offset_mirrored = total_roi_shape - end_of_roi_in_total\n        roi_offset = Coordinate(\n            total_roi_offset[d] + roi_in_total_offset_mirrored[d]\n            if mirror[d]\n            else roi_offset[d]\n            for d in range(self.dims)\n        )\n\n        roi.offset = roi_offset",
  "def __transpose_roi(self, roi, total_roi, transpose, lcm_voxel_size):\n        logger.debug(\"original roi = %s\", roi)\n\n        center = total_roi.center\n        if lcm_voxel_size is not None:\n            nearest_voxel_shift = Coordinate(\n                (d % v) for d, v in zip(center, lcm_voxel_size)\n            )\n            center = center - nearest_voxel_shift\n        logger.debug(\"center = %s\", center)\n\n        # Get distance from center, then transpose\n        dist_to_center = center - roi.offset\n        dist_to_center = Coordinate(\n            dist_to_center[transpose[d]] for d in range(self.dims)\n        )\n        logger.debug(\"dist_to_center = %s\", dist_to_center)\n\n        # Using the tranposed distance to center, get the correct offset.\n        new_offset = center - dist_to_center\n        logger.debug(\"new_offset = %s\", new_offset)\n\n        shape = tuple(roi.shape[transpose[d]] for d in range(self.dims))\n        roi.offset = new_offset\n        roi.shape = shape\n        logger.debug(\"tranposed roi = %s\", roi)",
  "class Reject(BatchFilter):\n    \"\"\"Reject batches based on the masked-in vs. masked-out ratio.\n\n    If a pipeline also contains a :class:`RandomLocation` node,\n    :class:`Reject` needs to be placed downstream of it.\n\n    Args:\n\n        mask (:class:`ArrayKey`, optional):\n\n            The mask to use, if any.\n\n        min_masked (``float``, optional):\n\n            The minimal required ratio of masked-in vs. masked-out voxels.\n            Defaults to 0.5.\n\n        ensure_nonempty (:class:`GraphKey`, optional)\n\n            Ensures there is at least one point in the batch.\n\n        reject_probability (``float``, optional):\n\n            The probability by which a batch that is not valid (less than\n            min_masked) is actually rejected. Defaults to 1., i.e. strict\n            rejection.\n    \"\"\"\n\n    def __init__(\n        self, mask=None, min_masked=0.5, ensure_nonempty=None, reject_probability=1.0\n    ):\n        self.mask = mask\n        self.min_masked = min_masked\n        self.ensure_nonempty = ensure_nonempty\n        self.reject_probability = reject_probability\n\n    def setup(self):\n        if self.mask:\n            assert self.mask in self.spec, (\n                \"Reject can only be used if %s is provided\" % self.mask\n            )\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in self.spec, (\n                \"Reject can only be used if %s is provided\" % self.ensure_nonempty\n            )\n        self.upstream_provider = self.get_upstream_provider()\n\n    def provide(self, request):\n\n        report_next_timeout = 10\n        num_rejected = 0\n\n        timing = Timing(self)\n        timing.start()\n        if self.mask:\n            assert self.mask in request, (\n                \"Reject can only be used if %s is provided\" % self.mask\n            )\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in request, (\n                \"Reject can only be used if %s is provided\" % self.ensure_nonempty\n            )\n\n        have_good_batch = False\n        while not have_good_batch:\n            batch = self.upstream_provider.request_batch(request)\n\n            if self.mask:\n                mask_ratio = batch.arrays[self.mask].data.mean()\n            else:\n                mask_ratio = None\n\n            if self.ensure_nonempty:\n                num_points = len(list(batch.graphs[self.ensure_nonempty].nodes))\n            else:\n                num_points = None\n\n            have_min_mask = mask_ratio is None or mask_ratio > self.min_masked\n            have_points = num_points is None or num_points > 0\n\n            have_good_batch = have_min_mask and have_points\n\n            if not have_good_batch and self.reject_probability < 1.0:\n                have_good_batch = random.random() > self.reject_probability\n\n            if not have_good_batch:\n                if self.mask:\n                    logger.debug(\n                        \"reject batch with mask ratio %f at %s\",\n                        mask_ratio,\n                        batch.arrays[self.mask].spec.roi,\n                    )\n                if self.ensure_nonempty:\n                    logger.debug(\n                        \"reject batch with empty points in %s\",\n                        batch.graphs[self.ensure_nonempty].spec.roi,\n                    )\n                num_rejected += 1\n\n                if timing.elapsed() > report_next_timeout:\n                    logger.warning(\n                        \"rejected %d batches, been waiting for a good one \" \"since %ds\",\n                        num_rejected,\n                        report_next_timeout,\n                    )\n                    report_next_timeout *= 2\n\n            else:\n                if self.mask:\n                    logger.debug(\n                        \"accepted batch with mask ratio %f at %s\",\n                        mask_ratio,\n                        batch.arrays[self.mask].spec.roi,\n                    )\n                if self.ensure_nonempty:\n                    logger.debug(\n                        \"accepted batch with nonempty points in %s\",\n                        self.ensure_nonempty,\n                    )\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __init__(\n        self, mask=None, min_masked=0.5, ensure_nonempty=None, reject_probability=1.0\n    ):\n        self.mask = mask\n        self.min_masked = min_masked\n        self.ensure_nonempty = ensure_nonempty\n        self.reject_probability = reject_probability",
  "def setup(self):\n        if self.mask:\n            assert self.mask in self.spec, (\n                \"Reject can only be used if %s is provided\" % self.mask\n            )\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in self.spec, (\n                \"Reject can only be used if %s is provided\" % self.ensure_nonempty\n            )\n        self.upstream_provider = self.get_upstream_provider()",
  "def provide(self, request):\n\n        report_next_timeout = 10\n        num_rejected = 0\n\n        timing = Timing(self)\n        timing.start()\n        if self.mask:\n            assert self.mask in request, (\n                \"Reject can only be used if %s is provided\" % self.mask\n            )\n        if self.ensure_nonempty:\n            assert self.ensure_nonempty in request, (\n                \"Reject can only be used if %s is provided\" % self.ensure_nonempty\n            )\n\n        have_good_batch = False\n        while not have_good_batch:\n            batch = self.upstream_provider.request_batch(request)\n\n            if self.mask:\n                mask_ratio = batch.arrays[self.mask].data.mean()\n            else:\n                mask_ratio = None\n\n            if self.ensure_nonempty:\n                num_points = len(list(batch.graphs[self.ensure_nonempty].nodes))\n            else:\n                num_points = None\n\n            have_min_mask = mask_ratio is None or mask_ratio > self.min_masked\n            have_points = num_points is None or num_points > 0\n\n            have_good_batch = have_min_mask and have_points\n\n            if not have_good_batch and self.reject_probability < 1.0:\n                have_good_batch = random.random() > self.reject_probability\n\n            if not have_good_batch:\n                if self.mask:\n                    logger.debug(\n                        \"reject batch with mask ratio %f at %s\",\n                        mask_ratio,\n                        batch.arrays[self.mask].spec.roi,\n                    )\n                if self.ensure_nonempty:\n                    logger.debug(\n                        \"reject batch with empty points in %s\",\n                        batch.graphs[self.ensure_nonempty].spec.roi,\n                    )\n                num_rejected += 1\n\n                if timing.elapsed() > report_next_timeout:\n                    logger.warning(\n                        \"rejected %d batches, been waiting for a good one \" \"since %ds\",\n                        num_rejected,\n                        report_next_timeout,\n                    )\n                    report_next_timeout *= 2\n\n            else:\n                if self.mask:\n                    logger.debug(\n                        \"accepted batch with mask ratio %f at %s\",\n                        mask_ratio,\n                        batch.arrays[self.mask].spec.roi,\n                    )\n                if self.ensure_nonempty:\n                    logger.debug(\n                        \"accepted batch with nonempty points in %s\",\n                        self.ensure_nonempty,\n                    )\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "class DaisyRequestBlocks(BatchFilter):\n    \"\"\"Iteratively requests batches similar to ``reference`` from upstream\n    providers, with their ROIs set to blocks distributed by ``daisy``.\n\n    The ROIs of the array or point specs in the reference can be set to either\n    the block's ``read_roi`` or ``write_roi``, see parameter ``roi_map``.\n\n    The batch request to this node has to be empty, as there is no guarantee\n    that this node will get to process all chunks required to fulfill a\n    particular batch request.\n\n    Args:\n\n        reference (:class:`BatchRequest`):\n\n            A reference :class:`BatchRequest`. This request will be shifted\n            according to blocks distributed by ``daisy``.\n\n        roi_map (``dict`` from :class:`ArrayKey` or :class:`GraphKey` to\n        ``string``):\n\n            A map indicating which daisy block ROI (``read_roi`` or\n            ``write_roi``) to use for which item in the reference request.\n\n        num_workers (``int``, optional):\n\n            If set to >1, upstream requests are made in parallel with that\n            number of workers.\n\n        block_done_callback (function, optional):\n\n            If given, will be called with arguments ``(block, start,\n            duration)`` for each block that was processed. ``start`` and\n            ``duration`` will be given in seconds, as in ``start =\n            time.time()`` and ``duration = time.time() - start``, right before\n            and after a block gets processed.\n\n            This callback can be used to log blocks that have successfully\n            finished processing, which can be used in ``check_function`` of\n            ``daisy.run_blockwise`` to skip already processed blocks in\n            repeated runs.\n    \"\"\"\n\n    def __init__(self, reference, roi_map, num_workers=1, block_done_callback=None):\n        self.reference = reference\n        self.roi_map = roi_map\n        self.num_workers = num_workers\n        self.block_done_callback = block_done_callback\n\n        if num_workers > 1:\n            self.request_queue = multiprocessing.Queue(maxsize=0)\n\n    def provide(self, request):\n        empty_request = len(request) == 0\n        if not empty_request:\n            raise RuntimeError(\"requests made to DaisyRequestBlocks have to be empty\")\n\n        if self.num_workers > 1:\n            self.workers = [\n                multiprocessing.Process(target=self.__get_chunks)\n                for _ in range(self.num_workers)\n            ]\n\n            for worker in self.workers:\n                worker.start()\n\n            for worker in self.workers:\n                worker.join()\n\n        else:\n            self.__get_chunks()\n\n        return Batch()\n\n    def __get_chunks(self):\n        daisy_client = daisy.Client()\n\n        while True:\n            with daisy_client.acquire_block() as block:\n                if block is None:\n                    return\n\n                logger.info(\"Processing block %s\", block)\n                start = time.time()\n\n                chunk_request = self.reference.copy()\n\n                for key, reference_spec in self.reference.items():\n                    roi_type = self.roi_map.get(key, None)\n\n                    if roi_type is None:\n                        raise RuntimeError(\n                            \"roi_map does not map item %s to either 'read_roi' \"\n                            \"or 'write_roi'\" % key\n                        )\n\n                    if roi_type == \"read_roi\":\n                        chunk_request[key].roi = Roi(\n                            block.read_roi.offset, block.read_roi.shape\n                        )\n                    elif roi_type == \"write_roi\":\n                        chunk_request[key].roi = Roi(\n                            block.write_roi.offset, block.write_roi.shape\n                        )\n                    else:\n                        raise RuntimeError(\n                            \"%s is not a vaid ROI type (read_roi or write_roi)\"\n                        )\n\n                self.get_upstream_provider().request_batch(chunk_request)\n\n                end = time.time()\n                if self.block_done_callback:\n                    self.block_done_callback(block, start, end - start)",
  "def __init__(self, reference, roi_map, num_workers=1, block_done_callback=None):\n        self.reference = reference\n        self.roi_map = roi_map\n        self.num_workers = num_workers\n        self.block_done_callback = block_done_callback\n\n        if num_workers > 1:\n            self.request_queue = multiprocessing.Queue(maxsize=0)",
  "def provide(self, request):\n        empty_request = len(request) == 0\n        if not empty_request:\n            raise RuntimeError(\"requests made to DaisyRequestBlocks have to be empty\")\n\n        if self.num_workers > 1:\n            self.workers = [\n                multiprocessing.Process(target=self.__get_chunks)\n                for _ in range(self.num_workers)\n            ]\n\n            for worker in self.workers:\n                worker.start()\n\n            for worker in self.workers:\n                worker.join()\n\n        else:\n            self.__get_chunks()\n\n        return Batch()",
  "def __get_chunks(self):\n        daisy_client = daisy.Client()\n\n        while True:\n            with daisy_client.acquire_block() as block:\n                if block is None:\n                    return\n\n                logger.info(\"Processing block %s\", block)\n                start = time.time()\n\n                chunk_request = self.reference.copy()\n\n                for key, reference_spec in self.reference.items():\n                    roi_type = self.roi_map.get(key, None)\n\n                    if roi_type is None:\n                        raise RuntimeError(\n                            \"roi_map does not map item %s to either 'read_roi' \"\n                            \"or 'write_roi'\" % key\n                        )\n\n                    if roi_type == \"read_roi\":\n                        chunk_request[key].roi = Roi(\n                            block.read_roi.offset, block.read_roi.shape\n                        )\n                    elif roi_type == \"write_roi\":\n                        chunk_request[key].roi = Roi(\n                            block.write_roi.offset, block.write_roi.shape\n                        )\n                    else:\n                        raise RuntimeError(\n                            \"%s is not a vaid ROI type (read_roi or write_roi)\"\n                        )\n\n                self.get_upstream_provider().request_batch(chunk_request)\n\n                end = time.time()\n                if self.block_done_callback:\n                    self.block_done_callback(block, start, end - start)",
  "class ElasticAugment(BatchFilter):\n    \"\"\"(DEPRICATED) Elasticly deform a batch. Requests larger batches upstream to avoid data\n    loss due to rotation and jitter.\n\n    Args:\n\n        control_point_spacing (``tuple`` of ``int``):\n\n            Distance between control points for the elastic deformation, in\n            voxels per dimension.\n\n        jitter_sigma (``tuple`` of ``float``):\n\n            Standard deviation of control point jitter distribution, in voxels\n            per dimension.\n\n        rotation_interval (``tuple`` of two ``floats``):\n\n            Interval to randomly sample rotation angles from (0, 2PI).\n\n        scale_interval (``tuple`` of two ``floats``):\n\n            Interval to randomly sample scale factors from.\n\n        prob_slip (``float``):\n\n            Probability of a section to \"slip\", i.e., be independently moved in\n            x-y.\n\n        prob_shift (``float``):\n\n            Probability of a section and all following sections to move in x-y.\n\n        max_misalign (``int``):\n\n            Maximal voxels to shift in x and y. Samples will be drawn\n            uniformly. Used if ``prob_slip + prob_shift`` > 0.\n\n        subsample (``int``):\n\n            Instead of creating an elastic transformation on the full\n            resolution, create one subsampled by the given factor, and linearly\n            interpolate to obtain the full resolution transformation. This can\n            significantly speed up this node, at the expense of having visible\n            piecewise linear deformations for large factors. Usually, a factor\n            of 4 can savely by used without noticable changes. However, the\n            default is 1 (i.e., no subsampling).\n\n        spatial_dims (``int``):\n\n            The number of spatial dimensions in arrays. Spatial dimensions are\n            assumed to be the last ones and cannot be more than 3 (default).\n            Set this value here to avoid treating channels as spacial\n            dimension. If, for example, your array is indexed as ``(c,y,x)``\n            (2D plus channels), you would want to set ``spatial_dims=2`` to\n            perform the elastic deformation only on x and y.\n\n        use_fast_points_transform (``bool``):\n\n            By solving for all of your points simultaneously with the following\n            3 step proceedure:\n            1) Rasterize nodes into numpy array\n            2) Apply elastic transform to array\n            3) Read out nodes via center of mass of transformed points\n            You can gain substantial speed up as opposed to calculating the\n            elastic transform for each point individually. However this may\n            lead to nodes being lost during the transform.\n\n        recompute_missing_points (``bool``):\n\n            Whether or not to compute the elastic transform node wise for nodes\n            that were lossed during the fast elastic transform process.\n    \"\"\"\n\n    def __init__(\n        self,\n        control_point_spacing,\n        jitter_sigma,\n        rotation_interval,\n        scale_interval=(1.0, 1.0),\n        prob_slip=0,\n        prob_shift=0,\n        max_misalign=0,\n        subsample=1,\n        spatial_dims=3,\n        use_fast_points_transform=False,\n        recompute_missing_points=True,\n    ):\n        warnings.warn(\n            \"ElasticAugment is deprecated, please use the DeformAugment\",\n            DeprecationWarning,\n        )\n\n        self.control_point_spacing = control_point_spacing\n        self.jitter_sigma = jitter_sigma\n        self.rotation_start = rotation_interval[0]\n        self.rotation_max_amount = rotation_interval[1] - rotation_interval[0]\n        self.scale_min = scale_interval[0]\n        self.scale_max = scale_interval[1]\n        self.prob_slip = prob_slip\n        self.prob_shift = prob_shift\n        self.max_misalign = max_misalign\n        self.subsample = subsample\n        self.spatial_dims = spatial_dims\n        self.use_fast_points_transform = use_fast_points_transform\n        self.recompute_missing_points = recompute_missing_points\n\n    def prepare(self, request):\n\n        # get the voxel size\n        self.voxel_size = self.__get_common_voxel_size(request)\n\n        # get the total ROI of all requests\n        total_roi = request.get_total_roi()\n        logger.debug(\"total ROI is %s\" % total_roi)\n\n        # First, get the total ROI of the request in spatial dimensions only.\n        # Channels and time don't matter. This is our master ROI.\n\n        # get master ROI\n        master_roi = Roi(\n            total_roi.begin[-self.spatial_dims :],\n            total_roi.shape[-self.spatial_dims :],\n        )\n        self.spatial_dims = master_roi.dims\n        logger.debug(\"master ROI is %s\" % master_roi)\n\n        # make sure the master ROI aligns with the voxel size\n        master_roi = master_roi.snap_to_grid(self.voxel_size, mode=\"grow\")\n        logger.debug(\"master ROI aligned with voxel size is %s\" % master_roi)\n\n        # get master roi in voxels\n        master_roi_voxels = master_roi / self.voxel_size\n        logger.debug(\"master ROI in voxels is %s\" % master_roi_voxels)\n\n        # Second, create a master transformation. This is a transformation that\n        # covers all voxels of the all requested ROIs. The master transformation\n        # is zero-based.\n\n        # create a transformation with the size of the master ROI in voxels\n        self.master_transformation = self.__create_transformation(\n            master_roi_voxels.shape\n        )\n\n        # Third, crop out parts of the master transformation for each of the\n        # smaller requested ROIs. Since these ROIs now have to align with the\n        # voxel size (which for points does not have to be the case), we also\n        # remember these smaller ROIs as target_rois in global world units.\n\n        # crop the parts corresponding to the requested ROIs\n        self.transformations = {}\n        self.target_rois = {}\n        deps = BatchRequest()\n        for key, spec in request.items():\n            spec = spec.copy()\n\n            if spec.roi is None:\n                continue\n\n            target_roi = Roi(\n                spec.roi.begin[-self.spatial_dims :],\n                spec.roi.shape[-self.spatial_dims :],\n            )\n            logger.debug(\"downstream request spatial ROI for %s is %s\", key, target_roi)\n\n            # make sure the target ROI aligns with the voxel grid (which might\n            # not be the case for points)\n            target_roi = target_roi.snap_to_grid(self.voxel_size, mode=\"grow\")\n            logger.debug(\n                \"downstream request spatial ROI aligned with voxel grid for %s \"\n                \"is %s\",\n                key,\n                target_roi,\n            )\n\n            # remember target ROI (this is where the transformation will project\n            # to)\n            self.target_rois[key] = target_roi\n\n            # get ROI in voxels\n            target_roi_voxels = target_roi / self.voxel_size\n\n            # get ROI relative to master ROI\n            target_roi_in_master_roi_voxels = (\n                target_roi_voxels - master_roi_voxels.begin\n            )\n\n            # crop out relevant part of transformation for this request\n            transformation = np.copy(\n                self.master_transformation[\n                    (slice(None),) + target_roi_in_master_roi_voxels.get_bounding_box()\n                ]\n            )\n            self.transformations[key] = transformation\n\n            # get ROI of all voxels necessary to perfrom transformation\n            #\n            # for that we follow the same transformations to get from the\n            # request ROI to the target ROI in master ROI in voxels, just in\n            # reverse\n            source_roi_in_master_roi_voxels = self.__get_source_roi(transformation)\n            source_roi_voxels = (\n                source_roi_in_master_roi_voxels + master_roi_voxels.begin\n            )\n            source_roi = source_roi_voxels * self.voxel_size\n\n            # transformation is still defined on voxels relative to master ROI\n            # in voxels (i.e., lowest source coordinate could be 5, but data\n            # array we get later starts at 0).\n            #\n            # shift transformation to be indexed relative to beginning of\n            # source_roi_voxels\n            self.__shift_transformation(\n                -source_roi_in_master_roi_voxels.begin, transformation\n            )\n\n            # update upstream request\n            spec.roi = Roi(\n                spec.roi.begin[: -self.spatial_dims]\n                + source_roi.begin[-self.spatial_dims :],\n                spec.roi.shape[: -self.spatial_dims]\n                + source_roi.shape[-self.spatial_dims :],\n            )\n\n            deps[key] = spec\n\n            logger.debug(\"upstream request roi for %s = %s\" % (key, spec.roi))\n\n        return deps\n\n    def process(self, batch, request):\n        for array_key, array in batch.arrays.items():\n            if array_key not in self.target_rois:\n                continue\n\n            # for arrays, the target ROI and the requested ROI should be the\n            # same in spatial coordinates\n            assert (\n                self.target_rois[array_key].begin\n                == request[array_key].roi.begin[-self.spatial_dims :]\n            ), \"Target roi offset {} does not match request roi offset {}\".format(\n                self.target_rois[array_key].begin,\n                request[array_key].roi.begin[-self.spatial_dims :],\n            )\n\n            assert (\n                self.target_rois[array_key].shape\n                == request[array_key].roi.shape[-self.spatial_dims :]\n            ), \"Target roi offset {} does not match request roi offset {}\".format(\n                self.target_rois[array_key].shape,\n                request[array_key].roi.shape[-self.spatial_dims :],\n            )\n\n            # reshape array data into (channels,) + spatial dims\n            shape = array.data.shape\n            channel_shape = shape[: -self.spatial_dims]\n            data = array.data.reshape((-1,) + shape[-self.spatial_dims :])\n\n            # apply transformation on each channel\n            data = np.array(\n                [\n                    augment.apply_transformation(\n                        data[c],\n                        self.transformations[array_key],\n                        interpolate=self.spec[array_key].interpolatable,\n                    )\n                    for c in range(data.shape[0])\n                ]\n            )\n\n            data_roi = request[array_key].roi / self.spec[array_key].voxel_size\n            array.data = data.reshape(\n                channel_shape + data_roi.shape[-self.spatial_dims :]\n            )\n\n            # restore original ROIs\n            array.spec.roi = request[array_key].roi\n\n        for graph_key, graph in batch.graphs.items():\n            nodes = list(graph.nodes)\n\n            if self.use_fast_points_transform:\n                missed_nodes = self.__fast_point_projection(\n                    self.transformations[graph_key],\n                    nodes,\n                    graph.spec.roi,\n                    target_roi=self.target_rois[graph_key],\n                )\n                if not self.recompute_missing_points:\n                    for node in set(missed_nodes):\n                        graph.remove_node(node, retain_connectivity=True)\n                    missed_nodes = []\n            else:\n                missed_nodes = nodes\n\n            for node in missed_nodes:\n                # logger.debug(\"projecting %s\", node.location)\n\n                # get location relative to beginning of upstream ROI\n                location = node.location - graph.spec.roi.begin\n                logger.debug(\"relative to upstream ROI: %s\", location)\n\n                # get spatial coordinates of node in voxels\n                location_voxels = location[-self.spatial_dims :] / self.voxel_size\n\n                # get projected location in transformation data space, this\n                # yields voxel coordinates relative to target ROI\n                projected_voxels = self.__project(\n                    self.transformations[graph_key], location_voxels\n                )\n\n                logger.debug(\n                    \"projected in voxels, relative to target ROI: %s\", projected_voxels\n                )\n\n                if projected_voxels is None:\n                    logger.debug(\"node outside of target, skipping\")\n                    graph.remove_node(node, retain_connectivity=True)\n                    continue\n\n                # convert to world units (now in float again)\n                projected = projected_voxels * np.array(self.voxel_size)\n\n                logger.debug(\n                    \"projected in world units, relative to target ROI: %s\", projected\n                )\n\n                # get global coordinates\n                projected += np.array(self.target_rois[graph_key].begin)\n\n                # update spatial coordinates of node location\n                node.location[-self.spatial_dims :] = projected\n\n                logger.debug(\"final location: %s\", node.location)\n\n                # finally, it can happen that a node no longer is contained in\n                # the requested ROI (because larger ROIs than necessary have\n                # been requested upstream)\n                if not request[graph_key].roi.contains(node.location):\n                    logger.debug(\"node outside of target, skipping\")\n                    graph.remove_node(node, retain_connectivity=True)\n                    continue\n\n            # restore original ROIs\n            graph.spec.roi = request[graph_key].roi\n\n    def __get_common_voxel_size(self, request):\n        voxel_size = None\n        prev = None\n        for array_key in request.array_specs.keys():\n            if voxel_size is None:\n                voxel_size = self.spec[array_key].voxel_size[-self.spatial_dims :]\n            elif self.spec[array_key].voxel_size is not None:\n                assert (\n                    voxel_size == self.spec[array_key].voxel_size[-self.spatial_dims :]\n                ), (\n                    \"ElasticAugment can only be used with arrays of same voxel sizes, \"\n                    \"but %s has %s, and %s has %s.\"\n                    % (\n                        array_key,\n                        self.spec[array_key].voxel_size,\n                        prev,\n                        self.spec[prev].voxel_size,\n                    )\n                )\n            prev = array_key\n\n        if voxel_size is None:\n            raise RuntimeError(\"voxel size must not be None\")\n\n        return Coordinate(voxel_size)\n\n    def __create_transformation(self, target_shape):\n        scale = self.scale_min + random.random() * (self.scale_max - self.scale_min)\n\n        transformation = augment.create_identity_transformation(\n            target_shape, subsample=self.subsample, scale=scale\n        )\n        if sum(self.jitter_sigma) > 0:\n            transformation += augment.create_elastic_transformation(\n                target_shape,\n                self.control_point_spacing,\n                self.jitter_sigma,\n                subsample=self.subsample,\n            )\n        rotation = random.random() * self.rotation_max_amount + self.rotation_start\n        if rotation != 0:\n            transformation += augment.create_rotation_transformation(\n                target_shape, rotation, subsample=self.subsample\n            )\n\n        if self.subsample > 1:\n            transformation = augment.upscale_transformation(\n                transformation, target_shape\n            )\n\n        if self.prob_slip + self.prob_shift > 0:\n            self.__misalign(transformation)\n\n        return transformation\n\n    def __fast_point_projection(self, transformation, nodes, source_roi, target_roi):\n        if len(nodes) < 1:\n            return []\n        # rasterize the points into an array\n        ids, locs = zip(\n            *[\n                (\n                    node.id,\n                    (np.floor(node.location).astype(int) - source_roi.begin)\n                    // self.voxel_size,\n                )\n                for node in nodes\n                if source_roi.contains(node.location)\n            ]\n        )\n        ids, locs = np.array(ids), tuple(zip(*locs))\n        points_array = np.zeros(source_roi.shape / self.voxel_size, dtype=np.int64)\n        points_array[locs] = ids\n\n        # reshape array data into (channels,) + spatial dims\n        shape = points_array.shape\n        data = points_array.reshape((-1,) + shape[-self.spatial_dims :])\n\n        # apply transformation on each channel\n        data = np.array(\n            [\n                augment.apply_transformation(\n                    data[c], transformation, interpolate=\"nearest\"\n                )\n                for c in range(data.shape[0])\n            ]\n        )\n\n        missing_points = []\n        projected_locs = ndimage.measurements.center_of_mass(data > 0, data, ids)\n        projected_locs = [\n            np.array(loc[-self.spatial_dims :]) * self.voxel_size + target_roi.begin\n            for loc in projected_locs\n        ]\n        node_dict = {node.id: node for node in nodes}\n        for point_id, proj_loc in zip(ids, projected_locs):\n            point = node_dict.pop(point_id)\n            if not any([np.isnan(x) for x in proj_loc]):\n                assert (\n                    len(proj_loc) == self.spatial_dims\n                ), \"projected location has wrong number of dimensions: {}, expected: {}\".format(\n                    len(proj_loc), self.spatial_dims\n                )\n                point.location[-self.spatial_dims :] = proj_loc\n            else:\n                missing_points.append(point)\n        for node in node_dict.values():\n            missing_points.append(point)\n        logger.debug(\n            \"{} of {} points lost in fast points projection\".format(\n                len(missing_points), len(ids)\n            )\n        )\n\n        return missing_points\n\n    def __project(self, transformation, location):\n        \"\"\"Find the projection of location given by transformation. Returns None\n        if projection lies outside of transformation.\"\"\"\n\n        dims = len(location)\n\n        # subtract location from transformation\n        diff = transformation.copy()\n        for d in range(dims):\n            diff[d] -= location[d]\n\n        # square\n        diff2 = diff * diff\n\n        # sum\n        dist = diff2.sum(axis=0)\n\n        # find grid point closes to location\n        center_grid = Coordinate(np.unravel_index(dist.argmin(), dist.shape))\n        center_source = self.__source_at(transformation, center_grid)\n\n        logger.debug(\"projecting %s onto grid\", location)\n        logger.debug(\"grid shape: %s\", transformation.shape[1:])\n        logger.debug(\"grid projection: %s\", center_grid)\n        logger.debug(\"dist shape: %s\", dist.shape)\n        logger.debug(\"dist.argmin(): %s\", dist.argmin())\n        logger.debug(\"dist[argmin]: %s\", dist[center_grid])\n        logger.debug(\n            \"transform[argmin]: %s\", transformation[(slice(None),) + center_grid]\n        )\n        logger.debug(\"min dist: %s\", dist.min())\n        logger.debug(\"center source: %s\", center_source)\n\n        # inspect grid edges incident to center_grid\n        for d in range(dims):\n            # nothing to do for dimensions without spatial extent\n            if transformation.shape[1 + d] == 1:\n                continue\n\n            dim_vector = Coordinate(1 if dd == d else 0 for dd in range(dims))\n            pos_grid = center_grid + dim_vector\n            neg_grid = center_grid - dim_vector\n            logger.debug(\"interpolating along %s\", dim_vector)\n\n            pos_u = -1\n            neg_u = -1\n\n            if pos_grid[d] < transformation.shape[1 + d]:\n                pos_source = self.__source_at(transformation, pos_grid)\n                logger.debug(\"pos source: %s\", pos_source)\n                pos_dist = pos_source[d] - center_source[d]\n                loc_dist = location[d] - center_source[d]\n                if pos_dist != 0:\n                    pos_u = loc_dist / pos_dist\n                else:\n                    pos_u = 0\n\n            if neg_grid[d] >= 0:\n                neg_source = self.__source_at(transformation, neg_grid)\n                logger.debug(\"neg source: %s\", neg_source)\n                neg_dist = neg_source[d] - center_source[d]\n                loc_dist = location[d] - center_source[d]\n                if neg_dist != 0:\n                    neg_u = loc_dist / neg_dist\n                else:\n                    neg_u = 0\n\n            logger.debug(\"pos u/neg u: %s/%s\", pos_u, neg_u)\n\n            # if a point only falls behind edges, it lies outside of the grid\n            if pos_u < 0 and neg_u < 0:\n                return None\n\n        return np.array(center_grid, dtype=np.float32)\n\n    def __source_at(self, transformation, index):\n        \"\"\"Read the source point of a transformation at index.\"\"\"\n\n        slices = (slice(None),) + tuple(slice(i, i + 1) for i in index)\n        return transformation[slices].flatten()\n\n    def __get_source_roi(self, transformation):\n        dims = transformation.shape[0]\n\n        # get bounding box of needed data for transformation\n        bb_min = Coordinate(\n            int(math.floor(transformation[d].min())) for d in range(dims)\n        )\n        bb_max = Coordinate(\n            int(math.ceil(transformation[d].max())) + 1 for d in range(dims)\n        )\n\n        # create roi sufficiently large to feed transformation\n        source_roi = Roi(bb_min, bb_max - bb_min)\n\n        return source_roi\n\n    def __shift_transformation(self, shift, transformation):\n        for d in range(transformation.shape[0]):\n            transformation[d] += shift[d]\n\n    def __misalign(self, transformation):\n        assert (\n            transformation.shape[0] == 3\n        ), \"misalign can only be applied to 3D volumes\"\n\n        num_sections = transformation[0].shape[0]\n\n        shifts = [Coordinate((0, 0, 0))] * num_sections\n        for z in range(num_sections):\n            r = random.random()\n\n            if r <= self.prob_slip:\n                shifts[z] = self.__random_offset()\n\n            elif r <= self.prob_slip + self.prob_shift:\n                offset = self.__random_offset()\n                for zp in range(z, num_sections):\n                    shifts[zp] += offset\n\n        logger.debug(\"misaligning sections with \" + str(shifts))\n\n        dims = 3\n        bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n        bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n        logger.debug(\"min/max of transformation: \" + str(bb_min) + \"/\" + str(bb_max))\n\n        for z in range(num_sections):\n            transformation[1][z, :, :] += shifts[z][1]\n            transformation[2][z, :, :] += shifts[z][2]\n\n        bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n        bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n        logger.debug(\n            \"min/max of transformation after misalignment: \"\n            + str(bb_min)\n            + \"/\"\n            + str(bb_max)\n        )\n\n    def __random_offset(self):\n        return Coordinate(\n            (0,)\n            + tuple(\n                self.max_misalign - random.randint(0, 2 * int(self.max_misalign))\n                for d in range(2)\n            )\n        )",
  "def __init__(\n        self,\n        control_point_spacing,\n        jitter_sigma,\n        rotation_interval,\n        scale_interval=(1.0, 1.0),\n        prob_slip=0,\n        prob_shift=0,\n        max_misalign=0,\n        subsample=1,\n        spatial_dims=3,\n        use_fast_points_transform=False,\n        recompute_missing_points=True,\n    ):\n        warnings.warn(\n            \"ElasticAugment is deprecated, please use the DeformAugment\",\n            DeprecationWarning,\n        )\n\n        self.control_point_spacing = control_point_spacing\n        self.jitter_sigma = jitter_sigma\n        self.rotation_start = rotation_interval[0]\n        self.rotation_max_amount = rotation_interval[1] - rotation_interval[0]\n        self.scale_min = scale_interval[0]\n        self.scale_max = scale_interval[1]\n        self.prob_slip = prob_slip\n        self.prob_shift = prob_shift\n        self.max_misalign = max_misalign\n        self.subsample = subsample\n        self.spatial_dims = spatial_dims\n        self.use_fast_points_transform = use_fast_points_transform\n        self.recompute_missing_points = recompute_missing_points",
  "def prepare(self, request):\n\n        # get the voxel size\n        self.voxel_size = self.__get_common_voxel_size(request)\n\n        # get the total ROI of all requests\n        total_roi = request.get_total_roi()\n        logger.debug(\"total ROI is %s\" % total_roi)\n\n        # First, get the total ROI of the request in spatial dimensions only.\n        # Channels and time don't matter. This is our master ROI.\n\n        # get master ROI\n        master_roi = Roi(\n            total_roi.begin[-self.spatial_dims :],\n            total_roi.shape[-self.spatial_dims :],\n        )\n        self.spatial_dims = master_roi.dims\n        logger.debug(\"master ROI is %s\" % master_roi)\n\n        # make sure the master ROI aligns with the voxel size\n        master_roi = master_roi.snap_to_grid(self.voxel_size, mode=\"grow\")\n        logger.debug(\"master ROI aligned with voxel size is %s\" % master_roi)\n\n        # get master roi in voxels\n        master_roi_voxels = master_roi / self.voxel_size\n        logger.debug(\"master ROI in voxels is %s\" % master_roi_voxels)\n\n        # Second, create a master transformation. This is a transformation that\n        # covers all voxels of the all requested ROIs. The master transformation\n        # is zero-based.\n\n        # create a transformation with the size of the master ROI in voxels\n        self.master_transformation = self.__create_transformation(\n            master_roi_voxels.shape\n        )\n\n        # Third, crop out parts of the master transformation for each of the\n        # smaller requested ROIs. Since these ROIs now have to align with the\n        # voxel size (which for points does not have to be the case), we also\n        # remember these smaller ROIs as target_rois in global world units.\n\n        # crop the parts corresponding to the requested ROIs\n        self.transformations = {}\n        self.target_rois = {}\n        deps = BatchRequest()\n        for key, spec in request.items():\n            spec = spec.copy()\n\n            if spec.roi is None:\n                continue\n\n            target_roi = Roi(\n                spec.roi.begin[-self.spatial_dims :],\n                spec.roi.shape[-self.spatial_dims :],\n            )\n            logger.debug(\"downstream request spatial ROI for %s is %s\", key, target_roi)\n\n            # make sure the target ROI aligns with the voxel grid (which might\n            # not be the case for points)\n            target_roi = target_roi.snap_to_grid(self.voxel_size, mode=\"grow\")\n            logger.debug(\n                \"downstream request spatial ROI aligned with voxel grid for %s \"\n                \"is %s\",\n                key,\n                target_roi,\n            )\n\n            # remember target ROI (this is where the transformation will project\n            # to)\n            self.target_rois[key] = target_roi\n\n            # get ROI in voxels\n            target_roi_voxels = target_roi / self.voxel_size\n\n            # get ROI relative to master ROI\n            target_roi_in_master_roi_voxels = (\n                target_roi_voxels - master_roi_voxels.begin\n            )\n\n            # crop out relevant part of transformation for this request\n            transformation = np.copy(\n                self.master_transformation[\n                    (slice(None),) + target_roi_in_master_roi_voxels.get_bounding_box()\n                ]\n            )\n            self.transformations[key] = transformation\n\n            # get ROI of all voxels necessary to perfrom transformation\n            #\n            # for that we follow the same transformations to get from the\n            # request ROI to the target ROI in master ROI in voxels, just in\n            # reverse\n            source_roi_in_master_roi_voxels = self.__get_source_roi(transformation)\n            source_roi_voxels = (\n                source_roi_in_master_roi_voxels + master_roi_voxels.begin\n            )\n            source_roi = source_roi_voxels * self.voxel_size\n\n            # transformation is still defined on voxels relative to master ROI\n            # in voxels (i.e., lowest source coordinate could be 5, but data\n            # array we get later starts at 0).\n            #\n            # shift transformation to be indexed relative to beginning of\n            # source_roi_voxels\n            self.__shift_transformation(\n                -source_roi_in_master_roi_voxels.begin, transformation\n            )\n\n            # update upstream request\n            spec.roi = Roi(\n                spec.roi.begin[: -self.spatial_dims]\n                + source_roi.begin[-self.spatial_dims :],\n                spec.roi.shape[: -self.spatial_dims]\n                + source_roi.shape[-self.spatial_dims :],\n            )\n\n            deps[key] = spec\n\n            logger.debug(\"upstream request roi for %s = %s\" % (key, spec.roi))\n\n        return deps",
  "def process(self, batch, request):\n        for array_key, array in batch.arrays.items():\n            if array_key not in self.target_rois:\n                continue\n\n            # for arrays, the target ROI and the requested ROI should be the\n            # same in spatial coordinates\n            assert (\n                self.target_rois[array_key].begin\n                == request[array_key].roi.begin[-self.spatial_dims :]\n            ), \"Target roi offset {} does not match request roi offset {}\".format(\n                self.target_rois[array_key].begin,\n                request[array_key].roi.begin[-self.spatial_dims :],\n            )\n\n            assert (\n                self.target_rois[array_key].shape\n                == request[array_key].roi.shape[-self.spatial_dims :]\n            ), \"Target roi offset {} does not match request roi offset {}\".format(\n                self.target_rois[array_key].shape,\n                request[array_key].roi.shape[-self.spatial_dims :],\n            )\n\n            # reshape array data into (channels,) + spatial dims\n            shape = array.data.shape\n            channel_shape = shape[: -self.spatial_dims]\n            data = array.data.reshape((-1,) + shape[-self.spatial_dims :])\n\n            # apply transformation on each channel\n            data = np.array(\n                [\n                    augment.apply_transformation(\n                        data[c],\n                        self.transformations[array_key],\n                        interpolate=self.spec[array_key].interpolatable,\n                    )\n                    for c in range(data.shape[0])\n                ]\n            )\n\n            data_roi = request[array_key].roi / self.spec[array_key].voxel_size\n            array.data = data.reshape(\n                channel_shape + data_roi.shape[-self.spatial_dims :]\n            )\n\n            # restore original ROIs\n            array.spec.roi = request[array_key].roi\n\n        for graph_key, graph in batch.graphs.items():\n            nodes = list(graph.nodes)\n\n            if self.use_fast_points_transform:\n                missed_nodes = self.__fast_point_projection(\n                    self.transformations[graph_key],\n                    nodes,\n                    graph.spec.roi,\n                    target_roi=self.target_rois[graph_key],\n                )\n                if not self.recompute_missing_points:\n                    for node in set(missed_nodes):\n                        graph.remove_node(node, retain_connectivity=True)\n                    missed_nodes = []\n            else:\n                missed_nodes = nodes\n\n            for node in missed_nodes:\n                # logger.debug(\"projecting %s\", node.location)\n\n                # get location relative to beginning of upstream ROI\n                location = node.location - graph.spec.roi.begin\n                logger.debug(\"relative to upstream ROI: %s\", location)\n\n                # get spatial coordinates of node in voxels\n                location_voxels = location[-self.spatial_dims :] / self.voxel_size\n\n                # get projected location in transformation data space, this\n                # yields voxel coordinates relative to target ROI\n                projected_voxels = self.__project(\n                    self.transformations[graph_key], location_voxels\n                )\n\n                logger.debug(\n                    \"projected in voxels, relative to target ROI: %s\", projected_voxels\n                )\n\n                if projected_voxels is None:\n                    logger.debug(\"node outside of target, skipping\")\n                    graph.remove_node(node, retain_connectivity=True)\n                    continue\n\n                # convert to world units (now in float again)\n                projected = projected_voxels * np.array(self.voxel_size)\n\n                logger.debug(\n                    \"projected in world units, relative to target ROI: %s\", projected\n                )\n\n                # get global coordinates\n                projected += np.array(self.target_rois[graph_key].begin)\n\n                # update spatial coordinates of node location\n                node.location[-self.spatial_dims :] = projected\n\n                logger.debug(\"final location: %s\", node.location)\n\n                # finally, it can happen that a node no longer is contained in\n                # the requested ROI (because larger ROIs than necessary have\n                # been requested upstream)\n                if not request[graph_key].roi.contains(node.location):\n                    logger.debug(\"node outside of target, skipping\")\n                    graph.remove_node(node, retain_connectivity=True)\n                    continue\n\n            # restore original ROIs\n            graph.spec.roi = request[graph_key].roi",
  "def __get_common_voxel_size(self, request):\n        voxel_size = None\n        prev = None\n        for array_key in request.array_specs.keys():\n            if voxel_size is None:\n                voxel_size = self.spec[array_key].voxel_size[-self.spatial_dims :]\n            elif self.spec[array_key].voxel_size is not None:\n                assert (\n                    voxel_size == self.spec[array_key].voxel_size[-self.spatial_dims :]\n                ), (\n                    \"ElasticAugment can only be used with arrays of same voxel sizes, \"\n                    \"but %s has %s, and %s has %s.\"\n                    % (\n                        array_key,\n                        self.spec[array_key].voxel_size,\n                        prev,\n                        self.spec[prev].voxel_size,\n                    )\n                )\n            prev = array_key\n\n        if voxel_size is None:\n            raise RuntimeError(\"voxel size must not be None\")\n\n        return Coordinate(voxel_size)",
  "def __create_transformation(self, target_shape):\n        scale = self.scale_min + random.random() * (self.scale_max - self.scale_min)\n\n        transformation = augment.create_identity_transformation(\n            target_shape, subsample=self.subsample, scale=scale\n        )\n        if sum(self.jitter_sigma) > 0:\n            transformation += augment.create_elastic_transformation(\n                target_shape,\n                self.control_point_spacing,\n                self.jitter_sigma,\n                subsample=self.subsample,\n            )\n        rotation = random.random() * self.rotation_max_amount + self.rotation_start\n        if rotation != 0:\n            transformation += augment.create_rotation_transformation(\n                target_shape, rotation, subsample=self.subsample\n            )\n\n        if self.subsample > 1:\n            transformation = augment.upscale_transformation(\n                transformation, target_shape\n            )\n\n        if self.prob_slip + self.prob_shift > 0:\n            self.__misalign(transformation)\n\n        return transformation",
  "def __fast_point_projection(self, transformation, nodes, source_roi, target_roi):\n        if len(nodes) < 1:\n            return []\n        # rasterize the points into an array\n        ids, locs = zip(\n            *[\n                (\n                    node.id,\n                    (np.floor(node.location).astype(int) - source_roi.begin)\n                    // self.voxel_size,\n                )\n                for node in nodes\n                if source_roi.contains(node.location)\n            ]\n        )\n        ids, locs = np.array(ids), tuple(zip(*locs))\n        points_array = np.zeros(source_roi.shape / self.voxel_size, dtype=np.int64)\n        points_array[locs] = ids\n\n        # reshape array data into (channels,) + spatial dims\n        shape = points_array.shape\n        data = points_array.reshape((-1,) + shape[-self.spatial_dims :])\n\n        # apply transformation on each channel\n        data = np.array(\n            [\n                augment.apply_transformation(\n                    data[c], transformation, interpolate=\"nearest\"\n                )\n                for c in range(data.shape[0])\n            ]\n        )\n\n        missing_points = []\n        projected_locs = ndimage.measurements.center_of_mass(data > 0, data, ids)\n        projected_locs = [\n            np.array(loc[-self.spatial_dims :]) * self.voxel_size + target_roi.begin\n            for loc in projected_locs\n        ]\n        node_dict = {node.id: node for node in nodes}\n        for point_id, proj_loc in zip(ids, projected_locs):\n            point = node_dict.pop(point_id)\n            if not any([np.isnan(x) for x in proj_loc]):\n                assert (\n                    len(proj_loc) == self.spatial_dims\n                ), \"projected location has wrong number of dimensions: {}, expected: {}\".format(\n                    len(proj_loc), self.spatial_dims\n                )\n                point.location[-self.spatial_dims :] = proj_loc\n            else:\n                missing_points.append(point)\n        for node in node_dict.values():\n            missing_points.append(point)\n        logger.debug(\n            \"{} of {} points lost in fast points projection\".format(\n                len(missing_points), len(ids)\n            )\n        )\n\n        return missing_points",
  "def __project(self, transformation, location):\n        \"\"\"Find the projection of location given by transformation. Returns None\n        if projection lies outside of transformation.\"\"\"\n\n        dims = len(location)\n\n        # subtract location from transformation\n        diff = transformation.copy()\n        for d in range(dims):\n            diff[d] -= location[d]\n\n        # square\n        diff2 = diff * diff\n\n        # sum\n        dist = diff2.sum(axis=0)\n\n        # find grid point closes to location\n        center_grid = Coordinate(np.unravel_index(dist.argmin(), dist.shape))\n        center_source = self.__source_at(transformation, center_grid)\n\n        logger.debug(\"projecting %s onto grid\", location)\n        logger.debug(\"grid shape: %s\", transformation.shape[1:])\n        logger.debug(\"grid projection: %s\", center_grid)\n        logger.debug(\"dist shape: %s\", dist.shape)\n        logger.debug(\"dist.argmin(): %s\", dist.argmin())\n        logger.debug(\"dist[argmin]: %s\", dist[center_grid])\n        logger.debug(\n            \"transform[argmin]: %s\", transformation[(slice(None),) + center_grid]\n        )\n        logger.debug(\"min dist: %s\", dist.min())\n        logger.debug(\"center source: %s\", center_source)\n\n        # inspect grid edges incident to center_grid\n        for d in range(dims):\n            # nothing to do for dimensions without spatial extent\n            if transformation.shape[1 + d] == 1:\n                continue\n\n            dim_vector = Coordinate(1 if dd == d else 0 for dd in range(dims))\n            pos_grid = center_grid + dim_vector\n            neg_grid = center_grid - dim_vector\n            logger.debug(\"interpolating along %s\", dim_vector)\n\n            pos_u = -1\n            neg_u = -1\n\n            if pos_grid[d] < transformation.shape[1 + d]:\n                pos_source = self.__source_at(transformation, pos_grid)\n                logger.debug(\"pos source: %s\", pos_source)\n                pos_dist = pos_source[d] - center_source[d]\n                loc_dist = location[d] - center_source[d]\n                if pos_dist != 0:\n                    pos_u = loc_dist / pos_dist\n                else:\n                    pos_u = 0\n\n            if neg_grid[d] >= 0:\n                neg_source = self.__source_at(transformation, neg_grid)\n                logger.debug(\"neg source: %s\", neg_source)\n                neg_dist = neg_source[d] - center_source[d]\n                loc_dist = location[d] - center_source[d]\n                if neg_dist != 0:\n                    neg_u = loc_dist / neg_dist\n                else:\n                    neg_u = 0\n\n            logger.debug(\"pos u/neg u: %s/%s\", pos_u, neg_u)\n\n            # if a point only falls behind edges, it lies outside of the grid\n            if pos_u < 0 and neg_u < 0:\n                return None\n\n        return np.array(center_grid, dtype=np.float32)",
  "def __source_at(self, transformation, index):\n        \"\"\"Read the source point of a transformation at index.\"\"\"\n\n        slices = (slice(None),) + tuple(slice(i, i + 1) for i in index)\n        return transformation[slices].flatten()",
  "def __get_source_roi(self, transformation):\n        dims = transformation.shape[0]\n\n        # get bounding box of needed data for transformation\n        bb_min = Coordinate(\n            int(math.floor(transformation[d].min())) for d in range(dims)\n        )\n        bb_max = Coordinate(\n            int(math.ceil(transformation[d].max())) + 1 for d in range(dims)\n        )\n\n        # create roi sufficiently large to feed transformation\n        source_roi = Roi(bb_min, bb_max - bb_min)\n\n        return source_roi",
  "def __shift_transformation(self, shift, transformation):\n        for d in range(transformation.shape[0]):\n            transformation[d] += shift[d]",
  "def __misalign(self, transformation):\n        assert (\n            transformation.shape[0] == 3\n        ), \"misalign can only be applied to 3D volumes\"\n\n        num_sections = transformation[0].shape[0]\n\n        shifts = [Coordinate((0, 0, 0))] * num_sections\n        for z in range(num_sections):\n            r = random.random()\n\n            if r <= self.prob_slip:\n                shifts[z] = self.__random_offset()\n\n            elif r <= self.prob_slip + self.prob_shift:\n                offset = self.__random_offset()\n                for zp in range(z, num_sections):\n                    shifts[zp] += offset\n\n        logger.debug(\"misaligning sections with \" + str(shifts))\n\n        dims = 3\n        bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n        bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n        logger.debug(\"min/max of transformation: \" + str(bb_min) + \"/\" + str(bb_max))\n\n        for z in range(num_sections):\n            transformation[1][z, :, :] += shifts[z][1]\n            transformation[2][z, :, :] += shifts[z][2]\n\n        bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n        bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n        logger.debug(\n            \"min/max of transformation after misalignment: \"\n            + str(bb_min)\n            + \"/\"\n            + str(bb_max)\n        )",
  "def __random_offset(self):\n        return Coordinate(\n            (0,)\n            + tuple(\n                self.max_misalign - random.randint(0, 2 * int(self.max_misalign))\n                for d in range(2)\n            )\n        )",
  "class DvidSource(BatchProvider):\n    \"\"\"A DVID array source.\n\n    Provides arrays from DVID servers for each array key given.\n\n    Args:\n\n        hostname (``string``):\n\n            The name of the DVID server.\n\n        port (``int``):\n\n            The port of the DVID server.\n\n        uuid (``string``):\n\n            The UUID of the DVID node to use.\n\n        datasets (``dict``, :class:`ArrayKey` -> ``string``):\n\n            Dictionary mapping array keys to DVID data instance names that this\n            source offers.\n\n        masks (``dict``, :class:`ArrayKey` -> ``string``, optional):\n\n            Dictionary of array keys to DVID ROI instance names. This will\n            create binary masks from DVID ROIs.\n\n        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):\n\n            An optional dictionary of array keys to specs to overwrite the\n            array specs automatically determined from the DVID server. This is\n            useful to set ``voxel_size``, for example. Only fields that are not\n            ``None`` in the given :class:`ArraySpec` will be used.\n    \"\"\"\n\n    def __init__(self, hostname, port, uuid, datasets, masks=None, array_specs=None):\n        self.hostname = hostname\n        self.port = port\n        self.url = \"http://{}:{}\".format(self.hostname, self.port)\n        self.uuid = uuid\n\n        self.datasets = datasets\n        self.masks = masks if masks is not None else {}\n\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        self.ndims = None\n\n    def setup(self):\n        for array_key, _ in self.datasets.items():\n            spec = self.__get_spec(array_key)\n            self.provides(array_key, spec)\n\n        for array_key, _ in self.masks.items():\n            spec = self.__get_mask_spec(array_key)\n            self.provides(array_key, spec)\n\n        logger.info(\"DvidSource.spec:\\n%s\", self.spec)\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        for array_key, request_spec in request.array_specs.items():\n            logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n            voxel_size = self.spec[array_key].voxel_size\n\n            # scale request roi to voxel units\n            dataset_roi = request_spec.roi / voxel_size\n\n            # shift request roi into dataset\n            dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n            # create array spec\n            array_spec = self.spec[array_key].copy()\n            array_spec.roi = request_spec.roi\n\n            # read the data\n            if array_key in self.datasets:\n                data = self.__read_array(self.datasets[array_key], dataset_roi)\n            elif array_key in self.masks:\n                data = self.__read_mask(self.masks[array_key], dataset_roi)\n            else:\n                assert False, (\n                    \"Encountered a request for %s that is neither a volume \"\n                    \"nor a mask.\" % array_key\n                )\n\n            # add array to batch\n            batch.arrays[array_key] = Array(data, array_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    def __get_info(self, array_key):\n        if array_key in self.datasets:\n            data = dvision.DVIDDataInstance(\n                self.hostname, self.port, self.uuid, self.datasets[array_key]\n            )\n\n        elif array_key in self.masks:\n            data = dvision.DVIDRegionOfInterest(\n                self.hostname, self.port, self.uuid, self.masks[array_key]\n            )\n\n        else:\n            assert False, (\n                \"Encountered a request that is neither a volume \" \"nor a mask.\"\n            )\n\n        return data.info\n\n    def __get_spec(self, array_key):\n        info = self.__get_info(array_key)\n\n        roi_min = info[\"Extended\"][\"MinPoint\"]\n        if roi_min is not None:\n            roi_min = Coordinate(roi_min[::-1])\n        roi_max = info[\"Extended\"][\"MaxPoint\"]\n        if roi_max is not None:\n            roi_max = Coordinate(roi_max[::-1])\n\n        data_roi = Roi(offset=roi_min, shape=(roi_max - roi_min))\n        data_dims = Coordinate(data_roi.shape)\n\n        if self.ndims is None:\n            self.ndims = len(data_dims)\n        else:\n            assert self.ndims == len(data_dims)\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            spec.voxel_size = Coordinate(info[\"Extended\"][\"VoxelSize\"])\n\n        if spec.roi is None:\n            spec.roi = data_roi * spec.voxel_size\n\n        data_dtype = dvision.DVIDDataInstance(\n            self.hostname, self.port, self.uuid, self.datasets[array_key]\n        ).dtype\n\n        if spec.dtype is not None:\n            assert spec.dtype == data_dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from instance %s dtype %s\"\n                % (\n                    self.array_specs[array_key].dtype,\n                    array_key,\n                    self.datasets[array_key],\n                    data_dtype,\n                )\n            )\n        else:\n            spec.dtype = data_dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s. \"\n                \"Based on the dtype %s, it has been set to %s. \"\n                \"This might not be what you want.\",\n                array_key,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec\n\n    def __get_mask_spec(self, mask_key):\n        # create initial array spec\n\n        if mask_key in self.array_specs:\n            spec = self.array_specs[mask_key].copy()\n        else:\n            spec = ArraySpec()\n\n        # get voxel size\n\n        if spec.voxel_size is None:\n            voxel_size = None\n            for array_key in self.datasets:\n                if voxel_size is None:\n                    voxel_size = self.spec[array_key].voxel_size\n                else:\n                    assert voxel_size == self.spec[array_key].voxel_size, (\n                        \"No voxel size was given for mask %s, and the voxel \"\n                        \"sizes of the volumes %s are not all the same. I don't \"\n                        \"know what voxel size to use to create the mask.\"\n                        % (mask_key, self.datasets.keys())\n                    )\n\n            spec.voxel_size = voxel_size\n\n        # get ROI\n\n        if spec.roi is None:\n            for array_key in self.datasets:\n                roi = self.spec[array_key].roi\n\n                if spec.roi is None:\n                    spec.roi = roi.copy()\n                else:\n                    spec.roi = roi.union(spec.roi)\n\n        # set interpolatable\n\n        if spec.interpolatable is None:\n            spec.interpolatable = False\n\n        # set datatype\n\n        if spec.dtype is not None and spec.dtype != np.uint8:\n            logger.warn(\n                \"Ignoring dtype in array_spec for %s, only np.uint8 \"\n                \"is allowed for masks.\",\n                mask_key,\n            )\n        spec.dtype = np.uint8\n\n        return spec\n\n    def __read_array(self, instance, roi):\n        data_instance = dvision.DVIDDataInstance(\n            self.hostname, self.port, self.uuid, instance\n        )\n\n        return data_instance[roi.get_bounding_box()]\n\n    def __read_mask(self, instance, roi):\n        dvid_roi = dvision.DVIDRegionOfInterest(\n            self.hostname, self.port, self.uuid, instance\n        )\n\n        return dvid_roi[roi.get_bounding_box()]\n\n    def __repr__(self):\n        return \"DvidSource(hostname={}, port={}, uuid={}\".format(\n            self.hostname, self.port, self.uuid\n        )",
  "def __init__(self, hostname, port, uuid, datasets, masks=None, array_specs=None):\n        self.hostname = hostname\n        self.port = port\n        self.url = \"http://{}:{}\".format(self.hostname, self.port)\n        self.uuid = uuid\n\n        self.datasets = datasets\n        self.masks = masks if masks is not None else {}\n\n        self.array_specs = array_specs if array_specs is not None else {}\n\n        self.ndims = None",
  "def setup(self):\n        for array_key, _ in self.datasets.items():\n            spec = self.__get_spec(array_key)\n            self.provides(array_key, spec)\n\n        for array_key, _ in self.masks.items():\n            spec = self.__get_mask_spec(array_key)\n            self.provides(array_key, spec)\n\n        logger.info(\"DvidSource.spec:\\n%s\", self.spec)",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n\n        batch = Batch()\n\n        for array_key, request_spec in request.array_specs.items():\n            logger.debug(\"Reading %s in %s...\", array_key, request_spec.roi)\n\n            voxel_size = self.spec[array_key].voxel_size\n\n            # scale request roi to voxel units\n            dataset_roi = request_spec.roi / voxel_size\n\n            # shift request roi into dataset\n            dataset_roi = dataset_roi - self.spec[array_key].roi.offset / voxel_size\n\n            # create array spec\n            array_spec = self.spec[array_key].copy()\n            array_spec.roi = request_spec.roi\n\n            # read the data\n            if array_key in self.datasets:\n                data = self.__read_array(self.datasets[array_key], dataset_roi)\n            elif array_key in self.masks:\n                data = self.__read_mask(self.masks[array_key], dataset_roi)\n            else:\n                assert False, (\n                    \"Encountered a request for %s that is neither a volume \"\n                    \"nor a mask.\" % array_key\n                )\n\n            # add array to batch\n            batch.arrays[array_key] = Array(data, array_spec)\n\n        logger.debug(\"done\")\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def __get_info(self, array_key):\n        if array_key in self.datasets:\n            data = dvision.DVIDDataInstance(\n                self.hostname, self.port, self.uuid, self.datasets[array_key]\n            )\n\n        elif array_key in self.masks:\n            data = dvision.DVIDRegionOfInterest(\n                self.hostname, self.port, self.uuid, self.masks[array_key]\n            )\n\n        else:\n            assert False, (\n                \"Encountered a request that is neither a volume \" \"nor a mask.\"\n            )\n\n        return data.info",
  "def __get_spec(self, array_key):\n        info = self.__get_info(array_key)\n\n        roi_min = info[\"Extended\"][\"MinPoint\"]\n        if roi_min is not None:\n            roi_min = Coordinate(roi_min[::-1])\n        roi_max = info[\"Extended\"][\"MaxPoint\"]\n        if roi_max is not None:\n            roi_max = Coordinate(roi_max[::-1])\n\n        data_roi = Roi(offset=roi_min, shape=(roi_max - roi_min))\n        data_dims = Coordinate(data_roi.shape)\n\n        if self.ndims is None:\n            self.ndims = len(data_dims)\n        else:\n            assert self.ndims == len(data_dims)\n\n        if array_key in self.array_specs:\n            spec = self.array_specs[array_key].copy()\n        else:\n            spec = ArraySpec()\n\n        if spec.voxel_size is None:\n            spec.voxel_size = Coordinate(info[\"Extended\"][\"VoxelSize\"])\n\n        if spec.roi is None:\n            spec.roi = data_roi * spec.voxel_size\n\n        data_dtype = dvision.DVIDDataInstance(\n            self.hostname, self.port, self.uuid, self.datasets[array_key]\n        ).dtype\n\n        if spec.dtype is not None:\n            assert spec.dtype == data_dtype, (\n                \"dtype %s provided in array_specs for %s, \"\n                \"but differs from instance %s dtype %s\"\n                % (\n                    self.array_specs[array_key].dtype,\n                    array_key,\n                    self.datasets[array_key],\n                    data_dtype,\n                )\n            )\n        else:\n            spec.dtype = data_dtype\n\n        if spec.interpolatable is None:\n            spec.interpolatable = spec.dtype in [\n                np.float32,\n                np.float64,\n                np.float128,\n                np.uint8,  # assuming this is not used for labels\n            ]\n            logger.warning(\n                \"WARNING: You didn't set 'interpolatable' for %s. \"\n                \"Based on the dtype %s, it has been set to %s. \"\n                \"This might not be what you want.\",\n                array_key,\n                spec.dtype,\n                spec.interpolatable,\n            )\n\n        return spec",
  "def __get_mask_spec(self, mask_key):\n        # create initial array spec\n\n        if mask_key in self.array_specs:\n            spec = self.array_specs[mask_key].copy()\n        else:\n            spec = ArraySpec()\n\n        # get voxel size\n\n        if spec.voxel_size is None:\n            voxel_size = None\n            for array_key in self.datasets:\n                if voxel_size is None:\n                    voxel_size = self.spec[array_key].voxel_size\n                else:\n                    assert voxel_size == self.spec[array_key].voxel_size, (\n                        \"No voxel size was given for mask %s, and the voxel \"\n                        \"sizes of the volumes %s are not all the same. I don't \"\n                        \"know what voxel size to use to create the mask.\"\n                        % (mask_key, self.datasets.keys())\n                    )\n\n            spec.voxel_size = voxel_size\n\n        # get ROI\n\n        if spec.roi is None:\n            for array_key in self.datasets:\n                roi = self.spec[array_key].roi\n\n                if spec.roi is None:\n                    spec.roi = roi.copy()\n                else:\n                    spec.roi = roi.union(spec.roi)\n\n        # set interpolatable\n\n        if spec.interpolatable is None:\n            spec.interpolatable = False\n\n        # set datatype\n\n        if spec.dtype is not None and spec.dtype != np.uint8:\n            logger.warn(\n                \"Ignoring dtype in array_spec for %s, only np.uint8 \"\n                \"is allowed for masks.\",\n                mask_key,\n            )\n        spec.dtype = np.uint8\n\n        return spec",
  "def __read_array(self, instance, roi):\n        data_instance = dvision.DVIDDataInstance(\n            self.hostname, self.port, self.uuid, instance\n        )\n\n        return data_instance[roi.get_bounding_box()]",
  "def __read_mask(self, instance, roi):\n        dvid_roi = dvision.DVIDRegionOfInterest(\n            self.hostname, self.port, self.uuid, instance\n        )\n\n        return dvid_roi[roi.get_bounding_box()]",
  "def __repr__(self):\n        return \"DvidSource(hostname={}, port={}, uuid={}\".format(\n            self.hostname, self.port, self.uuid\n        )",
  "class PredictProcessDied(Exception):\n    pass",
  "class GenericPredict(BatchFilter):\n    \"\"\"Generic predict node to add predictions of a trained network to each each\n    batch that passes through. This node alone does nothing and should be\n    subclassed for concrete implementations.\n\n    Args:\n\n        inputs (dict): Dictionary from names of input layers in the network to\n            :class:``ArrayKey`` or batch attribute name as string.\n\n        outputs (dict): Dictionary from the names of output layers in the\n            network to :class:``ArrayKey``. New arrays will be generated by\n            this node for each entry (if requested downstream).\n\n        array_specs (dict, optional): An optional dictionary of\n            :class:`ArrayKey` to :class:`ArraySpec` to set the array specs\n            generated arrays (``outputs`` and ``gradients``). This is useful\n            to set the ``voxel_size``, for example, if they differ from the\n            voxel size of the input arrays. Only fields that are not ``None``\n            in the given :class:`ArraySpec` will be used.\n\n        spawn_subprocess (bool, optional): Whether to run ``predict`` in a\n            separate process. Default is false.\n    \"\"\"\n\n    def __init__(self, inputs, outputs, array_specs=None, spawn_subprocess=False):\n        self.initialized = False\n        self.inputs = inputs\n        self.outputs = outputs\n        self.array_specs = {} if array_specs is None else array_specs\n        self.spawn_subprocess = spawn_subprocess\n        self.timer_start = None\n\n    def setup(self):\n        # get common voxel size of inputs, or None if they differ\n        common_voxel_size = None\n        for key in self.inputs.values():\n            if not isinstance(key, ArrayKey):\n                continue\n\n            voxel_size = self.spec[key].voxel_size\n\n            if common_voxel_size is None:\n                common_voxel_size = voxel_size\n            elif common_voxel_size != voxel_size:\n                common_voxel_size = None\n                break\n\n        # announce provided outputs\n        for key in self.outputs.values():\n            if key in self.array_specs:\n                spec = self.array_specs[key].copy()\n            else:\n                spec = ArraySpec()\n\n            if spec.voxel_size is None and not spec.nonspatial:\n                assert common_voxel_size is not None, (\n                    \"There is no common voxel size of the inputs, and no \"\n                    \"ArraySpec has been given for %s that defines \"\n                    \"voxel_size.\" % key\n                )\n\n                spec.voxel_size = common_voxel_size\n\n            if spec.interpolatable is None:\n                # default for predictions\n                spec.interpolatable = False\n\n            self.provides(key, spec)\n\n        if self.spawn_subprocess:\n            # start prediction as a producer pool, so that we can gracefully\n            # exit if anything goes wrong\n            self.worker = ProducerPool([self.__produce_predict_batch], queue_size=1)\n            self.batch_in = multiprocessing.Queue(maxsize=1)\n            self.batch_in_lock = multiprocessing.Lock()\n            self.batch_out_lock = multiprocessing.Lock()\n            self.worker.start()\n\n    def teardown(self):\n        if self.spawn_subprocess:\n            # signal \"stop\"\n            try:\n                self.batch_in.put((None, None), timeout=2)\n            except Full:\n                # worker process might be stopped already\n                pass\n            try:\n                self.worker.get(timeout=2)\n            except NoResult:\n                pass\n            self.worker.stop()\n        else:\n            self.stop()\n\n    def prepare(self, request):\n        if not self.initialized and not self.spawn_subprocess:\n            self.start()\n            self.initialized = True\n\n        deps = BatchRequest()\n        for key in self.inputs.values():\n            deps[key] = request[key]\n        return deps\n\n    def process(self, batch, request):\n        if self.spawn_subprocess:\n            start = time.time()\n            self.batch_in_lock.acquire()\n            logger.debug(\"waited for batch in lock for %.3fs\", time.time() - start)\n            start = time.time()\n            self.batch_in.put((batch, request))\n            logger.debug(\"queued batch for %.3fs\", time.time() - start)\n\n            start = time.time()\n            with self.batch_out_lock:\n                logger.debug(\"waited for batch out lock for %.3fs\", time.time() - start)\n\n                start = time.time()\n                self.batch_in_lock.release()\n                logger.debug(\"released batch in lock for %.3fs\", time.time() - start)\n                try:\n                    start = time.time()\n                    out = self.worker.get()\n                    logger.debug(\"retreived batch for %.3fs\", time.time() - start)\n                except WorkersDied:\n                    raise PredictProcessDied()\n\n            for array_key in self.outputs.values():\n                if array_key in request:\n                    batch.arrays[array_key] = out.arrays[array_key]\n\n        else:\n            self.predict(batch, request)\n\n    def start(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called before the first call to :fun:`predict`,\n        from the same process that :fun:`predict` will be called from. Use\n        this to initialize your model and hardware.\n        \"\"\"\n        pass\n\n    def predict(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        In this method, an implementation should predict arrays on the given\n        batch. Output arrays should be created according to the given request\n        and added to ``batch``.\"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'predict'\" % self.name())\n\n    def stop(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called after the last call to :fun:`predict`,\n        from the same process that :fun:`predict` will be called from. Use\n        this to tear down your model and free training hardware.\n        \"\"\"\n        pass\n\n    def __produce_predict_batch(self):\n        \"\"\"Process one batch.\"\"\"\n\n        if not self.initialized:\n            self.start()\n            self.initialized = True\n\n        self.time_out = 0\n        if self.timer_start is not None:\n            self.time_out = time.time() - self.timer_start\n\n        self.timer_start = time.time()\n        batch, request = self.batch_in.get()\n        self.time_in = time.time() - self.timer_start\n\n        # stop signal\n        if batch is None:\n            self.stop()\n            return None\n\n        self.timer_start = time.time()\n        self.predict(batch, request)\n        self.time_predict = time.time() - self.timer_start\n        self.timer_start = time.time()\n\n        logger.debug(\n            \"batch in: %.3fs, predict: %.3fs, batch out: %.3fs\",\n            self.time_in,\n            self.time_predict,\n            self.time_out,\n        )\n\n        return batch",
  "def __init__(self, inputs, outputs, array_specs=None, spawn_subprocess=False):\n        self.initialized = False\n        self.inputs = inputs\n        self.outputs = outputs\n        self.array_specs = {} if array_specs is None else array_specs\n        self.spawn_subprocess = spawn_subprocess\n        self.timer_start = None",
  "def setup(self):\n        # get common voxel size of inputs, or None if they differ\n        common_voxel_size = None\n        for key in self.inputs.values():\n            if not isinstance(key, ArrayKey):\n                continue\n\n            voxel_size = self.spec[key].voxel_size\n\n            if common_voxel_size is None:\n                common_voxel_size = voxel_size\n            elif common_voxel_size != voxel_size:\n                common_voxel_size = None\n                break\n\n        # announce provided outputs\n        for key in self.outputs.values():\n            if key in self.array_specs:\n                spec = self.array_specs[key].copy()\n            else:\n                spec = ArraySpec()\n\n            if spec.voxel_size is None and not spec.nonspatial:\n                assert common_voxel_size is not None, (\n                    \"There is no common voxel size of the inputs, and no \"\n                    \"ArraySpec has been given for %s that defines \"\n                    \"voxel_size.\" % key\n                )\n\n                spec.voxel_size = common_voxel_size\n\n            if spec.interpolatable is None:\n                # default for predictions\n                spec.interpolatable = False\n\n            self.provides(key, spec)\n\n        if self.spawn_subprocess:\n            # start prediction as a producer pool, so that we can gracefully\n            # exit if anything goes wrong\n            self.worker = ProducerPool([self.__produce_predict_batch], queue_size=1)\n            self.batch_in = multiprocessing.Queue(maxsize=1)\n            self.batch_in_lock = multiprocessing.Lock()\n            self.batch_out_lock = multiprocessing.Lock()\n            self.worker.start()",
  "def teardown(self):\n        if self.spawn_subprocess:\n            # signal \"stop\"\n            try:\n                self.batch_in.put((None, None), timeout=2)\n            except Full:\n                # worker process might be stopped already\n                pass\n            try:\n                self.worker.get(timeout=2)\n            except NoResult:\n                pass\n            self.worker.stop()\n        else:\n            self.stop()",
  "def prepare(self, request):\n        if not self.initialized and not self.spawn_subprocess:\n            self.start()\n            self.initialized = True\n\n        deps = BatchRequest()\n        for key in self.inputs.values():\n            deps[key] = request[key]\n        return deps",
  "def process(self, batch, request):\n        if self.spawn_subprocess:\n            start = time.time()\n            self.batch_in_lock.acquire()\n            logger.debug(\"waited for batch in lock for %.3fs\", time.time() - start)\n            start = time.time()\n            self.batch_in.put((batch, request))\n            logger.debug(\"queued batch for %.3fs\", time.time() - start)\n\n            start = time.time()\n            with self.batch_out_lock:\n                logger.debug(\"waited for batch out lock for %.3fs\", time.time() - start)\n\n                start = time.time()\n                self.batch_in_lock.release()\n                logger.debug(\"released batch in lock for %.3fs\", time.time() - start)\n                try:\n                    start = time.time()\n                    out = self.worker.get()\n                    logger.debug(\"retreived batch for %.3fs\", time.time() - start)\n                except WorkersDied:\n                    raise PredictProcessDied()\n\n            for array_key in self.outputs.values():\n                if array_key in request:\n                    batch.arrays[array_key] = out.arrays[array_key]\n\n        else:\n            self.predict(batch, request)",
  "def start(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called before the first call to :fun:`predict`,\n        from the same process that :fun:`predict` will be called from. Use\n        this to initialize your model and hardware.\n        \"\"\"\n        pass",
  "def predict(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        In this method, an implementation should predict arrays on the given\n        batch. Output arrays should be created according to the given request\n        and added to ``batch``.\"\"\"\n        raise NotImplementedError(\"Class %s does not implement 'predict'\" % self.name())",
  "def stop(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called after the last call to :fun:`predict`,\n        from the same process that :fun:`predict` will be called from. Use\n        this to tear down your model and free training hardware.\n        \"\"\"\n        pass",
  "def __produce_predict_batch(self):\n        \"\"\"Process one batch.\"\"\"\n\n        if not self.initialized:\n            self.start()\n            self.initialized = True\n\n        self.time_out = 0\n        if self.timer_start is not None:\n            self.time_out = time.time() - self.timer_start\n\n        self.timer_start = time.time()\n        batch, request = self.batch_in.get()\n        self.time_in = time.time() - self.timer_start\n\n        # stop signal\n        if batch is None:\n            self.stop()\n            return None\n\n        self.timer_start = time.time()\n        self.predict(batch, request)\n        self.time_predict = time.time() - self.timer_start\n        self.timer_start = time.time()\n\n        logger.debug(\n            \"batch in: %.3fs, predict: %.3fs, batch out: %.3fs\",\n            self.time_in,\n            self.time_predict,\n            self.time_out,\n        )\n\n        return batch",
  "class Scan(BatchFilter):\n    \"\"\"Iteratively requests batches of size ``reference`` from upstream\n    providers in a scanning fashion, until all requested ROIs are covered. If\n    the batch request to this node is empty, it will scan the complete upstream\n    ROIs (and return nothing). Otherwise, it scans only the requested ROIs and\n    returns a batch assembled of the smaller requests. In either case, the\n    upstream requests will be contained in the downstream requested ROI or\n    upstream ROIs.\n\n    See also :class:`Hdf5Write`.\n\n    Args:\n\n        reference (:class:`BatchRequest`):\n\n            A reference :class:`BatchRequest`. This request will be shifted in\n            a scanning fashion over the upstream ROIs of the requested arrays\n            or points.\n\n        num_workers (``int``, optional):\n\n            If set to >1, upstream requests are made in parallel with that\n            number of workers.\n\n        cache_size (``int``, optional):\n\n            If multiple workers are used, how many batches to hold at most.\n    \"\"\"\n\n    def __init__(self, reference, num_workers=1, cache_size=50):\n        self.reference = reference.copy()\n        self.num_workers = num_workers\n        self.cache_size = cache_size\n        self.workers = None\n        self.batch = None\n\n    def setup(self):\n        if self.num_workers > 1:\n            self.request_queue = multiprocessing.Queue(maxsize=0)\n            self.workers = ProducerPool(\n                [self._worker_get_chunk for _ in range(self.num_workers)],\n                queue_size=self.cache_size,\n            )\n            self.workers.start()\n\n    def teardown(self):\n        if self.num_workers > 1:\n            self.workers.stop()\n\n    def provide(self, request):\n        empty_request = len(request) == 0\n        if empty_request:\n            scan_spec = self.spec\n        else:\n            scan_spec = request\n\n        stride = self._get_stride()\n        shift_roi = self._get_shift_roi(scan_spec)\n\n        shifts = self._enumerate_shifts(shift_roi, stride)\n        num_chunks = len(shifts)\n\n        logger.info(\"scanning over %d chunks\", num_chunks)\n\n        # the batch to return\n        self.batch = Batch()\n\n        if self.num_workers > 1:\n            for shift in shifts:\n                shifted_reference = self._shift_request(self.reference, shift)\n                self.request_queue.put(shifted_reference)\n\n            for i in tqdm.tqdm(range(num_chunks)):\n                chunk = self.workers.get()\n\n                if not empty_request:\n                    self._add_to_batch(request, chunk)\n\n                logger.debug(\"processed chunk %d/%d\", i + 1, num_chunks)\n\n        else:\n            for i, shift in enumerate(tqdm.tqdm(shifts)):\n                shifted_reference = self._shift_request(self.reference, shift)\n                chunk = self._get_chunk(shifted_reference)\n\n                if not empty_request:\n                    self._add_to_batch(request, chunk)\n\n                logger.debug(\"processed chunk %d/%d\", i + 1, num_chunks)\n\n        batch = self.batch\n        self.batch = None\n\n        logger.debug(\"returning batch %s\", batch)\n\n        return batch\n\n    def _get_stride(self):\n        \"\"\"Get the maximal amount by which ``reference`` can be moved, such\n        that it tiles the space.\"\"\"\n\n        stride = None\n\n        # get the least common multiple of all voxel sizes, we have to stride\n        # at least that far\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(self.reference.array_specs.keys())\n\n        # that's just the minimal size in each dimension\n        for key, reference_spec in self.reference.items():\n            shape = reference_spec.roi.shape\n\n            for d in range(len(lcm_voxel_size)):\n                assert shape[d] >= lcm_voxel_size[d], (\n                    \"Shape of reference \"\n                    \"ROI %s for %s is \"\n                    \"smaller than least \"\n                    \"common multiple of \"\n                    \"voxel size \"\n                    \"%s\" % (reference_spec.roi, key, lcm_voxel_size)\n                )\n\n            if stride is None:\n                stride = shape\n            else:\n                stride = Coordinate((min(a, b) for a, b in zip(stride, shape)))\n\n        return stride\n\n    def _get_shift_roi(self, spec):\n        \"\"\"Get the minimal and maximal shift (as a ROI) to apply to\n        ``self.reference``, such that it is still fully contained in ``spec``.\n        \"\"\"\n\n        total_shift_roi = None\n\n        # get individual shift ROIs and intersect them\n        for key, reference_spec in self.reference.items():\n            logger.debug(\"getting shift roi for %s with spec %s\", key, reference_spec)\n\n            if key not in spec:\n                logger.debug(\"skipping, %s not in upstream spec\", key)\n                continue\n            if spec[key].roi is None:\n                logger.debug(\"skipping, %s has not ROI\", key)\n                continue\n\n            logger.debug(\"upstream ROI is %s\", spec[key].roi)\n\n            for r, s in zip(reference_spec.roi.shape, spec[key].roi.shape):\n                assert s is None or r <= s, (\n                    \"reference %s with ROI %s does not fit into provided \"\n                    \"upstream %s\" % (key, reference_spec.roi, spec[key].roi)\n                )\n\n            # we have a reference ROI\n            #\n            #    [--------) [9]\n            #    3        12\n            #\n            # and a spec ROI\n            #\n            #                 [---------------) [16]\n            #                 16              32\n            #\n            # min and max shifts of reference are\n            #\n            #                 [--------) [9]\n            #                 16       25\n            #                        [--------) [9]\n            #                        23       32\n            #\n            # therefore, all possible ways to shift the reference such that it\n            # is contained in the spec are at least 16-3=13 and at most 23-3=20\n            # (inclusive)\n            #\n            #              [-------) [8]\n            #              13      21\n            #\n            # 1. the starting point is beginning of spec - beginning of reference\n            # 2. the length is length of spec - length of reference + 1\n\n            # 1. get the starting point of the shift ROI\n            shift_begin = spec[key].roi.begin - reference_spec.roi.begin\n\n            # 2. get the shape of the shift ROI\n            shift_shape = spec[key].roi.shape - reference_spec.roi.shape + 1\n\n            # create a ROI...\n            shift_roi = Roi(shift_begin, shift_shape)\n\n            logger.debug(\"shift ROI for %s is %s\", key, shift_roi)\n\n            # ...and intersect it with previous shift ROIs\n            if total_shift_roi is None:\n                total_shift_roi = shift_roi\n            else:\n                total_shift_roi = total_shift_roi.intersect(shift_roi)\n                if total_shift_roi.empty:\n                    raise RuntimeError(\n                        \"There is no location where the ROIs \"\n                        \"the reference %s are contained in the \"\n                        \"request/upstream ROIs \"\n                        \"%s.\" % (self.reference, spec)\n                    )\n\n            logger.debug(\n                \"intersected with total shift ROI this yields %s\", total_shift_roi\n            )\n\n        if total_shift_roi is None:\n            raise RuntimeError(\n                \"None of the upstream ROIs are bounded (all \"\n                \"ROIs are None). Scan needs at least one \"\n                \"bounded upstream ROI.\"\n            )\n\n        return total_shift_roi\n\n    def _enumerate_shifts(self, shift_roi, stride):\n        \"\"\"Produces a sequence of shift coordinates starting at the beginning\n        of ``shift_roi``, progressing with ``stride``. The maximum shift\n        coordinate in any dimension will be the last point inside the shift roi\n        in this dimension.\"\"\"\n\n        min_shift = shift_roi.offset\n        max_shift = max(min_shift, Coordinate(m - 1 for m in shift_roi.end))\n\n        shift = np.array(min_shift)\n        shifts = []\n\n        dims = len(min_shift)\n\n        logger.debug(\"enumerating possible shifts of %s in %s\", stride, shift_roi)\n\n        while True:\n            logger.debug(\"adding %s\", shift)\n            shifts.append(Coordinate(shift))\n\n            if (shift == max_shift).all():\n                break\n\n            # count up dimensions\n            for d in range(dims):\n                if shift[d] >= max_shift[d]:\n                    if d == dims - 1:\n                        break\n                    shift[d] = min_shift[d]\n                else:\n                    shift[d] += stride[d]\n                    # snap to last possible shift, don't overshoot\n                    if shift[d] > max_shift[d]:\n                        shift[d] = max_shift[d]\n                    break\n\n        return shifts\n\n    def _shift_request(self, request, shift):\n        shifted = request.copy()\n        for _, spec in shifted.items():\n            spec.roi = spec.roi.shift(shift)\n\n        return shifted\n\n    def _worker_get_chunk(self):\n        request = self.request_queue.get()\n        return self._get_chunk(request)\n\n    def _get_chunk(self, request):\n        return self.get_upstream_provider().request_batch(request)\n\n    def _add_to_batch(self, spec, chunk):\n        if self.batch.get_total_roi() is None:\n            self.batch = self._setup_batch(spec, chunk)\n        self.batch.profiling_stats.merge_with(chunk.profiling_stats)\n\n        for array_key, array in chunk.arrays.items():\n            if array_key not in spec:\n                continue\n            self._fill(\n                self.batch.arrays[array_key].data,\n                array.data,\n                spec.array_specs[array_key].roi,\n                array.spec.roi,\n                self.spec[array_key].voxel_size,\n            )\n\n        for graph_key, graphs in chunk.graphs.items():\n            if graph_key not in spec:\n                continue\n            self._fill_points(\n                self.batch.graphs[graph_key],\n                graphs,\n                spec.graph_specs[graph_key].roi,\n                graphs.spec.roi,\n            )\n\n    def _setup_batch(self, batch_spec, chunk):\n        \"\"\"Allocate a batch matching the sizes of ``batch_spec``, using\n        ``chunk`` as template.\"\"\"\n\n        batch = Batch()\n\n        for array_key, spec in batch_spec.array_specs.items():\n            roi = spec.roi\n            voxel_size = self.spec[array_key].voxel_size\n\n            # get the 'non-spatial' shape of the chunk-batch\n            # and append the shape of the request to it\n            array = chunk.arrays[array_key]\n            shape = array.data.shape[: -roi.dims]\n            shape += roi.shape // voxel_size\n\n            spec = self.spec[array_key].copy()\n            spec.roi = roi\n            logger.info(\"allocating array of shape %s for %s\", shape, array_key)\n            batch.arrays[array_key] = Array(\n                data=np.zeros(shape, dtype=spec.dtype), spec=spec\n            )\n\n        for graph_key, spec in batch_spec.graph_specs.items():\n            roi = spec.roi\n            spec = self.spec[graph_key].copy()\n            spec.roi = roi\n            batch.graphs[graph_key] = Graph(nodes=[], edges=[], spec=spec)\n\n        logger.debug(\"setup batch to fill %s\", batch)\n\n        return batch\n\n    def _fill(self, a, b, roi_a, roi_b, voxel_size):\n        logger.debug(\"filling \" + str(roi_b) + \" into \" + str(roi_a))\n\n        roi_a = roi_a // voxel_size\n        roi_b = roi_b // voxel_size\n\n        common_roi = roi_a.intersect(roi_b)\n        if common_roi.empty:\n            return\n\n        common_in_a_roi = common_roi - roi_a.offset\n        common_in_b_roi = common_roi - roi_b.offset\n\n        slices_a = common_in_a_roi.get_bounding_box()\n        slices_b = common_in_b_roi.get_bounding_box()\n\n        if len(a.shape) > len(slices_a):\n            slices_a = (slice(None),) * (len(a.shape) - len(slices_a)) + slices_a\n            slices_b = (slice(None),) * (len(b.shape) - len(slices_b)) + slices_b\n\n        a[slices_a] = b[slices_b]\n\n    def _fill_points(self, a, b, roi_a, roi_b):\n        \"\"\"\n        Take points from b and add them to a.\n        Nodes marked temporary must be ignored. Temporary nodes are nodes\n        that were created during processing. Since it is impossible to know\n        in general, that a node created during processing of a subgraph was\n        not assigned an id that is already used by the full graph, we cannot\n        include temporary nodes and assume there will not be ambiguous node\n        id's that correspond to multiple distinct nodes.\n        \"\"\"\n        logger.debug(\"filling points of \" + str(roi_b) + \" into points of\" + str(roi_a))\n\n        common_roi = roi_a.intersect(roi_b)\n        if common_roi is None:\n            return\n\n        for node in b.nodes:\n            if not node.temporary and roi_a.contains(node.location):\n                a.add_node(node)\n        for e in b.edges:\n            bu = b.node(e.u)\n            bv = b.node(e.v)\n            if (\n                not bu.temporary\n                and not bv.temporary\n                and a.contains(bu.id)\n                and a.contains(bv.id)\n            ):\n                a.add_edge(e)",
  "def __init__(self, reference, num_workers=1, cache_size=50):\n        self.reference = reference.copy()\n        self.num_workers = num_workers\n        self.cache_size = cache_size\n        self.workers = None\n        self.batch = None",
  "def setup(self):\n        if self.num_workers > 1:\n            self.request_queue = multiprocessing.Queue(maxsize=0)\n            self.workers = ProducerPool(\n                [self._worker_get_chunk for _ in range(self.num_workers)],\n                queue_size=self.cache_size,\n            )\n            self.workers.start()",
  "def teardown(self):\n        if self.num_workers > 1:\n            self.workers.stop()",
  "def provide(self, request):\n        empty_request = len(request) == 0\n        if empty_request:\n            scan_spec = self.spec\n        else:\n            scan_spec = request\n\n        stride = self._get_stride()\n        shift_roi = self._get_shift_roi(scan_spec)\n\n        shifts = self._enumerate_shifts(shift_roi, stride)\n        num_chunks = len(shifts)\n\n        logger.info(\"scanning over %d chunks\", num_chunks)\n\n        # the batch to return\n        self.batch = Batch()\n\n        if self.num_workers > 1:\n            for shift in shifts:\n                shifted_reference = self._shift_request(self.reference, shift)\n                self.request_queue.put(shifted_reference)\n\n            for i in tqdm.tqdm(range(num_chunks)):\n                chunk = self.workers.get()\n\n                if not empty_request:\n                    self._add_to_batch(request, chunk)\n\n                logger.debug(\"processed chunk %d/%d\", i + 1, num_chunks)\n\n        else:\n            for i, shift in enumerate(tqdm.tqdm(shifts)):\n                shifted_reference = self._shift_request(self.reference, shift)\n                chunk = self._get_chunk(shifted_reference)\n\n                if not empty_request:\n                    self._add_to_batch(request, chunk)\n\n                logger.debug(\"processed chunk %d/%d\", i + 1, num_chunks)\n\n        batch = self.batch\n        self.batch = None\n\n        logger.debug(\"returning batch %s\", batch)\n\n        return batch",
  "def _get_stride(self):\n        \"\"\"Get the maximal amount by which ``reference`` can be moved, such\n        that it tiles the space.\"\"\"\n\n        stride = None\n\n        # get the least common multiple of all voxel sizes, we have to stride\n        # at least that far\n        lcm_voxel_size = self.spec.get_lcm_voxel_size(self.reference.array_specs.keys())\n\n        # that's just the minimal size in each dimension\n        for key, reference_spec in self.reference.items():\n            shape = reference_spec.roi.shape\n\n            for d in range(len(lcm_voxel_size)):\n                assert shape[d] >= lcm_voxel_size[d], (\n                    \"Shape of reference \"\n                    \"ROI %s for %s is \"\n                    \"smaller than least \"\n                    \"common multiple of \"\n                    \"voxel size \"\n                    \"%s\" % (reference_spec.roi, key, lcm_voxel_size)\n                )\n\n            if stride is None:\n                stride = shape\n            else:\n                stride = Coordinate((min(a, b) for a, b in zip(stride, shape)))\n\n        return stride",
  "def _get_shift_roi(self, spec):\n        \"\"\"Get the minimal and maximal shift (as a ROI) to apply to\n        ``self.reference``, such that it is still fully contained in ``spec``.\n        \"\"\"\n\n        total_shift_roi = None\n\n        # get individual shift ROIs and intersect them\n        for key, reference_spec in self.reference.items():\n            logger.debug(\"getting shift roi for %s with spec %s\", key, reference_spec)\n\n            if key not in spec:\n                logger.debug(\"skipping, %s not in upstream spec\", key)\n                continue\n            if spec[key].roi is None:\n                logger.debug(\"skipping, %s has not ROI\", key)\n                continue\n\n            logger.debug(\"upstream ROI is %s\", spec[key].roi)\n\n            for r, s in zip(reference_spec.roi.shape, spec[key].roi.shape):\n                assert s is None or r <= s, (\n                    \"reference %s with ROI %s does not fit into provided \"\n                    \"upstream %s\" % (key, reference_spec.roi, spec[key].roi)\n                )\n\n            # we have a reference ROI\n            #\n            #    [--------) [9]\n            #    3        12\n            #\n            # and a spec ROI\n            #\n            #                 [---------------) [16]\n            #                 16              32\n            #\n            # min and max shifts of reference are\n            #\n            #                 [--------) [9]\n            #                 16       25\n            #                        [--------) [9]\n            #                        23       32\n            #\n            # therefore, all possible ways to shift the reference such that it\n            # is contained in the spec are at least 16-3=13 and at most 23-3=20\n            # (inclusive)\n            #\n            #              [-------) [8]\n            #              13      21\n            #\n            # 1. the starting point is beginning of spec - beginning of reference\n            # 2. the length is length of spec - length of reference + 1\n\n            # 1. get the starting point of the shift ROI\n            shift_begin = spec[key].roi.begin - reference_spec.roi.begin\n\n            # 2. get the shape of the shift ROI\n            shift_shape = spec[key].roi.shape - reference_spec.roi.shape + 1\n\n            # create a ROI...\n            shift_roi = Roi(shift_begin, shift_shape)\n\n            logger.debug(\"shift ROI for %s is %s\", key, shift_roi)\n\n            # ...and intersect it with previous shift ROIs\n            if total_shift_roi is None:\n                total_shift_roi = shift_roi\n            else:\n                total_shift_roi = total_shift_roi.intersect(shift_roi)\n                if total_shift_roi.empty:\n                    raise RuntimeError(\n                        \"There is no location where the ROIs \"\n                        \"the reference %s are contained in the \"\n                        \"request/upstream ROIs \"\n                        \"%s.\" % (self.reference, spec)\n                    )\n\n            logger.debug(\n                \"intersected with total shift ROI this yields %s\", total_shift_roi\n            )\n\n        if total_shift_roi is None:\n            raise RuntimeError(\n                \"None of the upstream ROIs are bounded (all \"\n                \"ROIs are None). Scan needs at least one \"\n                \"bounded upstream ROI.\"\n            )\n\n        return total_shift_roi",
  "def _enumerate_shifts(self, shift_roi, stride):\n        \"\"\"Produces a sequence of shift coordinates starting at the beginning\n        of ``shift_roi``, progressing with ``stride``. The maximum shift\n        coordinate in any dimension will be the last point inside the shift roi\n        in this dimension.\"\"\"\n\n        min_shift = shift_roi.offset\n        max_shift = max(min_shift, Coordinate(m - 1 for m in shift_roi.end))\n\n        shift = np.array(min_shift)\n        shifts = []\n\n        dims = len(min_shift)\n\n        logger.debug(\"enumerating possible shifts of %s in %s\", stride, shift_roi)\n\n        while True:\n            logger.debug(\"adding %s\", shift)\n            shifts.append(Coordinate(shift))\n\n            if (shift == max_shift).all():\n                break\n\n            # count up dimensions\n            for d in range(dims):\n                if shift[d] >= max_shift[d]:\n                    if d == dims - 1:\n                        break\n                    shift[d] = min_shift[d]\n                else:\n                    shift[d] += stride[d]\n                    # snap to last possible shift, don't overshoot\n                    if shift[d] > max_shift[d]:\n                        shift[d] = max_shift[d]\n                    break\n\n        return shifts",
  "def _shift_request(self, request, shift):\n        shifted = request.copy()\n        for _, spec in shifted.items():\n            spec.roi = spec.roi.shift(shift)\n\n        return shifted",
  "def _worker_get_chunk(self):\n        request = self.request_queue.get()\n        return self._get_chunk(request)",
  "def _get_chunk(self, request):\n        return self.get_upstream_provider().request_batch(request)",
  "def _add_to_batch(self, spec, chunk):\n        if self.batch.get_total_roi() is None:\n            self.batch = self._setup_batch(spec, chunk)\n        self.batch.profiling_stats.merge_with(chunk.profiling_stats)\n\n        for array_key, array in chunk.arrays.items():\n            if array_key not in spec:\n                continue\n            self._fill(\n                self.batch.arrays[array_key].data,\n                array.data,\n                spec.array_specs[array_key].roi,\n                array.spec.roi,\n                self.spec[array_key].voxel_size,\n            )\n\n        for graph_key, graphs in chunk.graphs.items():\n            if graph_key not in spec:\n                continue\n            self._fill_points(\n                self.batch.graphs[graph_key],\n                graphs,\n                spec.graph_specs[graph_key].roi,\n                graphs.spec.roi,\n            )",
  "def _setup_batch(self, batch_spec, chunk):\n        \"\"\"Allocate a batch matching the sizes of ``batch_spec``, using\n        ``chunk`` as template.\"\"\"\n\n        batch = Batch()\n\n        for array_key, spec in batch_spec.array_specs.items():\n            roi = spec.roi\n            voxel_size = self.spec[array_key].voxel_size\n\n            # get the 'non-spatial' shape of the chunk-batch\n            # and append the shape of the request to it\n            array = chunk.arrays[array_key]\n            shape = array.data.shape[: -roi.dims]\n            shape += roi.shape // voxel_size\n\n            spec = self.spec[array_key].copy()\n            spec.roi = roi\n            logger.info(\"allocating array of shape %s for %s\", shape, array_key)\n            batch.arrays[array_key] = Array(\n                data=np.zeros(shape, dtype=spec.dtype), spec=spec\n            )\n\n        for graph_key, spec in batch_spec.graph_specs.items():\n            roi = spec.roi\n            spec = self.spec[graph_key].copy()\n            spec.roi = roi\n            batch.graphs[graph_key] = Graph(nodes=[], edges=[], spec=spec)\n\n        logger.debug(\"setup batch to fill %s\", batch)\n\n        return batch",
  "def _fill(self, a, b, roi_a, roi_b, voxel_size):\n        logger.debug(\"filling \" + str(roi_b) + \" into \" + str(roi_a))\n\n        roi_a = roi_a // voxel_size\n        roi_b = roi_b // voxel_size\n\n        common_roi = roi_a.intersect(roi_b)\n        if common_roi.empty:\n            return\n\n        common_in_a_roi = common_roi - roi_a.offset\n        common_in_b_roi = common_roi - roi_b.offset\n\n        slices_a = common_in_a_roi.get_bounding_box()\n        slices_b = common_in_b_roi.get_bounding_box()\n\n        if len(a.shape) > len(slices_a):\n            slices_a = (slice(None),) * (len(a.shape) - len(slices_a)) + slices_a\n            slices_b = (slice(None),) * (len(b.shape) - len(slices_b)) + slices_b\n\n        a[slices_a] = b[slices_b]",
  "def _fill_points(self, a, b, roi_a, roi_b):\n        \"\"\"\n        Take points from b and add them to a.\n        Nodes marked temporary must be ignored. Temporary nodes are nodes\n        that were created during processing. Since it is impossible to know\n        in general, that a node created during processing of a subgraph was\n        not assigned an id that is already used by the full graph, we cannot\n        include temporary nodes and assume there will not be ambiguous node\n        id's that correspond to multiple distinct nodes.\n        \"\"\"\n        logger.debug(\"filling points of \" + str(roi_b) + \" into points of\" + str(roi_a))\n\n        common_roi = roi_a.intersect(roi_b)\n        if common_roi is None:\n            return\n\n        for node in b.nodes:\n            if not node.temporary and roi_a.contains(node.location):\n                a.add_node(node)\n        for e in b.edges:\n            bu = b.node(e.u)\n            bv = b.node(e.v)\n            if (\n                not bu.temporary\n                and not bv.temporary\n                and a.contains(bu.id)\n                and a.contains(bv.id)\n            ):\n                a.add_edge(e)",
  "class GraphSource(BatchProvider):\n    \"\"\"Creates a gunpowder graph source from a daisy graph provider.\n    Queries for graphs from a given Roi will only return edges completely\n    contained within the Roi - edges that cross the boundary will not be\n    included.\n\n    Arguments:\n\n        graph_provider (:class:`daisy.SharedGraphProvider`):\n            A daisy graph provider to read the graph from.\n            Can be backed by MongoDB or any other implemented backend.\n\n        graph (:class:`GraphKey`):\n            The key of the graph to create\n\n        graph_spec (:class:`GraphSpec`, optional):\n            An optional :class:`GraphSpec` containing a roi and optionally\n            whether the graph is directed. The default is to have an unbounded\n            roi and detect directedness from the graph_provider.\n    \"\"\"\n\n    def __init__(self, graph_provider, graph, graph_spec=None):\n        self.graph_provider = graph_provider\n        self.graph = graph\n        self.graph_spec = graph_spec\n\n    def setup(self):\n        if self.graph_spec is not None:\n            roi = self.graph_spec.roi\n            if self.graph_spec.directed is not None:\n                assert self.graph_spec.directed == self.graph_provider.directed\n        else:\n            roi = None\n        spec = GraphSpec(roi=roi, directed=self.graph_provider.directed)\n        self.provides(self.graph, spec)\n\n    def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n        batch = Batch()\n        roi = request[self.graph].roi.copy()\n        graph = GraphSource.create_gp_graph_from_daisy(self.graph_provider, roi)\n        batch.graphs[self.graph] = graph\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch\n\n    @staticmethod\n    def create_gp_graph_from_daisy(graph_provider, roi):\n        \"\"\"A static method to convert a daisy graph into a gunpowder graph.\n        Only includes edges if both endpoints are within the roi.\n\n        Arguments:\n            graph_provider (:class:`daisy.SharedGraphProvider`)\n                A daisy graph provider to read the graph from\n\n            roi (:class:`Roi`):\n                The roi in which to read the graph\n\n        Returns:\n            An instance of :class:`Graph` containing the nodes and edges read\n            from the daisy graph provider in the given roi.\n\n        \"\"\"\n        logger.debug(\"Creating gunpowder graph from daisy graph provider\")\n        daisy_graph = graph_provider[roi]\n        logger.debug(\"%d nodes found in roi %s\", len(daisy_graph), roi)\n        spec = GraphSpec(roi=roi, directed=daisy_graph.is_directed())\n        dangling_nodes = []\n        for node, data in daisy_graph.nodes(data=True):\n            position_attribute = graph_provider.position_attribute\n            if type(position_attribute) == list:\n                if position_attribute[0] not in data:\n                    dangling_nodes.append(node)\n                    continue\n                location = np.array(\n                    [data[attr] for attr in position_attribute], dtype=np.float32\n                )\n            else:\n                if position_attribute not in data:\n                    dangling_nodes.append(node)\n                    continue\n                location = np.array(data[position_attribute], dtype=np.float32)\n            data[\"location\"] = location\n            data[\"id\"] = node\n\n        logger.debug(\"Dangling nodes: %s\", dangling_nodes)\n        for n in dangling_nodes:\n            daisy_graph.remove_node(n)\n\n        if daisy_graph.is_directed():\n            pure_nx_graph = nx.DiGraph()\n        else:\n            pure_nx_graph = nx.Graph()\n\n        pure_nx_graph.update(daisy_graph)\n        return Graph.from_nx_graph(pure_nx_graph, spec)",
  "def __init__(self, graph_provider, graph, graph_spec=None):\n        self.graph_provider = graph_provider\n        self.graph = graph\n        self.graph_spec = graph_spec",
  "def setup(self):\n        if self.graph_spec is not None:\n            roi = self.graph_spec.roi\n            if self.graph_spec.directed is not None:\n                assert self.graph_spec.directed == self.graph_provider.directed\n        else:\n            roi = None\n        spec = GraphSpec(roi=roi, directed=self.graph_provider.directed)\n        self.provides(self.graph, spec)",
  "def provide(self, request):\n        timing = Timing(self)\n        timing.start()\n        batch = Batch()\n        roi = request[self.graph].roi.copy()\n        graph = GraphSource.create_gp_graph_from_daisy(self.graph_provider, roi)\n        batch.graphs[self.graph] = graph\n\n        timing.stop()\n        batch.profiling_stats.add(timing)\n\n        return batch",
  "def create_gp_graph_from_daisy(graph_provider, roi):\n        \"\"\"A static method to convert a daisy graph into a gunpowder graph.\n        Only includes edges if both endpoints are within the roi.\n\n        Arguments:\n            graph_provider (:class:`daisy.SharedGraphProvider`)\n                A daisy graph provider to read the graph from\n\n            roi (:class:`Roi`):\n                The roi in which to read the graph\n\n        Returns:\n            An instance of :class:`Graph` containing the nodes and edges read\n            from the daisy graph provider in the given roi.\n\n        \"\"\"\n        logger.debug(\"Creating gunpowder graph from daisy graph provider\")\n        daisy_graph = graph_provider[roi]\n        logger.debug(\"%d nodes found in roi %s\", len(daisy_graph), roi)\n        spec = GraphSpec(roi=roi, directed=daisy_graph.is_directed())\n        dangling_nodes = []\n        for node, data in daisy_graph.nodes(data=True):\n            position_attribute = graph_provider.position_attribute\n            if type(position_attribute) == list:\n                if position_attribute[0] not in data:\n                    dangling_nodes.append(node)\n                    continue\n                location = np.array(\n                    [data[attr] for attr in position_attribute], dtype=np.float32\n                )\n            else:\n                if position_attribute not in data:\n                    dangling_nodes.append(node)\n                    continue\n                location = np.array(data[position_attribute], dtype=np.float32)\n            data[\"location\"] = location\n            data[\"id\"] = node\n\n        logger.debug(\"Dangling nodes: %s\", dangling_nodes)\n        for n in dangling_nodes:\n            daisy_graph.remove_node(n)\n\n        if daisy_graph.is_directed():\n            pure_nx_graph = nx.DiGraph()\n        else:\n            pure_nx_graph = nx.Graph()\n\n        pure_nx_graph.update(daisy_graph)\n        return Graph.from_nx_graph(pure_nx_graph, spec)",
  "class Hdf5LikeWrite(BatchFilter):\n    \"\"\"Assemble arrays of passing batches in one HDF5-like container. This is\n    useful to store chunks produced by :class:`Scan` on disk without keeping\n    the larger array in memory. The ROIs of the passing arrays will be used to\n    determine the position where to store the data in the dataset.\n\n    Args:\n\n        dataset_names (``dict``, :class:`ArrayKey` -> ``string``):\n\n            A dictionary from array keys to names of the datasets to store them\n            in.\n\n        output_dir (``string``):\n\n            The directory to save the container. Will be created, if it does\n            not exist.\n\n        output_filename (``string``):\n\n            The output filename of the container. Will be created, if it does\n            not exist, otherwise data is overwritten in the existing container.\n\n        compression_type (``string`` or ``int``):\n\n            Compression strategy.  Legal values are ``gzip``, ``szip``,\n            ``lzf``. If an integer between 1 and 10, this indicates ``gzip``\n            compression level.\n\n        dataset_dtypes (``dict``, :class:`ArrayKey` -> data type):\n\n            A dictionary from array keys to datatype (eg. ``np.int8``). If\n            given, arrays are stored using this type. The original arrays\n            within the pipeline remain unchanged.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_names,\n        output_dir=\".\",\n        output_filename=\"output.hdf\",\n        compression_type=None,\n        dataset_dtypes=None,\n    ):\n        warnings.warn(\n            \"HDF5LikeWrite is depricated and will soon be removed in v2.0\",\n            DeprecationWarning,\n        )\n\n        self.dataset_names = dataset_names\n        self.output_dir = output_dir\n        self.output_filename = output_filename\n        self.compression_type = compression_type\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.dataset_offsets = {}\n\n    def setup(self):\n        for key in self.dataset_names.keys():\n            self.updates(key, self.spec[key])\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.dataset_names.keys():\n            deps[key] = request[key]\n        return deps\n\n    def _open_file(self, filename):\n        raise NotImplementedError(\"Only implemented in subclasses\")\n\n    def _get_voxel_size(self, dataset):\n        return Coordinate(dataset.attrs[\"resolution\"])\n\n    def _get_offset(self, dataset):\n        return Coordinate(dataset.attrs[\"offset\"])\n\n    def _set_voxel_size(self, dataset, voxel_size):\n        dataset.attrs[\"resolution\"] = voxel_size\n\n    def _set_offset(self, dataset, offset):\n        dataset.attrs[\"offset\"] = offset\n\n    def init_datasets(self, batch):\n        filename = os.path.join(self.output_dir, self.output_filename)\n        logger.debug(\"Initializing container %s\", filename)\n\n        try:\n            os.makedirs(self.output_dir)\n        except:\n            pass\n\n        for array_key, dataset_name in self.dataset_names.items():\n            logger.debug(\"Initializing dataset for %s\", array_key)\n\n            assert array_key in self.spec, (\n                \"Asked to store %s, but is not provided upstream.\" % array_key\n            )\n            assert array_key in batch.arrays, (\n                \"Asked to store %s, but is not part of batch.\" % array_key\n            )\n\n            array = batch.arrays[array_key]\n            dims = array.spec.roi.dims\n            batch_shape = array.data.shape\n\n            with self._open_file(filename) as data_file:\n                # if a dataset already exists, read its meta-information (if\n                # present)\n                if dataset_name in data_file:\n                    offset = self._get_offset(data_file[dataset_name]) or Coordinate(\n                        (0,) * dims\n                    )\n\n                else:\n                    provided_roi = self.spec[array_key].roi\n\n                    if provided_roi is None:\n                        raise RuntimeError(\n                            \"Dataset %s does not exist in %s, and no ROI is \"\n                            \"provided for %s. I don't know how to initialize \"\n                            \"the dataset.\" % (dataset_name, filename, array_key)\n                        )\n\n                    offset = provided_roi.offset\n                    voxel_size = array.spec.voxel_size\n                    data_shape = provided_roi.shape // voxel_size\n\n                    logger.debug(\"Shape in voxels: %s\", data_shape)\n                    # add channel dimensions (if present)\n                    data_shape = batch_shape[:-dims] + data_shape\n                    logger.debug(\"Shape with channel dimensions: %s\", data_shape)\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                    else:\n                        dtype = batch.arrays[array_key].data.dtype\n\n                    logger.debug(\n                        \"create_dataset: %s, %s, %s, %s, offset=%s, resolution=%s\",\n                        dataset_name,\n                        data_shape,\n                        self.compression_type,\n                        dtype,\n                        offset,\n                        voxel_size,\n                    )\n\n                    dataset = data_file.create_dataset(\n                        name=dataset_name,\n                        shape=data_shape,\n                        compression=self.compression_type,\n                        dtype=dtype,\n                    )\n\n                    self._set_offset(dataset, offset)\n                    self._set_voxel_size(dataset, voxel_size)\n\n                logger.debug(\n                    \"%s (%s in %s) has offset %s\",\n                    array_key,\n                    dataset_name,\n                    filename,\n                    offset,\n                )\n                self.dataset_offsets[array_key] = offset\n\n    def process(self, batch, request):\n        filename = os.path.join(self.output_dir, self.output_filename)\n\n        if not self.dataset_offsets:\n            self.init_datasets(batch)\n\n        with self._open_file(filename) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                dataset = data_file[dataset_name]\n\n                array_roi = batch.arrays[array_key].spec.roi\n                voxel_size = self.spec[array_key].voxel_size\n                dims = array_roi.dims\n                channel_slices = (slice(None),) * max(0, len(dataset.shape) - dims)\n\n                dataset_roi = Roi(\n                    self.dataset_offsets[array_key],\n                    Coordinate(dataset.shape[-dims:]) * voxel_size,\n                )\n                common_roi = array_roi.intersect(dataset_roi)\n\n                if common_roi.empty:\n                    logger.warn(\n                        \"array %s with ROI %s lies outside of dataset ROI %s, \"\n                        \"skipping writing\" % (array_key, array_roi, dataset_roi)\n                    )\n                    continue\n\n                dataset_voxel_roi = (\n                    common_roi - self.dataset_offsets[array_key]\n                ) // voxel_size\n                dataset_voxel_slices = dataset_voxel_roi.to_slices()\n                array_voxel_roi = (common_roi - array_roi.offset) // voxel_size\n                array_voxel_slices = array_voxel_roi.to_slices()\n\n                logger.debug(\n                    \"writing %s to voxel coordinates %s\"\n                    % (array_key, dataset_voxel_roi)\n                )\n\n                data = batch.arrays[array_key].data[channel_slices + array_voxel_slices]\n                dataset[channel_slices + dataset_voxel_slices] = data",
  "def __init__(\n        self,\n        dataset_names,\n        output_dir=\".\",\n        output_filename=\"output.hdf\",\n        compression_type=None,\n        dataset_dtypes=None,\n    ):\n        warnings.warn(\n            \"HDF5LikeWrite is depricated and will soon be removed in v2.0\",\n            DeprecationWarning,\n        )\n\n        self.dataset_names = dataset_names\n        self.output_dir = output_dir\n        self.output_filename = output_filename\n        self.compression_type = compression_type\n        if dataset_dtypes is None:\n            self.dataset_dtypes = {}\n        else:\n            self.dataset_dtypes = dataset_dtypes\n\n        self.dataset_offsets = {}",
  "def setup(self):\n        for key in self.dataset_names.keys():\n            self.updates(key, self.spec[key])\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.dataset_names.keys():\n            deps[key] = request[key]\n        return deps",
  "def _open_file(self, filename):\n        raise NotImplementedError(\"Only implemented in subclasses\")",
  "def _get_voxel_size(self, dataset):\n        return Coordinate(dataset.attrs[\"resolution\"])",
  "def _get_offset(self, dataset):\n        return Coordinate(dataset.attrs[\"offset\"])",
  "def _set_voxel_size(self, dataset, voxel_size):\n        dataset.attrs[\"resolution\"] = voxel_size",
  "def _set_offset(self, dataset, offset):\n        dataset.attrs[\"offset\"] = offset",
  "def init_datasets(self, batch):\n        filename = os.path.join(self.output_dir, self.output_filename)\n        logger.debug(\"Initializing container %s\", filename)\n\n        try:\n            os.makedirs(self.output_dir)\n        except:\n            pass\n\n        for array_key, dataset_name in self.dataset_names.items():\n            logger.debug(\"Initializing dataset for %s\", array_key)\n\n            assert array_key in self.spec, (\n                \"Asked to store %s, but is not provided upstream.\" % array_key\n            )\n            assert array_key in batch.arrays, (\n                \"Asked to store %s, but is not part of batch.\" % array_key\n            )\n\n            array = batch.arrays[array_key]\n            dims = array.spec.roi.dims\n            batch_shape = array.data.shape\n\n            with self._open_file(filename) as data_file:\n                # if a dataset already exists, read its meta-information (if\n                # present)\n                if dataset_name in data_file:\n                    offset = self._get_offset(data_file[dataset_name]) or Coordinate(\n                        (0,) * dims\n                    )\n\n                else:\n                    provided_roi = self.spec[array_key].roi\n\n                    if provided_roi is None:\n                        raise RuntimeError(\n                            \"Dataset %s does not exist in %s, and no ROI is \"\n                            \"provided for %s. I don't know how to initialize \"\n                            \"the dataset.\" % (dataset_name, filename, array_key)\n                        )\n\n                    offset = provided_roi.offset\n                    voxel_size = array.spec.voxel_size\n                    data_shape = provided_roi.shape // voxel_size\n\n                    logger.debug(\"Shape in voxels: %s\", data_shape)\n                    # add channel dimensions (if present)\n                    data_shape = batch_shape[:-dims] + data_shape\n                    logger.debug(\"Shape with channel dimensions: %s\", data_shape)\n\n                    if array_key in self.dataset_dtypes:\n                        dtype = self.dataset_dtypes[array_key]\n                    else:\n                        dtype = batch.arrays[array_key].data.dtype\n\n                    logger.debug(\n                        \"create_dataset: %s, %s, %s, %s, offset=%s, resolution=%s\",\n                        dataset_name,\n                        data_shape,\n                        self.compression_type,\n                        dtype,\n                        offset,\n                        voxel_size,\n                    )\n\n                    dataset = data_file.create_dataset(\n                        name=dataset_name,\n                        shape=data_shape,\n                        compression=self.compression_type,\n                        dtype=dtype,\n                    )\n\n                    self._set_offset(dataset, offset)\n                    self._set_voxel_size(dataset, voxel_size)\n\n                logger.debug(\n                    \"%s (%s in %s) has offset %s\",\n                    array_key,\n                    dataset_name,\n                    filename,\n                    offset,\n                )\n                self.dataset_offsets[array_key] = offset",
  "def process(self, batch, request):\n        filename = os.path.join(self.output_dir, self.output_filename)\n\n        if not self.dataset_offsets:\n            self.init_datasets(batch)\n\n        with self._open_file(filename) as data_file:\n            for array_key, dataset_name in self.dataset_names.items():\n                dataset = data_file[dataset_name]\n\n                array_roi = batch.arrays[array_key].spec.roi\n                voxel_size = self.spec[array_key].voxel_size\n                dims = array_roi.dims\n                channel_slices = (slice(None),) * max(0, len(dataset.shape) - dims)\n\n                dataset_roi = Roi(\n                    self.dataset_offsets[array_key],\n                    Coordinate(dataset.shape[-dims:]) * voxel_size,\n                )\n                common_roi = array_roi.intersect(dataset_roi)\n\n                if common_roi.empty:\n                    logger.warn(\n                        \"array %s with ROI %s lies outside of dataset ROI %s, \"\n                        \"skipping writing\" % (array_key, array_roi, dataset_roi)\n                    )\n                    continue\n\n                dataset_voxel_roi = (\n                    common_roi - self.dataset_offsets[array_key]\n                ) // voxel_size\n                dataset_voxel_slices = dataset_voxel_roi.to_slices()\n                array_voxel_roi = (common_roi - array_roi.offset) // voxel_size\n                array_voxel_slices = array_voxel_roi.to_slices()\n\n                logger.debug(\n                    \"writing %s to voxel coordinates %s\"\n                    % (array_key, dataset_voxel_roi)\n                )\n\n                data = batch.arrays[array_key].data[channel_slices + array_voxel_slices]\n                dataset[channel_slices + dataset_voxel_slices] = data",
  "class Hdf5Write(Hdf5LikeWrite):\n    \"\"\"Assemble arrays of passing batches in one HDF5 file. This is useful to\n    store chunks produced by :class:`Scan` on disk without keeping the larger\n    array in memory. The ROIs of the passing arrays will be used to determine\n    the position where to store the data in the dataset.\n\n    Args:\n\n        dataset_names (``dict``, :class:`ArrayKey` -> ``string``):\n\n            A dictionary from array keys to names of the datasets to store them\n            in.\n\n        output_dir (``string``):\n\n            The directory to save the HDF5 file. Will be created, if it does\n            not exist.\n\n        output_filename (``string``):\n\n            The output filename of the container. Will be created, if it does\n            not exist, otherwise data is overwritten in the existing container.\n\n        compression_type (``string`` or ``int``):\n\n            Compression strategy.  Legal values are ``gzip``, ``szip``,\n            ``lzf``. If an integer between 1 and 10, this indicates ``gzip``\n            compression level.\n\n        dataset_dtypes (``dict``, :class:`ArrayKey` -> data type):\n\n            A dictionary from array keys to datatype (eg. ``np.int8``). If\n            given, arrays are stored using this type. The original arrays\n            within the pipeline remain unchanged.\n    \"\"\"\n\n    def _open_file(self, filename):\n        if os.path.exists(filename):\n            return h5py.File(filename, \"r+\")\n        else:\n            return h5py.File(filename, \"w\")",
  "def _open_file(self, filename):\n        if os.path.exists(filename):\n            return h5py.File(filename, \"r+\")\n        else:\n            return h5py.File(filename, \"w\")",
  "class RenumberConnectedComponents(BatchFilter):\n    \"\"\"Find connected components of the same value, and replace each component\n    with a new label.\n\n    Args:\n\n        labels (:class:`ArrayKey`):\n\n            The label array to modify.\n    \"\"\"\n\n    def __init__(self, labels):\n        self.labels = labels\n\n    def process(self, batch, request):\n        components = batch.arrays[self.labels].data\n        dtype = components.dtype\n        simple_neighborhood = malis.mknhood3d()\n        affinities_from_components = malis.seg_to_affgraph(\n            components, simple_neighborhood\n        )\n        components, _ = malis.connected_components_affgraph(\n            affinities_from_components, simple_neighborhood\n        )\n        batch.arrays[self.labels].data = components.astype(dtype)",
  "def __init__(self, labels):\n        self.labels = labels",
  "def process(self, batch, request):\n        components = batch.arrays[self.labels].data\n        dtype = components.dtype\n        simple_neighborhood = malis.mknhood3d()\n        affinities_from_components = malis.seg_to_affgraph(\n            components, simple_neighborhood\n        )\n        components, _ = malis.connected_components_affgraph(\n            affinities_from_components, simple_neighborhood\n        )\n        batch.arrays[self.labels].data = components.astype(dtype)",
  "class Squeeze(BatchFilter):\n    \"\"\"Squeeze a batch at a given axis\n\n    Args:\n        arrays (List[ArrayKey]): ArrayKeys to squeeze.\n        axis: Position of the single-dimensional axis to remove, defaults to 0.\n    \"\"\"\n\n    def __init__(self, arrays: List[ArrayKey], axis: int = 0):\n        self.arrays = arrays\n        self.axis = axis\n\n    def setup(self):\n        self.enable_autoskip()\n        for array in self.arrays:\n            self.updates(array, self.spec[array].copy())\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for array in self.arrays:\n            if array in request:\n                deps[array] = request[array].copy()\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n        for array in self.arrays:\n            if array in batch:\n                if not batch[array].spec.nonspatial:\n                    spatial_dims = request[array].roi.dims\n                    if self.axis >= batch[array].data.ndim - spatial_dims:\n                        raise ValueError(\n                            (\n                                f\"Squeeze.axis={self.axis} not permitted. \"\n                                \"Squeeze only supported for \"\n                                \"non-spatial dimensions of Array.\"\n                            )\n                        )\n\n                outputs[array] = batch[array]\n                outputs[array].data = np.squeeze(batch[array].data, self.axis)\n                logger.debug(f\"{array} shape: {outputs[array].data.shape}\")\n\n        return outputs",
  "def __init__(self, arrays: List[ArrayKey], axis: int = 0):\n        self.arrays = arrays\n        self.axis = axis",
  "def setup(self):\n        self.enable_autoskip()\n        for array in self.arrays:\n            self.updates(array, self.spec[array].copy())",
  "def prepare(self, request):\n        deps = BatchRequest()\n        for array in self.arrays:\n            if array in request:\n                deps[array] = request[array].copy()\n        return deps",
  "def process(self, batch, request):\n        outputs = Batch()\n        for array in self.arrays:\n            if array in batch:\n                if not batch[array].spec.nonspatial:\n                    spatial_dims = request[array].roi.dims\n                    if self.axis >= batch[array].data.ndim - spatial_dims:\n                        raise ValueError(\n                            (\n                                f\"Squeeze.axis={self.axis} not permitted. \"\n                                \"Squeeze only supported for \"\n                                \"non-spatial dimensions of Array.\"\n                            )\n                        )\n\n                outputs[array] = batch[array]\n                outputs[array].data = np.squeeze(batch[array].data, self.axis)\n                logger.debug(f\"{array} shape: {outputs[array].data.shape}\")\n\n        return outputs",
  "class NoiseAugment(BatchFilter):\n    \"\"\"Add random noise to an array. Uses the scikit-image function skimage.util.random_noise.\n    See scikit-image documentation for more information on arguments and additional kwargs.\n\n    Args:\n\n        array (:class:`ArrayKey`):\n\n            The intensity array to modify. Should be of type float and within range [-1, 1] or [0, 1].\n\n        mode (``string``):\n\n            Type of noise to add, see scikit-image documentation.\n\n        clip (``bool``):\n\n            Whether to preserve the image range (either [-1, 1] or [0, 1]) by clipping values in the end, see\n            scikit-image documentation\n    \"\"\"\n\n    def __init__(self, array, mode=\"gaussian\", clip=True, **kwargs):\n        self.array = array\n        self.mode = mode\n        self.clip = clip\n        self.kwargs = kwargs\n\n    def setup(self):\n        self.enable_autoskip()\n        self.updates(self.array, self.spec[self.array])\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array].copy()\n        return deps\n\n    def process(self, batch, request):\n        raw = batch.arrays[self.array]\n\n        assert raw.data.dtype == np.float32 or raw.data.dtype == np.float64, (\n            \"Noise augmentation requires float types for the raw array (not \"\n            + str(raw.data.dtype)\n            + \"). Consider using Normalize before.\"\n        )\n        if self.clip:\n            assert (\n                raw.data.min() >= -1 and raw.data.max() <= 1\n            ), \"Noise augmentation expects raw values in [-1,1] or [0,1]. Consider using Normalize before.\"\n\n        seed = request.random_seed\n\n        try:\n\n            raw.data = skimage.util.random_noise(\n                raw.data, mode=self.mode, rng=seed, clip=self.clip, **self.kwargs\n            ).astype(raw.data.dtype)\n\n        except ValueError:\n\n            # legacy version of skimage random_noise\n            raw.data = skimage.util.random_noise(\n                raw.data, mode=self.mode, seed=seed, clip=self.clip, **self.kwargs\n            ).astype(raw.data.dtype)",
  "def __init__(self, array, mode=\"gaussian\", clip=True, **kwargs):\n        self.array = array\n        self.mode = mode\n        self.clip = clip\n        self.kwargs = kwargs",
  "def setup(self):\n        self.enable_autoskip()\n        self.updates(self.array, self.spec[self.array])",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array].copy()\n        return deps",
  "def process(self, batch, request):\n        raw = batch.arrays[self.array]\n\n        assert raw.data.dtype == np.float32 or raw.data.dtype == np.float64, (\n            \"Noise augmentation requires float types for the raw array (not \"\n            + str(raw.data.dtype)\n            + \"). Consider using Normalize before.\"\n        )\n        if self.clip:\n            assert (\n                raw.data.min() >= -1 and raw.data.max() <= 1\n            ), \"Noise augmentation expects raw values in [-1,1] or [0,1]. Consider using Normalize before.\"\n\n        seed = request.random_seed\n\n        try:\n\n            raw.data = skimage.util.random_noise(\n                raw.data, mode=self.mode, rng=seed, clip=self.clip, **self.kwargs\n            ).astype(raw.data.dtype)\n\n        except ValueError:\n\n            # legacy version of skimage random_noise\n            raw.data = skimage.util.random_noise(\n                raw.data, mode=self.mode, seed=seed, clip=self.clip, **self.kwargs\n            ).astype(raw.data.dtype)",
  "class DownSample(BatchFilter):\n    \"\"\"Downsample arrays in a batch by given factors.\n\n    Args:\n\n        source (:class:`ArrayKey`):\n\n            The key of the array to downsample.\n\n        factor (``int`` or ``tuple`` of ``int``):\n\n            The factor to downsample with.\n\n        target (:class:`ArrayKey`):\n\n            The key of the array to store the downsampled ``source``.\n    \"\"\"\n\n    def __init__(self, source, factor, target):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n        assert isinstance(factor, numbers.Number) or isinstance(\n            factor, tuple\n        ), \"Scaling factor should be a number or a tuple of numbers.\"\n\n        self.source = source\n        self.factor = (\n            factor if isinstance(factor, numbers.Number) else Coordinate(factor)\n        )\n        self.target = target\n\n    def setup(self):\n        spec = self.spec[self.source].copy()\n        spec.voxel_size *= self.factor\n        self.provides(self.target, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.source] = request[self.target]\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n        data = batch.arrays[self.source].data\n\n        channel_dims = len(data.shape) - batch.arrays[self.source].spec.roi.dims\n\n        # downsample\n        if isinstance(self.factor, tuple):\n            slices = tuple(slice(None, None) for _ in range(channel_dims)) + tuple(\n                slice(None, None, k) for k in self.factor\n            )\n        else:\n            slices = tuple(slice(None, None) for _ in range(channel_dims)) + tuple(\n                slice(None, None, self.factor)\n                for i in range(batch[self.source].spec.roi.dims)\n            )\n\n        logger.debug(\"downsampling %s with %s\", self.source, slices)\n\n        data = data[slices]\n\n        # create output array\n        spec = self.spec[self.target].copy()\n        spec.roi = request[self.target].roi\n        outputs.arrays[self.target] = Array(data, spec)\n\n        return outputs",
  "def __init__(self, source, factor, target):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n        assert isinstance(factor, numbers.Number) or isinstance(\n            factor, tuple\n        ), \"Scaling factor should be a number or a tuple of numbers.\"\n\n        self.source = source\n        self.factor = (\n            factor if isinstance(factor, numbers.Number) else Coordinate(factor)\n        )\n        self.target = target",
  "def setup(self):\n        spec = self.spec[self.source].copy()\n        spec.voxel_size *= self.factor\n        self.provides(self.target, spec)\n        self.enable_autoskip()",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.source] = request[self.target]\n        return deps",
  "def process(self, batch, request):\n        outputs = Batch()\n        data = batch.arrays[self.source].data\n\n        channel_dims = len(data.shape) - batch.arrays[self.source].spec.roi.dims\n\n        # downsample\n        if isinstance(self.factor, tuple):\n            slices = tuple(slice(None, None) for _ in range(channel_dims)) + tuple(\n                slice(None, None, k) for k in self.factor\n            )\n        else:\n            slices = tuple(slice(None, None) for _ in range(channel_dims)) + tuple(\n                slice(None, None, self.factor)\n                for i in range(batch[self.source].spec.roi.dims)\n            )\n\n        logger.debug(\"downsampling %s with %s\", self.source, slices)\n\n        data = data[slices]\n\n        # create output array\n        spec = self.spec[self.target].copy()\n        spec.roi = request[self.target].roi\n        outputs.arrays[self.target] = Array(data, spec)\n\n        return outputs",
  "class UpSample(BatchFilter):\n    \"\"\"Upsample arrays in a batch by given factors.\n\n    Args:\n\n        source (:class:`ArrayKey`):\n\n            The key of the array to upsample.\n\n        factor (``int`` or ``Coordinate``):\n\n            The factor to upsample with.\n\n        target (:class:`ArrayKey`):\n\n            The key of the array to store the upsampled ``source``.\n    \"\"\"\n\n    def __init__(self, source, factor, target):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n        assert isinstance(factor, numbers.Number) or isinstance(\n            factor, Coordinate\n        ), \"Scaling factor should be a number or a Coordinate.\"\n\n        self.source = source\n        self.factor = factor\n        self.target = target\n\n    def setup(self):\n        spec = self.spec[self.source].copy()\n\n        if not isinstance(self.factor, Coordinate):\n            self.factor = Coordinate((self.factor,) * spec.roi.dims)\n\n        assert spec.voxel_size % self.factor == (0,) * len(\n            spec.voxel_size\n        ), \"voxel size of upsampled volume is not integer: %s/%s = %s\" % (\n            spec.voxel_size,\n            self.factor,\n            tuple(v / f for v, f in zip(spec.voxel_size, self.factor)),\n        )\n        spec.voxel_size /= self.factor\n        self.provides(self.target, spec)\n\n    def prepare(self, request):\n        deps = BatchRequest()\n\n        if self.target not in request:\n            return\n\n        logger.debug(\"preparing upsampling of \" + str(self.source))\n\n        upstream_voxel_size = self.spec[self.source].voxel_size\n\n        request_roi = request[self.target].roi.snap_to_grid(\n            upstream_voxel_size, mode=\"grow\"\n        )\n        logger.debug(\"request ROI is %s\" % request_roi)\n\n        # add or merge to batch request\n        deps[self.source] = ArraySpec(roi=request_roi)\n\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n\n        if self.target not in request:\n            return\n\n        input_roi = batch.arrays[self.source].spec.roi\n        request_roi = request[self.target].roi\n\n        assert input_roi.contains(request_roi)\n\n        # upsample\n\n        logger.debug(\"upsampling %s with %s\", self.source, self.factor)\n\n        crop = batch.arrays[self.source]\n        data = crop.data\n\n        for d, f in enumerate(self.factor):\n            data = np.repeat(data, f, axis=-self.factor.dims + d)\n\n        # create output array\n        spec = self.spec[self.target].copy()\n        spec.roi = input_roi\n        outputs.arrays[self.target] = Array(data, spec).crop(request_roi)\n        return outputs",
  "def __init__(self, source, factor, target):\n        assert isinstance(source, ArrayKey)\n        assert isinstance(target, ArrayKey)\n        assert isinstance(factor, numbers.Number) or isinstance(\n            factor, Coordinate\n        ), \"Scaling factor should be a number or a Coordinate.\"\n\n        self.source = source\n        self.factor = factor\n        self.target = target",
  "def setup(self):\n        spec = self.spec[self.source].copy()\n\n        if not isinstance(self.factor, Coordinate):\n            self.factor = Coordinate((self.factor,) * spec.roi.dims)\n\n        assert spec.voxel_size % self.factor == (0,) * len(\n            spec.voxel_size\n        ), \"voxel size of upsampled volume is not integer: %s/%s = %s\" % (\n            spec.voxel_size,\n            self.factor,\n            tuple(v / f for v, f in zip(spec.voxel_size, self.factor)),\n        )\n        spec.voxel_size /= self.factor\n        self.provides(self.target, spec)",
  "def prepare(self, request):\n        deps = BatchRequest()\n\n        if self.target not in request:\n            return\n\n        logger.debug(\"preparing upsampling of \" + str(self.source))\n\n        upstream_voxel_size = self.spec[self.source].voxel_size\n\n        request_roi = request[self.target].roi.snap_to_grid(\n            upstream_voxel_size, mode=\"grow\"\n        )\n        logger.debug(\"request ROI is %s\" % request_roi)\n\n        # add or merge to batch request\n        deps[self.source] = ArraySpec(roi=request_roi)\n\n        return deps",
  "def process(self, batch, request):\n        outputs = Batch()\n\n        if self.target not in request:\n            return\n\n        input_roi = batch.arrays[self.source].spec.roi\n        request_roi = request[self.target].roi\n\n        assert input_roi.contains(request_roi)\n\n        # upsample\n\n        logger.debug(\"upsampling %s with %s\", self.source, self.factor)\n\n        crop = batch.arrays[self.source]\n        data = crop.data\n\n        for d, f in enumerate(self.factor):\n            data = np.repeat(data, f, axis=-self.factor.dims + d)\n\n        # create output array\n        spec = self.spec[self.target].copy()\n        spec.roi = input_roi\n        outputs.arrays[self.target] = Array(data, spec).crop(request_roi)\n        return outputs",
  "class IntensityAugment(BatchFilter):\n    \"\"\"Randomly scale and shift the values of an intensity array.\n\n    Args:\n\n        array (:class:`ArrayKey`):\n\n            The intensity array to modify.\n\n        scale_min (``float``):\n        scale_max (``float``):\n        shift_min (``float``):\n        shift_max (``float``):\n\n            The min and max of the uniformly randomly drawn scaling and\n            shifting values for the intensity augmentation. Intensities are\n            changed as::\n\n                a = a.mean() + (a-a.mean())*scale + shift\n\n        z_section_wise (``bool``):\n\n            Perform the augmentation z-section wise. Requires 3D arrays and\n            assumes that z is the first dimension.\n\n        clip (``bool``):\n\n            Set to False if modified values should not be clipped to [0, 1]\n            Disables range check!\n    \"\"\"\n\n    def __init__(\n        self,\n        array,\n        scale_min,\n        scale_max,\n        shift_min,\n        shift_max,\n        z_section_wise=False,\n        clip=True,\n    ):\n        self.array = array\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n        self.shift_min = shift_min\n        self.shift_max = shift_max\n        self.z_section_wise = z_section_wise\n        self.clip = clip\n\n    def setup(self):\n        self.enable_autoskip()\n        self.updates(self.array, self.spec[self.array])\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array].copy()\n        return deps\n\n    def process(self, batch, request):\n        raw = batch.arrays[self.array]\n\n        assert (\n            not self.z_section_wise or raw.spec.roi.dims == 3\n        ), \"If you specify 'z_section_wise', I expect 3D data.\"\n        assert raw.data.dtype == np.float32 or raw.data.dtype == np.float64, (\n            \"Intensity augmentation requires float types for the raw array (not \"\n            + str(raw.data.dtype)\n            + \"). Consider using Normalize before.\"\n        )\n        if self.clip:\n            assert (\n                raw.data.min() >= 0 and raw.data.max() <= 1\n            ), \"Intensity augmentation expects raw values in [0,1]. Consider using Normalize before.\"\n\n        if self.z_section_wise:\n            for z in range((raw.spec.roi / self.spec[self.array].voxel_size).shape[0]):\n                raw.data[z] = self.__augment(\n                    raw.data[z],\n                    np.random.uniform(low=self.scale_min, high=self.scale_max),\n                    np.random.uniform(low=self.shift_min, high=self.shift_max),\n                )\n        else:\n            raw.data = self.__augment(\n                raw.data,\n                np.random.uniform(low=self.scale_min, high=self.scale_max),\n                np.random.uniform(low=self.shift_min, high=self.shift_max),\n            )\n\n        # clip values, we might have pushed them out of [0,1]\n        if self.clip:\n            raw.data[raw.data > 1] = 1\n            raw.data[raw.data < 0] = 0\n\n    def __augment(self, a, scale, shift):\n        return a.mean() + (a - a.mean()) * scale + shift",
  "def __init__(\n        self,\n        array,\n        scale_min,\n        scale_max,\n        shift_min,\n        shift_max,\n        z_section_wise=False,\n        clip=True,\n    ):\n        self.array = array\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n        self.shift_min = shift_min\n        self.shift_max = shift_max\n        self.z_section_wise = z_section_wise\n        self.clip = clip",
  "def setup(self):\n        self.enable_autoskip()\n        self.updates(self.array, self.spec[self.array])",
  "def prepare(self, request):\n        deps = BatchRequest()\n        deps[self.array] = request[self.array].copy()\n        return deps",
  "def process(self, batch, request):\n        raw = batch.arrays[self.array]\n\n        assert (\n            not self.z_section_wise or raw.spec.roi.dims == 3\n        ), \"If you specify 'z_section_wise', I expect 3D data.\"\n        assert raw.data.dtype == np.float32 or raw.data.dtype == np.float64, (\n            \"Intensity augmentation requires float types for the raw array (not \"\n            + str(raw.data.dtype)\n            + \"). Consider using Normalize before.\"\n        )\n        if self.clip:\n            assert (\n                raw.data.min() >= 0 and raw.data.max() <= 1\n            ), \"Intensity augmentation expects raw values in [0,1]. Consider using Normalize before.\"\n\n        if self.z_section_wise:\n            for z in range((raw.spec.roi / self.spec[self.array].voxel_size).shape[0]):\n                raw.data[z] = self.__augment(\n                    raw.data[z],\n                    np.random.uniform(low=self.scale_min, high=self.scale_max),\n                    np.random.uniform(low=self.shift_min, high=self.shift_max),\n                )\n        else:\n            raw.data = self.__augment(\n                raw.data,\n                np.random.uniform(low=self.scale_min, high=self.scale_max),\n                np.random.uniform(low=self.shift_min, high=self.shift_max),\n            )\n\n        # clip values, we might have pushed them out of [0,1]\n        if self.clip:\n            raw.data[raw.data > 1] = 1\n            raw.data[raw.data < 0] = 0",
  "def __augment(self, a, scale, shift):\n        return a.mean() + (a - a.mean()) * scale + shift"
]